{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K4CYwe_JQkP"
      },
      "source": [
        "# Chapter 4: Shakespeare NLP Exercises: From Classical NLP to Neural Language Models\n",
        "\n",
        "**This exercise has 20 smaller tasks, for a total of 18 points, and you will have two weeks to complete it instead of one. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "Welcome to this comprehensive hands-on workshop on Natural Language Processing! You will journey from classical text processing techniques to building and comparing neural language models using Shakespeare's complete works.\n",
        "\n",
        "## What You Will Learn\n",
        "\n",
        "- **Stage 0**: Corpus processing, tokenization, and word embeddings\n",
        "- **Stage 1**: Character-level RNN language models\n",
        "- **Stage 2**: Word-level RNN language models with theatrical chat interfaces\n",
        "- **Stage 3**: LSTM language models and architecture comparison\n",
        "\n",
        "## Instructions for Students\n",
        "\n",
        "Throughout this notebook, you will find code sections marked with:\n",
        "```python\n",
        "# START STUDENT CODE\n",
        "...\n",
        "# END STUDENT CODE\n",
        "```\n",
        "\n",
        "Any and all required tasks can and should be completed by writing code into these sections.\n",
        "\n",
        "XXXXX lösungen rauscutten, hinweise aus non-solution notebook wieder einpflegen\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1R8GtiJQkQ"
      },
      "source": [
        "## Setup and Dependencies\n",
        "\n",
        "First, let's install the required packages and check our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrf4X7maJQkQ",
        "outputId": "a3b8b53b-7008-4a6a-8640-0ee82dc05d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n",
            "Using device: cpu\n",
            "Data directories created.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (uncomment if running in Colab)\n",
        "# !pip install torch numpy requests\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "from collections import Counter\n",
        "\n",
        "# Check device availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Create data directories\n",
        "os.makedirs(\"data/works\", exist_ok=True)\n",
        "print(\"Data directories created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go9CEh7-JQkQ"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 0: Corpus & Classical NLP Foundations\n",
        "\n",
        "In this stage, you will work with the complete works of Shakespeare to learn fundamental text processing techniques:\n",
        "- Downloading and segmenting a large text corpus\n",
        "- Tokenization and normalization\n",
        "- Working with pretrained word embeddings (GloVe)\n",
        "\n",
        "---\n",
        "\n",
        "## Stage 0.1 – Shakespeare Corpus Download & Segmentation\n",
        "\n",
        "This exercise introduces you to working with a **real, unstructured text corpus**. You will download the complete works of William Shakespeare from Project Gutenberg and convert the raw file into a collection of **separate, clean text files**, one per work. To make sure everyone can actually start working on the exercises and gets stuck right at the start, we provide code that starts you off and does the following things for you:\n",
        "\n",
        "1. **Download the Shakespeare corpus** from Project Gutenberg\n",
        "2. **Analyze the structure** - find the Table of Contents and title markers\n",
        "3. **Segment the corpus** into separate work files\n",
        "4. **Verify your segmentation** by printing statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNVIR58lJQkR",
        "outputId": "adcf5eb8-362b-4f22-c283-a3e21ab32d52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found data/pg100.txt, skipping download.\n",
            "Found Table of Contents around line 34\n",
            "Identified 44 works from TOC.\n",
            "\n",
            "======================================================================\n",
            "SEGMENTATION SUMMARY\n",
            "======================================================================\n",
            "Work Title                                              Chars    Lines\n",
            "----------------------------------------------------------------------\n",
            "THE SONNETS                                             98332     2777\n",
            "ALL’S WELL THAT ENDS WELL                              134624     4964\n",
            "THE TRAGEDY OF ANTONY AND CLEOPATRA                    152400     6641\n",
            "AS YOU LIKE IT                                         127042     4438\n",
            "THE COMEDY OF ERRORS                                    88333     3201\n",
            "THE TRAGEDY OF CORIOLANUS                              165954     6445\n",
            "CYMBELINE                                              161238     5885\n",
            "THE TRAGEDY OF HAMLET, PRINCE OF DENMARK               177938     6698\n",
            "THE FIRST PART OF KING HENRY THE FOURTH                141709     4807\n",
            "THE SECOND PART OF KING HENRY THE FOURTH               153547     5196\n",
            "THE LIFE OF KING HENRY THE FIFTH                       153226     4946\n",
            "THE FIRST PART OF HENRY THE SIXTH                      130854     4630\n",
            "THE SECOND PART OF KING HENRY THE SIXTH                150503     5182\n",
            "THE THIRD PART OF KING HENRY THE SIXTH                 146004     5122\n",
            "KING HENRY THE EIGHTH                                  143802     5139\n",
            "THE LIFE AND DEATH OF KING JOHN                        121399     4100\n",
            "THE TRAGEDY OF JULIUS CAESAR                           116492     4647\n",
            "THE TRAGEDY OF KING LEAR                               155335     6109\n",
            "LOVE’S LABOUR’S LOST                                   127480     5008\n",
            "THE TRAGEDY OF MACBETH                                 104455     4150\n",
            "MEASURE FOR MEASURE                                    126279     4884\n",
            "THE MERCHANT OF VENICE                                 121274     4171\n",
            "THE MERRY WIVES OF WINDSOR                             130500     4823\n",
            "A MIDSUMMER NIGHT’S DREAM                               96731     3485\n",
            "MUCH ADO ABOUT NOTHING                                 122748     4600\n",
            "THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE             154219     6282\n",
            "PERICLES, PRINCE OF TYRE                               111406     4193\n",
            "KING RICHARD THE SECOND                                130059     4287\n",
            "KING RICHARD THE THIRD                                 176462     6475\n",
            "THE TRAGEDY OF ROMEO AND JULIET                        142446     5267\n",
            "THE TAMING OF THE SHREW                                123751     4868\n",
            "THE TEMPEST                                             98748     3836\n",
            "THE LIFE OF TIMON OF ATHENS                            111694     4421\n",
            "THE TRAGEDY OF TITUS ANDRONICUS                        120509     4125\n",
            "TROILUS AND CRESSIDA                                   157375     6204\n",
            "TWELFTH NIGHT; OR, WHAT YOU WILL                       115458     4496\n",
            "THE TWO GENTLEMEN OF VERONA                            101929     4251\n",
            "THE TWO NOBLE KINSMEN                                  142421     5511\n",
            "THE WINTER’S TALE                                      144023     5015\n",
            "A LOVER’S COMPLAINT                                     14364      383\n",
            "THE PASSIONATE PILGRIM                                  17060      582\n",
            "THE PHOENIX AND THE TURTLE                               2075       94\n",
            "THE RAPE OF LUCRECE                                     86811     2187\n",
            "VENUS AND ADONIS                                        77408     1787\n",
            "\n",
            "Total works extracted: 44\n"
          ]
        }
      ],
      "source": [
        "RAW_FILE = 'data/pg100.txt'\n",
        "WORKS_DIR = 'data/works'\n",
        "\n",
        "def download_corpus():\n",
        "    \"\"\"Download the Shakespeare corpus from Project Gutenberg if not present.\"\"\"\n",
        "\n",
        "    if not os.path.exists(RAW_FILE):\n",
        "        print(f\"Downloading corpus...\")\n",
        "        url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            with open(RAW_FILE, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            print(\"Download complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading file: {e}\")\n",
        "            return False\n",
        "    else:\n",
        "        print(f\"Found {RAW_FILE}, skipping download.\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def segment_corpus():\n",
        "    \"\"\"Segment the corpus into individual works.\"\"\"\n",
        "    # Read the file\n",
        "    with open(RAW_FILE, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Find Table of Contents\n",
        "    toc_start_idx = -1\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"Contents\" in line and len(line.strip()) < 20:\n",
        "            if i < 200:  # TOC should be in first 200 lines\n",
        "                toc_start_idx = i\n",
        "                break\n",
        "\n",
        "    if toc_start_idx == -1:\n",
        "        print(\"Could not find Table of Contents.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Found Table of Contents around line {toc_start_idx + 1}\")\n",
        "\n",
        "    # Extract titles from TOC\n",
        "    potential_titles = []\n",
        "    first_candidate = None\n",
        "    toc_end_idx = -1\n",
        "\n",
        "    for i in range(toc_start_idx + 1, len(lines)):\n",
        "        stripped = lines[i].strip()\n",
        "        if not stripped:\n",
        "            continue\n",
        "\n",
        "        if first_candidate is None:\n",
        "            first_candidate = stripped\n",
        "            potential_titles.append(stripped)\n",
        "            continue\n",
        "\n",
        "        # Check if this line matches the first candidate (start of first work)\n",
        "        if stripped == first_candidate:\n",
        "            toc_end_idx = i\n",
        "            break\n",
        "\n",
        "        potential_titles.append(stripped)\n",
        "\n",
        "        if i > 3000:  # Safety break\n",
        "            print(\"Warning: TOC parsing went too far.\")\n",
        "            break\n",
        "\n",
        "    if toc_end_idx == -1:\n",
        "        print(\"Could not determine end of TOC.\")\n",
        "        return []\n",
        "\n",
        "    works_titles = potential_titles\n",
        "    print(f\"Identified {len(works_titles)} works from TOC.\")\n",
        "\n",
        "    # Find start positions of each work\n",
        "    work_starts = {}\n",
        "    current_search_idx = toc_end_idx\n",
        "    work_starts[works_titles[0]] = current_search_idx\n",
        "\n",
        "    for k in range(1, len(works_titles)):\n",
        "        title = works_titles[k]\n",
        "        found = False\n",
        "        for j in range(current_search_idx + 1, len(lines)):\n",
        "            if lines[j].strip() == title:\n",
        "                work_starts[title] = j\n",
        "                current_search_idx = j\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            print(f\"Warning: Could not find start of '{title}'\")\n",
        "\n",
        "    # Write to files\n",
        "    os.makedirs(WORKS_DIR, exist_ok=True)\n",
        "    sorted_works = sorted(work_starts.items(), key=lambda x: x[1])\n",
        "\n",
        "    extracted_works = []\n",
        "    for i in range(len(sorted_works)):\n",
        "        title, start_line = sorted_works[i]\n",
        "\n",
        "        if i < len(sorted_works) - 1:\n",
        "            end_line = sorted_works[i + 1][1]\n",
        "        else:\n",
        "            end_line = len(lines)\n",
        "\n",
        "        content_lines = lines[start_line:end_line]\n",
        "        text_content = \"\".join(content_lines)\n",
        "\n",
        "        # Clean title for filename\n",
        "        safe_title = re.sub(r'[^\\w\\s-]', '', title).strip().lower()\n",
        "        safe_title = re.sub(r'[\\s-]+', '_', safe_title)\n",
        "        filename = f\"{safe_title}.txt\"\n",
        "\n",
        "        out_path = os.path.join(WORKS_DIR, filename)\n",
        "        with open(out_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(text_content)\n",
        "\n",
        "        extracted_works.append((title, filename, len(text_content), len(content_lines)))\n",
        "\n",
        "    return extracted_works\n",
        "    # Task 0.2: END STUDENT CODE\n",
        "\n",
        "# Run the corpus download and segmentation\n",
        "if download_corpus():\n",
        "    works = segment_corpus()\n",
        "\n",
        "    if works:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"SEGMENTATION SUMMARY\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"{'Work Title':<50} {'Chars':>10} {'Lines':>8}\")\n",
        "        print(\"-\" * 70)\n",
        "        for title, filename, chars, lines_count in works:\n",
        "            print(f\"{title[:48]:<50} {chars:>10} {lines_count:>8}\")\n",
        "        print(f\"\\nTotal works extracted: {len(works)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlC1EEi9JQkR"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 0.2 – Basic Tokenization and Normalization\n",
        "\n",
        "### Description\n",
        "\n",
        "In this exercise, you will build foundational text-processing utilities. You will design a simple **tokenizer** and apply basic **normalization** steps to Shakespeare's works. This mirrors the early stages of many NLP pipelines.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Implement a minimal **tokenizer** for plain-text data\n",
        "- Apply common **normalization** steps such as lowercasing and punctuation handling\n",
        "- Inspect token distributions to understand corpus characteristics\n",
        "- Compute statistics for single works and the entire corpus\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. **Implement a basic tokenizer (3x0.5 points)** that:\n",
        "   - Splits text into word-like units\n",
        "   - Treats whitespace as a separator\n",
        "   - Separates punctuation into its own tokens\n",
        "   \n",
        "2. **Add normalization steps (1 point)**:\n",
        "   - Lowercase all tokens\n",
        "   - Normalize curly/smart quotes to straight quotes\n",
        "   \n",
        "3. **Inspect tokenized output** and compute statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHbteSPuJQkR",
        "outputId": "54ef0f1c-1f95-4cb3-f9ee-e5140d72c7e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing: the_tragedy_of_romeo_and_juliet.txt\n",
            "\n",
            "Total tokens: 33150\n",
            "Unique tokens (vocabulary size): 3783\n",
            "\n",
            "--- First 50 tokens ---\n",
            "['the', 'tragedy', 'of', 'romeo', 'and', 'juliet', 'contents', 'the', 'prologue', '.', 'act', 'i', 'scene', 'i', '.', 'a', 'public', 'place', '.', 'scene', 'ii', '.', 'a', 'street', '.', 'scene', 'iii', '.', 'room', 'in', \"capulet's\", 'house', '.', 'scene', 'iv', '.', 'a', 'street', '.', 'scene', 'v', '.', 'a', 'hall', 'in', \"capulet's\", 'house', '.', 'act', 'ii']\n",
            "\n",
            "--- Top 10 most frequent tokens ---\n",
            "  ',': 2704\n",
            "  '.': 2600\n",
            "  'and': 736\n",
            "  'the': 688\n",
            "  'i': 583\n",
            "  'to': 541\n",
            "  'a': 488\n",
            "  'of': 395\n",
            "  '?': 369\n",
            "  'my': 356\n",
            "\n",
            "--- Sample rare tokens ---\n",
            "  'figure': 1\n",
            "  'sacrifices': 1\n",
            "  'glooming': 1\n",
            "  'pardon'd': 1\n",
            "  'punished': 1\n"
          ]
        }
      ],
      "source": [
        "# Exercise 0.2: Basic Tokenization and Normalization\n",
        "\n",
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Tokenizes the input text into a list of strings.\n",
        "\n",
        "    Design Decisions:\n",
        "    1. Lowercase: Applied to reduce vocabulary size.\n",
        "    2. Punctuation: Separated from words into their own tokens.\n",
        "    3. Contractions: Kept together using regex (e.g., \"don't\" stays as one token).\n",
        "\n",
        "    Args:\n",
        "        text: Input text string\n",
        "\n",
        "    Returns:\n",
        "        List of token strings\n",
        "    \"\"\"\n",
        "    # Task 0.3: START STUDENT CODE\n",
        "\n",
        "    # HINT:\n",
        "    # 1. Lowercase the text\n",
        "    text = text.lower()\n",
        "    # 2. Normalize smart quotes ('', \"\") to straight quotes (' ', \"\")\n",
        "    text = text.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "    # 3. Replace multiple whitespace chars (newlines, tabs) with single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # 4. Use re.findall() with a regex pattern to extract tokens:\n",
        "    #    - Words with optional contractions: \\w+(?:'\\w+)?\n",
        "    #    - Single punctuation: [^\\w\\s]\n",
        "\n",
        "    tokens = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", text)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "    # Task 0.3: END STUDENT CODE\n",
        "\n",
        "# Test the tokenizer on a sample work\n",
        "sample_work = 'the_tragedy_of_romeo_and_juliet.txt'\n",
        "sample_path = os.path.join(WORKS_DIR, sample_work)\n",
        "\n",
        "if os.path.exists(sample_path):\n",
        "    with open(sample_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    tokens = tokenize(text)\n",
        "    vocab = Counter(tokens)\n",
        "\n",
        "    print(f\"Analyzing: {sample_work}\")\n",
        "    print(f\"\\nTotal tokens: {len(tokens)}\")\n",
        "    print(f\"Unique tokens (vocabulary size): {len(vocab)}\")\n",
        "\n",
        "    print(\"\\n--- First 50 tokens ---\")\n",
        "    print(tokens[:50])\n",
        "\n",
        "    print(\"\\n--- Top 10 most frequent tokens ---\")\n",
        "    for token, count in vocab.most_common(10):\n",
        "        print(f\"  '{token}': {count}\")\n",
        "\n",
        "    print(\"\\n--- Sample rare tokens ---\")\n",
        "    for token, count in vocab.most_common()[-5:]:\n",
        "        print(f\"  '{token}': {count}\")\n",
        "else:\n",
        "    print(f\"Sample work not found at {sample_path}. Run Exercise 0.1 first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPdxk4hxJQkR",
        "outputId": "6b4ca997-ef2f-43fd-93ac-cf7227519730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CORPUS-WIDE STATISTICS\n",
            "======================================================================\n",
            "\n",
            "Found 44 works in the corpus.\n",
            "\n",
            "Work                                               Total Tokens  Unique Tokens\n",
            "----------------------------------------------------------------------------\n",
            "the_tragedy_of_hamlet_prince_of_denmark.txt               40789           4828\n",
            "king_richard_the_third.txt                                39454           4125\n",
            "the_tragedy_of_coriolanus.txt                             37220           4134\n",
            "cymbeline.txt                                             36677           4335\n",
            "the_tragedy_of_othello_the_moor_of_venice.txt             35978           3879\n",
            "the_tragedy_of_king_lear.txt                              35786           4271\n",
            "troilus_and_cressida.txt                                  35646           4323\n",
            "the_second_part_of_king_henry_the_fourth.txt              34849           4176\n",
            "the_tragedy_of_antony_and_cleopatra.txt                   34531           4027\n",
            "the_life_of_king_henry_the_fifth.txt                      34270           4628\n",
            "... (showing top 10 by token count)\n",
            "\n",
            "======================================================================\n",
            "AGGREGATED STATISTICS\n",
            "======================================================================\n",
            "\n",
            "Total tokens across all works: 1,221,061\n",
            "Unique tokens (vocabulary size): 28,383\n",
            "Average tokens per work: 27,751\n",
            "\n",
            "--- Top 20 Most Common Tokens in Corpus ---\n",
            "                ,:    94713 occurrences\n",
            "                .:    91379 occurrences\n",
            "              the:    30467 occurrences\n",
            "              and:    28552 occurrences\n",
            "                i:    22197 occurrences\n",
            "               to:    20745 occurrences\n",
            "               of:    18838 occurrences\n",
            "                a:    16492 occurrences\n",
            "              you:    14470 occurrences\n",
            "               my:    13201 occurrences\n",
            "                ;:    12919 occurrences\n",
            "               in:    12492 occurrences\n",
            "             that:    11840 occurrences\n",
            "                ?:    11396 occurrences\n",
            "               is:     9762 occurrences\n",
            "              not:     9095 occurrences\n",
            "             with:     8556 occurrences\n",
            "               me:     8277 occurrences\n",
            "              for:     8226 occurrences\n",
            "               it:     8218 occurrences\n",
            "\n",
            "--- Token Coverage Statistics ---\n",
            "  Top   10 tokens cover  28.7% of all tokens\n",
            "  Top   50 tokens cover  49.8% of all tokens\n",
            "  Top  100 tokens cover  59.5% of all tokens\n",
            "  Top  500 tokens cover  76.3% of all tokens\n",
            "  Top 1000 tokens cover  82.7% of all tokens\n"
          ]
        }
      ],
      "source": [
        "# Exercise 0.2 (continued): Corpus-wide Statistics\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CORPUS-WIDE STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Collect statistics from all works\n",
        "work_files = [f for f in os.listdir(WORKS_DIR) if f.endswith('.txt')]\n",
        "print(f\"\\nFound {len(work_files)} works in the corpus.\\n\")\n",
        "\n",
        "corpus_tokens = []\n",
        "corpus_vocab = Counter()\n",
        "work_stats = []\n",
        "\n",
        "for work_file in sorted(work_files):\n",
        "    work_path = os.path.join(WORKS_DIR, work_file)\n",
        "    with open(work_path, 'r', encoding='utf-8') as f:\n",
        "        work_text = f.read()\n",
        "\n",
        "    work_tokens = tokenize(work_text)\n",
        "    work_vocab = Counter(work_tokens)\n",
        "\n",
        "    corpus_tokens.extend(work_tokens)\n",
        "    corpus_vocab.update(work_vocab)\n",
        "\n",
        "    work_stats.append({\n",
        "        'name': work_file,\n",
        "        'total_tokens': len(work_tokens),\n",
        "        'unique_tokens': len(work_vocab)\n",
        "    })\n",
        "\n",
        "# Print per-work summary\n",
        "print(f\"{'Work':<50} {'Total Tokens':>12} {'Unique Tokens':>14}\")\n",
        "print(\"-\" * 76)\n",
        "for stat in sorted(work_stats, key=lambda x: x['total_tokens'], reverse=True)[:10]:\n",
        "    print(f\"{stat['name']:<50} {stat['total_tokens']:>12} {stat['unique_tokens']:>14}\")\n",
        "print(\"... (showing top 10 by token count)\")\n",
        "\n",
        "# Print corpus-wide statistics\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"AGGREGATED STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal tokens across all works: {len(corpus_tokens):,}\")\n",
        "print(f\"Unique tokens (vocabulary size): {len(corpus_vocab):,}\")\n",
        "print(f\"Average tokens per work: {len(corpus_tokens) / len(work_files):,.0f}\")\n",
        "\n",
        "print(\"\\n--- Top 20 Most Common Tokens in Corpus ---\")\n",
        "for token, count in corpus_vocab.most_common(20):\n",
        "    print(f\"  {token:>15}: {count:>8} occurrences\")\n",
        "\n",
        "print(\"\\n--- Token Coverage Statistics ---\")\n",
        "total_tokens = len(corpus_tokens)\n",
        "for n in [10, 50, 100, 500, 1000]:\n",
        "    top_n_count = sum(count for _, count in corpus_vocab.most_common(n))\n",
        "    coverage = (top_n_count / total_tokens) * 100\n",
        "    print(f\"  Top {n:>4} tokens cover {coverage:>5.1f}% of all tokens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m3DOuAtJQkR"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 0.3 – Working With Pretrained Word Embeddings\n",
        "\n",
        "### Description\n",
        "\n",
        "In this exercise, you will download a widely used pretrained word embedding model (GloVe) and explore semantic relationships between words by computing cosine similarities manually.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Load a pretrained embedding model into Python\n",
        "- Implement cosine similarity manually using tensor operations\n",
        "- Compute and analyze similarities between selected word pairs\n",
        "- Observe how semantic and syntactic relationships are reflected in vector space\n",
        "- Perform analogy operations (e.g., king - man + woman ≈ queen)\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Download GloVe embeddings (100-dimensional version)\n",
        "2. Load embeddings into a dictionary\n",
        "3. **Implement cosine similarity** manually **(0.5 points)**\n",
        "4. Explore semantic relationships** between word pairs - this requires **building a find_nearest function** that finds the nearest neighbor from a set of input vectors and a target vector. **(1 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Lx3XhJnhJQkR"
      },
      "outputs": [],
      "source": [
        "# Exercise 0.3: Working With Pretrained Word Embeddings\n",
        "\n",
        "GLOVE_PATH = 'data/glove.6B.100d.txt'\n",
        "\n",
        "\n",
        "def load_glove(path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Load GloVe embeddings from a file.\n",
        "\n",
        "    Args:\n",
        "        path: Path to the GloVe file\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping words to torch tensors\n",
        "    \"\"\"\n",
        "    print(f\"Loading GloVe from {path}...\")\n",
        "    embeddings = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            try:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if len(vector) == 100:  # Ensure correct dimension\n",
        "                    embeddings[word] = torch.tensor(vector)\n",
        "            except ValueError:\n",
        "                continue\n",
        "    print(f\"Loaded {len(embeddings)} words.\")\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1: torch.Tensor, vec2: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two vectors manually.\n",
        "\n",
        "    Formula: cos(θ) = (A · B) / (|A| * |B|)\n",
        "\n",
        "    Args:\n",
        "        vec1: First vector\n",
        "        vec2: Second vector\n",
        "\n",
        "    Returns:\n",
        "        Cosine similarity as a float\n",
        "    \"\"\"\n",
        "    # Task 1.1: START STUDENT CODE\n",
        "\n",
        "    # HINT: Implement the cosine similarity formula: cos(θ) = (A · B) / (|A| * |B|)\n",
        "    # 1. Compute dot product: torch.dot(vec1, vec2)\n",
        "    dot_product = torch.dot(vec1, vec2)\n",
        "    # 2. Compute norms: torch.norm()\n",
        "    norm_vec1 = torch.norm(vec1)\n",
        "    norm_vec2 = torch.norm(vec2)\n",
        "    # 3. Handle zero norms (avoid division by zero)\n",
        "    if norm_vec1.item() == 0 or norm_vec2.item() == 0:\n",
        "        return 0.0\n",
        "    # 4. Return the result as a Python float using .item()\n",
        "    cosine_sim = dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "    return cosine_sim.item()\n",
        "\n",
        "    # Task 1.1: END STUDENT CODE\n",
        "\n",
        "\n",
        "def find_nearest(embeddings: dict, target_vec: torch.Tensor, n: int = 5,\n",
        "                 exclude_words: list = None) -> list:\n",
        "    \"\"\"\n",
        "    Find the n nearest neighbors to a target vector by computing\n",
        "    the dot product of each word embedding with the target vector.\n",
        "\n",
        "    Args:\n",
        "        embeddings: Dictionary of word embeddings\n",
        "        target_vec: Target vector to find neighbors for\n",
        "        n: Number of neighbors to return\n",
        "        exclude_words: Words to exclude from results\n",
        "\n",
        "    Returns:\n",
        "        List of (word, similarity) tuples\n",
        "    \"\"\"\n",
        "    # Task 1.2: START STUDENT CODE\n",
        "    if exclude_words is None:\n",
        "        exclude_words = []\n",
        "    # HINT:\n",
        "    # 1. Convert embeddings dict to lists: words and vocab_matrix (stacked tensors)\n",
        "    words = list(embeddings.keys())\n",
        "    vocab_matrix = torch.stack([embeddings[w] for w in words])\n",
        "    # 2. Compute norms for all vocabulary vectors and the target vector\n",
        "    vocab_norms = torch.norm(vocab_matrix, dim=1)\n",
        "    target_norm = torch.norm(target_vec)\n",
        "    # 3. Compute dot products between vocab_matrix and target_vec using matmul\n",
        "    dot_products = torch.matmul(vocab_matrix, target_vec)\n",
        "    # 4. Compute cosine similarities using the formula\n",
        "    cosine_sims = dot_products / (vocab_norms * target_norm + 1e-8)\n",
        "    # 5. Use torch.topk() to find top k highest scores\n",
        "    topk_vals, topk_idx = torch.topk(cosine_sims, n)\n",
        "    # 6. Filter out excluded_words and return top n results as (word, score) tuples\n",
        "    nearest = [(words[i], topk_vals[j].item()) for j, i in enumerate(topk_idx) if words[i] not in exclude_words]\n",
        "    return nearest\n",
        "\n",
        "    # Task 1.2: END STUDENT CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmYBy6cJmX6E",
        "outputId": "2a7b6514-8798-4aa4-d025-36cbd795935e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-16 09:51:55--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-12-16 09:51:55--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-12-16 09:51:55--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.09MB/s    in 2m 40s  \n",
            "\n",
            "2025-12-16 09:54:35 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: data/glove.6B.50d.txt   \n",
            "  inflating: data/glove.6B.100d.txt  \n",
            "  inflating: data/glove.6B.200d.txt  \n",
            "  inflating: data/glove.6B.300d.txt  \n"
          ]
        }
      ],
      "source": [
        "# Download GLOVE - you only need to do this once.\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d data/\n",
        "\n",
        "# If this fails, you can download manually from:\n",
        "# https://nlp.stanford.edu/data/glove.6B.zip\n",
        "# and extract the zip file into the /data directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhRXbmv-mX6E",
        "outputId": "e2da6359-df07-4d14-cb2b-fbfe94948f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe from data/glove.6B.100d.txt...\n",
            "Loaded 400000 words.\n",
            "\n",
            "--- Cosine Similarity Between Word Pairs ---\n",
            "  king         - monarch     : 0.6978\n",
            "  love         - affection   : 0.6255\n",
            "  war          - peace       : 0.6155\n",
            "  love         - hate        : 0.5704\n",
            "  doctor       - nurse       : 0.7522\n",
            "  poison       - dagger      : 0.3359\n",
            "  romeo        - juliet      : 0.6607\n",
            "  tragedy      - comedy      : 0.3790\n",
            "\n",
            "--- Word Analogies ---\n",
            "\n",
            "  king - man + woman = ?\n",
            "    queen: 0.7834\n",
            "    monarch: 0.6934\n",
            "    throne: 0.6833\n",
            "    daughter: 0.6809\n",
            "\n",
            "  paris - france + italy = ?\n",
            "    rome: 0.8084\n",
            "    milan: 0.7317\n",
            "    naples: 0.7090\n",
            "    venice: 0.7010\n",
            "\n",
            "  father - man + woman = ?\n",
            "    mother: 0.9137\n",
            "    daughter: 0.8749\n",
            "    wife: 0.8636\n",
            "    husband: 0.8385\n"
          ]
        }
      ],
      "source": [
        "# Load embeddings and explore relationships\n",
        "\n",
        "if os.path.exists(GLOVE_PATH):\n",
        "    embeddings = load_glove(GLOVE_PATH)\n",
        "\n",
        "    # Explore semantic relationships\n",
        "    pairs = [\n",
        "        (\"king\", \"monarch\"), (\"love\", \"affection\"),  # Synonyms\n",
        "        (\"war\", \"peace\"), (\"love\", \"hate\"),  # Antonyms\n",
        "        (\"doctor\", \"nurse\"), (\"poison\", \"dagger\"),  # Semantic fields\n",
        "        (\"romeo\", \"juliet\"), (\"tragedy\", \"comedy\")  # Shakespeare-related\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Cosine Similarity Between Word Pairs ---\")\n",
        "    for w1, w2 in pairs:\n",
        "        if w1 in embeddings and w2 in embeddings:\n",
        "            sim = cosine_similarity(embeddings[w1], embeddings[w2])\n",
        "            print(f\"  {w1:12} - {w2:12}: {sim:.4f}\")\n",
        "        else:\n",
        "            missing = [w for w in [w1, w2] if w not in embeddings]\n",
        "            print(f\"  Missing: {missing}\")\n",
        "\n",
        "    # Analogies\n",
        "    print(\"\\n--- Word Analogies ---\")\n",
        "\n",
        "    def solve_analogy(pos1, neg1, pos2):\n",
        "        \"\"\"Solve: pos1 - neg1 + pos2 = ?\"\"\"\n",
        "        print(f\"\\n  {pos1} - {neg1} + {pos2} = ?\")\n",
        "        if all(w in embeddings for w in [pos1, neg1, pos2]):\n",
        "            vec = embeddings[pos1] - embeddings[neg1] + embeddings[pos2]\n",
        "            neighbors = find_nearest(embeddings, vec, n=5, exclude_words=[pos1, neg1, pos2])\n",
        "            for word, score in neighbors:\n",
        "                print(f\"    {word}: {score:.4f}\")\n",
        "        else:\n",
        "            print(\"    Word(s) not in vocabulary.\")\n",
        "\n",
        "    solve_analogy(\"king\", \"man\", \"woman\")  # Expected: queen\n",
        "    solve_analogy(\"paris\", \"france\", \"italy\")  # Expected: rome\n",
        "    solve_analogy(\"father\", \"man\", \"woman\")  # Expected: mother\n",
        "else:\n",
        "    print(f\"GloVe file not found at {GLOVE_PATH}\")\n",
        "    print(\"Please download from: https://nlp.stanford.edu/data/glove.6B.zip\")\n",
        "    print(\"Extract glove.6B.100d.txt to the data/ directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-kj6IzRJQkR"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 1: Character-Level Language Modeling\n",
        "\n",
        "In this stage, you will build your first **neural language model** over Shakespeare's text using a **character-level recurrent neural network (RNN)** in PyTorch. You will:\n",
        "\n",
        "- Construct a character-level dataset from one work\n",
        "- Implement and train an RNN-based language model (on GPU if available)\n",
        "- Generate text using greedy decoding and temperature sampling\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 1.1 – Character Vocabulary and Sequential Dataset\n",
        "\n",
        "### Description\n",
        "\n",
        "You will construct a **character-level representation** of a Shakespeare work and prepare a dataset for **next-character prediction**.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Build a **character vocabulary** from raw text\n",
        "- Map characters to integer indices and back\n",
        "- Prepare sliding-window input–target pairs for sequence modeling\n",
        "- Wrap the data in a PyTorch `Dataset` and `DataLoader`\n",
        "\n",
        "### Task\n",
        "\n",
        "Implement a PyTorch Dataset class that implements the usual functions; \\_\\_init\\_\\_, \\_\\_len\\_\\_, and \\_\\_getitem\\_\\_. The init function needs to take the text corpus as a string and a sequence length as an integer. The getitem function takes an index integer as usual and returns a number of characters (sequence length) starting at that index in the text corpus - this is the data - and a target that has the same length, but which is offset by one to the right. For example, if the corpus is the text `Hello, I am a dog.`, then the output at sequence length 5 and index 0 would be `Hello` (data) and `ello,` (target). **(3 x 0.5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0CGVClVmJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 1.1: Character Vocabulary and Sequential Dataset\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for character-level language modeling.\n",
        "\n",
        "    Creates input-target pairs using a sliding window over the text.\n",
        "    Input: characters from position t to t + seq_len - 1\n",
        "    Target: characters from position t + 1 to t + seq_len\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text: str, seq_len: int):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            text: The full text as a string\n",
        "            seq_len: Length of each sequence (number of characters)\n",
        "        \"\"\"\n",
        "        # Task 1.3: START STUDENT CODE\n",
        "\n",
        "        # HINT:\n",
        "        # 1. Store text and seq_len\n",
        "        self.text = text\n",
        "        self.seq_len = seq_len\n",
        "        # 2. Build character set: unique chars from text, sorted for consistency\n",
        "        self.chars = sorted(list(set(text)))\n",
        "        self.vocab_size = len(self.chars)\n",
        "        # 3. Create bidirectional mappings: char_to_idx and idx_to_char\n",
        "        self.char_to_idx = {ch: idx for idx, ch in enumerate(self.chars)}\n",
        "        self.idx_to_char = {idx: ch for idx, ch in enumerate(self.chars)}\n",
        "        # 4. Encode entire text as tensor of indices using char_to_idx\n",
        "        self.data = torch.tensor(\n",
        "            [self.char_to_idx[ch] for ch in text],\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        # Task 1.3: END STUDENT CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of samples we can create from the text.\"\"\"\n",
        "        # Task 1.4: START STUDENT CODE\n",
        "\n",
        "        # HINT: Each sample needs seq_len + 1 characters (seq_len for input, last one for target)\n",
        "        # So max samples = len(self.data) - seq_len\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "        # Task 1.4: END STUDENT CODE\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single sample.\n",
        "\n",
        "        Returns:\n",
        "            input_seq: tensor of shape [seq_len]\n",
        "            target_seq: tensor of shape [seq_len]\n",
        "        \"\"\"\n",
        "        # Task 1.5: START STUDENT CODE\n",
        "\n",
        "        # HINT: Implement sliding window for next-character prediction\n",
        "        # 1. Extract seq_len + 1 characters starting at idx\n",
        "        chunk = self.data[idx : idx + self.seq_len + 1]\n",
        "        # 2. Split into input (first seq_len) and target (last seq_len, shifted by 1)\n",
        "        input_seq = chunk[:-1]\n",
        "        target_seq = chunk[1:]\n",
        "        # 3. Return both as tensors\n",
        "        return input_seq, target_seq\n",
        "\n",
        "        # Task 1.5: END STUDENT CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2IFopP4mX6G",
        "outputId": "8bacc55d-8073-4189-b742-10c840c7f3e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 1.1: Character Dataset ---\n",
            "Loaded text length: 142446 characters\n",
            "Vocabulary Size: 70\n",
            "Characters: '\\n !&,-.:;?ABCDEFGHIJKLMNOPQRSTUVWYZ[]_abcdefghijklmnopqrstuvwxyzæ—‘’“”'\n",
            "Number of samples: 142346\n",
            "\n",
            "Batch Input Shape: torch.Size([64, 100])\n",
            "Batch Target Shape: torch.Size([64, 100])\n",
            "\n",
            "Sample Input (first 50 chars): ' And therefore, if you should deal double with\\nher'\n",
            "Sample Target (first 50 chars): 'And therefore, if you should deal double with\\nher,'\n",
            "\n",
            "--- Design Notes ---\n",
            "seq_len=100: Captures sufficient context (approx 1-2 lines of verse)\n",
            "batch_size=64: Good balance between efficiency and memory usage\n"
          ]
        }
      ],
      "source": [
        "# Test the CharDataset\n",
        "print(\"--- Exercise 1.1: Character Dataset ---\")\n",
        "\n",
        "# Load Romeo and Juliet (or another work)\n",
        "work_path = os.path.join(WORKS_DIR, 'the_tragedy_of_romeo_and_juliet.txt')\n",
        "if not os.path.exists(work_path):\n",
        "    print(\"Work file not found. Please run Exercise 0.1 first.\")\n",
        "else:\n",
        "    with open(work_path, 'r', encoding='utf-8') as f:\n",
        "        char_text = f.read()\n",
        "\n",
        "    print(f\"Loaded text length: {len(char_text)} characters\")\n",
        "\n",
        "    # Create dataset\n",
        "    # seq_len=100 captures roughly 1-2 lines of verse - good context for learning structure\n",
        "    CHAR_SEQ_LEN = 100\n",
        "    CHAR_BATCH_SIZE = 64\n",
        "\n",
        "    char_dataset = CharDataset(char_text, CHAR_SEQ_LEN)\n",
        "    print(f\"Vocabulary Size: {char_dataset.vocab_size}\")\n",
        "    print(f\"Characters: {repr(''.join(char_dataset.char_to_idx.keys()))}\")\n",
        "    print(f\"Number of samples: {len(char_dataset)}\")\n",
        "\n",
        "    # Create DataLoader\n",
        "    char_dataloader = DataLoader(char_dataset, batch_size=CHAR_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Verify with one batch\n",
        "    inputs, targets = next(iter(char_dataloader))\n",
        "    print(f\"\\nBatch Input Shape: {inputs.shape}\")\n",
        "    print(f\"Batch Target Shape: {targets.shape}\")\n",
        "\n",
        "    # Show a sample\n",
        "    sample_input = \"\".join([char_dataset.idx_to_char[i.item()] for i in inputs[0]])\n",
        "    sample_target = \"\".join([char_dataset.idx_to_char[i.item()] for i in targets[0]])\n",
        "    print(f\"\\nSample Input (first 50 chars): {repr(sample_input[:50])}\")\n",
        "    print(f\"Sample Target (first 50 chars): {repr(sample_target[:50])}\")\n",
        "\n",
        "    print(f\"\\n--- Design Notes ---\")\n",
        "    print(f\"seq_len={CHAR_SEQ_LEN}: Captures sufficient context (approx 1-2 lines of verse)\")\n",
        "    print(f\"batch_size={CHAR_BATCH_SIZE}: Good balance between efficiency and memory usage\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2soyETvJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 1.2 – Character-Level RNN Language Model in PyTorch\n",
        "\n",
        "### Description\n",
        "\n",
        "You will implement and train a **character-level RNN-based language model** using the dataset from Exercise 1.1. The model will learn to predict the next character given the previous characters.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Define a simple **recurrent neural network** for language modeling\n",
        "- Use an **embedding layer**, an `nn.RNN`, and a linear output layer\n",
        "- Train a neural language model with **cross-entropy loss**\n",
        "- Run training on **GPU** where available\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "```\n",
        "Input (char indices) → Embedding → RNN → Linear → Output (vocab logits)\n",
        "```\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Set up device selection (GPU if available)\n",
        "2. Implement the `CharRNNLM` model class **(1 point)**\n",
        "3. Train the model with cross-entropy loss and Adam optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdhl6pY8JQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 1.2: Character-Level RNN Language Model\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class CharRNNLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level RNN Language Model.\n",
        "\n",
        "    Architecture:\n",
        "    - Embedding layer: maps character indices to dense vectors\n",
        "    - RNN layer: processes sequences and maintains hidden state\n",
        "    - Linear layer: maps hidden states to vocabulary logits\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_size: int):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Number of unique characters\n",
        "            emb_dim: Dimension of character embeddings\n",
        "            hidden_size: Number of hidden units in RNN\n",
        "        \"\"\"\n",
        "        # Task 1.6: START STUDENT CODE\n",
        "\n",
        "        # HINT: Build a simple RNN-based language model\n",
        "        # 1. Call super().__init__()\n",
        "        super().__init__()\n",
        "        # 2. Create embedding layer: nn.Embedding(vocab_size, emb_dim)\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim)\n",
        "        # 3. Create RNN layer: nn.RNN(emb_dim, hidden_size, batch_first=True)\n",
        "        self.rnn = nn.RNN(input_size=emb_dim, hidden_size=hidden_size, batch_first=True)\n",
        "        # 4. Create linear output layer: nn.Linear(hidden_size, vocab_size)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Task 1.6: END STUDENT CODE\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len)\n",
        "            hidden: Optional initial hidden state\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits of shape (batch, seq_len, vocab_size)\n",
        "            hidden: Final hidden state\n",
        "        \"\"\"\n",
        "        # Task 1.7: START STUDENT CODE\n",
        "\n",
        "        # HINT: Forward pass through the model:\n",
        "        # 1. Embed the input indices\n",
        "        embeds = self.embedding(x)\n",
        "        # 2. Pass through RNN to get hidden states\n",
        "        rnn_out, hidden = self.rnn(embeds, hidden)\n",
        "        # 3. Pass hidden states through linear layer to get logits\n",
        "        logits = self.fc(rnn_out)\n",
        "        # 4. Return logits and final hidden state\n",
        "        return logits, hidden\n",
        "\n",
        "        # Task 1.7: END STUDENT CODE\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T903E9MSJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 1.2 (continued): Training the Character RNN\n",
        "\n",
        "print(\"--- Exercise 1.2: Training Character RNN ---\")\n",
        "\n",
        "# Model hyperparameters\n",
        "CHAR_EMB_DIM = 64\n",
        "CHAR_HIDDEN_SIZE = 256\n",
        "CHAR_EPOCHS = 5\n",
        "CHAR_LR = 0.002\n",
        "\n",
        "# Create model\n",
        "char_model = CharRNNLM(\n",
        "    vocab_size=char_dataset.vocab_size,\n",
        "    emb_dim=CHAR_EMB_DIM,\n",
        "    hidden_size=CHAR_HIDDEN_SIZE\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in char_model.parameters()):,} parameters\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(char_model.parameters(), lr=CHAR_LR)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Training Notes ---\")\n",
        "print(\"The loss should decrease over epochs, indicating the model is learning.\")\n",
        "print(\"If loss doesn't decrease, try: lower learning rate, more epochs, or larger model.\")\n",
        "\n",
        "# Training loop\n",
        "char_model.train()\n",
        "for epoch in range(CHAR_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(char_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass (hidden state starts as None/zeros)\n",
        "        logits, _ = char_model(inputs)\n",
        "\n",
        "        # Reshape for loss: (batch * seq_len, vocab_size) vs (batch * seq_len)\n",
        "        loss = criterion(logits.view(-1, char_dataset.vocab_size), targets.view(-1))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(char_model.parameters(), 5)  # Gradient clipping for stability\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(char_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{CHAR_EPOCHS}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save model checkpoint\n",
        "char_model_path = \"char_rnn_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': char_model.state_dict(),\n",
        "    'vocab_size': char_dataset.vocab_size,\n",
        "    'emb_dim': CHAR_EMB_DIM,\n",
        "    'hidden_size': CHAR_HIDDEN_SIZE,\n",
        "    'char_to_idx': char_dataset.char_to_idx,\n",
        "    'idx_to_char': char_dataset.idx_to_char,\n",
        "}, char_model_path)\n",
        "print(f\"\\nModel saved to {char_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAhqSmaVJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 1.3 – Text Generation and Temperature Sampling\n",
        "\n",
        "### Description\n",
        "\n",
        "You will implement text generation functions for your character-level language model and experiment with different **sampling strategies**, including **greedy decoding** and **temperature-scaled sampling**.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Generate text autoregressively from a trained character-level language model\n",
        "- Implement **greedy decoding** and observe its limitations\n",
        "- Implement **temperature-based sampling** from a categorical distribution\n",
        "- Qualitatively compare generated outputs at different temperatures\n",
        "\n",
        "### Temperature Explained\n",
        "\n",
        "- **Temperature = 0 (or very low)**: Greedy - always picks most likely character. Repetitive but \"safe\".\n",
        "- **Temperature = 1.0**: Standard sampling from the learned distribution.\n",
        "- **Temperature > 1.0**: More random/creative, but may produce nonsense.\n",
        "- **Temperature < 1.0**: More focused/conservative, less variety.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Implement a `generate_greedy` function - This function should generate a string of predefined length, given a model and a start_text string. **(1 point)**\n",
        "2. Implement a `generate_with_temperature` function - This function should generate a string of predefined length, given a model and a start_text string, but the generation of each new character is up to chance. The probabilities that a possible new character is selected is based on the temperature-scaled logits (our normal outputs times the temperature factor). **(1 point)**\n",
        "3. Compare outputs at different temperatures. **(0.5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zTu0R_jJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 1.3: Text Generation and Temperature Sampling\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_greedy(model, start_text: str, length: int, char_to_idx: dict,\n",
        "                    idx_to_char: dict, device='cpu') -> str:\n",
        "    \"\"\"\n",
        "    Generate text using greedy decoding (always pick most likely next character).\n",
        "\n",
        "    Args:\n",
        "        model: Trained CharRNNLM model\n",
        "        start_text: Initial prompt text\n",
        "        length: Number of characters to generate\n",
        "        char_to_idx: Character to index mapping\n",
        "        idx_to_char: Index to character mapping\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Generated text string (including prompt)\n",
        "    \"\"\"\n",
        "    # Task 1.8: START STUDENT CODE\n",
        "\n",
        "    # HINT: Implement greedy decoding for text generation\n",
        "    # 1. Set model to eval mode and move to device\n",
        "    # 2. Encode the start_text as character indices (handle unknown with fallback)\n",
        "    # 3. Process prompt through model to initialize hidden state\n",
        "    # 4. Loop for 'length' iterations:\n",
        "    #    - Pick character with highest probability (torch.argmax)\n",
        "    #    - Append to generated_text\n",
        "    #    - Feed single character to model for next step\n",
        "    # 5. Return generated text\n",
        "    pass\n",
        "\n",
        "    # Task 1.8: END STUDENT CODE\n",
        "\n",
        "\n",
        "def generate_with_temperature(model, start_text: str, length: int, temperature: float,\n",
        "                              char_to_idx: dict, idx_to_char: dict, device='cpu') -> str:\n",
        "    \"\"\"\n",
        "    Generate text using temperature-scaled sampling.\n",
        "\n",
        "    Args:\n",
        "        model: Trained CharRNNLM model\n",
        "        start_text: Initial prompt text\n",
        "        length: Number of characters to generate\n",
        "        temperature: Sampling temperature (higher = more random)\n",
        "        char_to_idx: Character to index mapping\n",
        "        idx_to_char: Index to character mapping\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Generated text string (including prompt)\n",
        "    \"\"\"\n",
        "    # Task 1.9: START STUDENT CODE\n",
        "\n",
        "    # HINT: Implement temperature-scaled sampling for more creative text\n",
        "    # 1. Set model to eval mode and move to device\n",
        "    # 2. Encode start_text and process through model\n",
        "    # 3. Loop for 'length' iterations:\n",
        "    #    - If temperature very small (~0): use greedy (torch.argmax)\n",
        "    #    - Otherwise: scale logits by temperature, softmax to probabilities, sample\n",
        "    #    - Use torch.multinomial() to sample from probability distribution\n",
        "    #    - Append character and feed to model\n",
        "    # 4. Return generated text\n",
        "    pass\n",
        "\n",
        "    # Task 1.9: END STUDENT CODE\n",
        "\n",
        "# Test generation\n",
        "print(\"--- Exercise 1.3: Text Generation ---\")\n",
        "\n",
        "prompts = [\"ROMEO.\", \"The \"]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print('='*60)\n",
        "\n",
        "    print(\"\\n--- Greedy Decoding ---\")\n",
        "    greedy_text = generate_greedy(\n",
        "        char_model, prompt, 200,\n",
        "        char_dataset.char_to_idx, char_dataset.idx_to_char, device\n",
        "    )\n",
        "    print(greedy_text)\n",
        "\n",
        "    for temp in [0.5, 1.0, 2.0]:\n",
        "        print(f\"\\n--- Temperature {temp} ---\")\n",
        "        temp_text = generate_with_temperature(\n",
        "            char_model, prompt, 200, temp,\n",
        "            char_dataset.char_to_idx, char_dataset.idx_to_char, device\n",
        "        )\n",
        "        print(temp_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QpE0X_HJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 2: Word-Level Language Modeling & Theatrical Chat Interface\n",
        "\n",
        "In this stage, you will move from **character-level** to **word-level** language modeling. You will:\n",
        "\n",
        "- Build a word-level vocabulary and dataset from Shakespeare's works\n",
        "- Implement and train a **word-level RNN language model**\n",
        "- Construct a simple **theatrical chat interface** that simulates dialog between characters\n",
        "- Create a **turn-aware** model with special end-of-turn tokens\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 2.1 – Word-Level Vocabulary and Sequential Dataset\n",
        "\n",
        "### Description\n",
        "\n",
        "You will construct a **word-level representation** of Shakespeare's text and prepare a dataset for **next-word prediction**.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Extend a tokenizer for word-level modeling\n",
        "- Build a **word vocabulary** with frequency cutoffs\n",
        "- Handle out-of-vocabulary words with `<UNK>` token\n",
        "- Prepare sliding-window input-target pairs for next-word prediction\n",
        "\n",
        "### Task\n",
        "\n",
        "Instead of a character-based dataset, we now transition to a word-based dataset. The methods are the same as before, except we now split the text into words and limit ourselves to a vocabulary of a fixed size `vocab_size`, which should consist of the most common tokens in the corpus (hint: use a Counter). The vocabulary should also contain an `unk_token = \"<UNK>\"`, which is our stand-in for any future words we do not know (either rare tokens from this corpus or other texts).\n",
        "\n",
        "1. Build the vocabulary. **(0.5 points)**\n",
        "2. Build `<UNK>` handling. **(0.5 points)**\n",
        "3. Build the rest of the dataset. **(0.5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZgTNZqYJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.1: Word-Level Vocabulary and Sequential Dataset\n",
        "\n",
        "# You can reuse the tokenize function from Exercise 0.2 (already defined above)\n",
        "\n",
        "class WordDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for word-level language modeling.\n",
        "\n",
        "    Creates input-target pairs using a sliding window over tokenized text.\n",
        "    Handles vocabulary building and out-of-vocabulary words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text: str, seq_len: int, vocab_size: int = 30000):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            text: The full text as a string\n",
        "            seq_len: Length of each sequence (number of words)\n",
        "            vocab_size: Maximum vocabulary size (most frequent words)\n",
        "        \"\"\"\n",
        "        # Task 2.1: START STUDENT CODE\n",
        "\n",
        "        # HINT:\n",
        "        # 1. Store seq_len and tokenize the text\n",
        "        # 2. Build vocabulary: count token frequencies, keep most common up to vocab_size\n",
        "        # 3. Create bidirectional mappings with an <UNK> token for OOV words\n",
        "        # 4. Encode all tokens to indices\n",
        "        pass\n",
        "\n",
        "        # Task 2.1: END STUDENT CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        # Task 2.2: START STUDENT CODE\n",
        "        \"\"\"Number of samples available.\"\"\"\n",
        "\n",
        "        # HINT: Similar to CharDataset, return the number of valid sequences\n",
        "        pass\n",
        "\n",
        "        # Task 2.2: END STUDENT CODE\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Task 2.3: START STUDENT CODE\n",
        "        \"\"\"Get a single sample (input, target pair).\"\"\"\n",
        "\n",
        "        # HINT: Implement sliding window for next-word prediction (same as character level)\n",
        "        pass\n",
        "\n",
        "        # Task 2.3: END STUDENT CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yh_5zKDmX6I"
      },
      "outputs": [],
      "source": [
        "# Load the full corpus for word-level modeling\n",
        "print(\"--- Exercise 2.1: Word-Level Dataset ---\")\n",
        "print(\"\\nLoading full Shakespeare corpus...\")\n",
        "\n",
        "all_text = []\n",
        "work_files = sorted([f for f in os.listdir(WORKS_DIR) if f.endswith('.txt')])\n",
        "# use a selective works if training takes too long\n",
        "work_files = [f for f in work_files if \"romeo\" in f]\n",
        "\n",
        "print(f\"Found {len(work_files)} works\")\n",
        "\n",
        "for filename in work_files:\n",
        "    with open(os.path.join(WORKS_DIR, filename), 'r', encoding='utf-8') as f:\n",
        "        all_text.append(f.read())\n",
        "\n",
        "full_corpus = \"\\n\".join(all_text)\n",
        "print(f\"Total corpus length: {len(full_corpus):,} characters\")\n",
        "\n",
        "# Create dataset\n",
        "WORD_SEQ_LEN = 100  # 100 words of context\n",
        "WORD_BATCH_SIZE = 64\n",
        "WORD_VOCAB_SIZE = 30000\n",
        "\n",
        "word_dataset = WordDataset(full_corpus, WORD_SEQ_LEN, vocab_size=WORD_VOCAB_SIZE)\n",
        "word_dataloader = DataLoader(word_dataset, batch_size=WORD_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Verify\n",
        "inputs, targets = next(iter(word_dataloader))\n",
        "print(f\"\\nBatch shapes: Input {inputs.shape}, Target {targets.shape}\")\n",
        "\n",
        "# Decode sample\n",
        "sample_input = ' '.join([word_dataset.idx_to_word[i.item()] for i in inputs[0][:20]])\n",
        "print(f\"\\nSample input (first 20 words): {sample_input}...\")\n",
        "\n",
        "print(f\"\\n--- Design Notes ---\")\n",
        "print(f\"seq_len={WORD_SEQ_LEN}: Longer context window for word-level modeling\")\n",
        "print(f\"vocab_size={WORD_VOCAB_SIZE}: Accommodates Shakespeare's vocabulary\")\n",
        "print(f\"<UNK> handling: Rare words mapped to single UNK token\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEEDPce4JQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 2.2 – Word-Level RNN Language Model in PyTorch\n",
        "\n",
        "### Description\n",
        "\n",
        "You will implement and train a **word-level RNN-based language model** using the dataset from Exercise 2.1. The model will learn to predict the next word given a sequence of preceding words.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Define a word-level recurrent neural language model\n",
        "- Use larger embedding dimensions appropriate for word-level modeling\n",
        "- Train with cross-entropy loss on next-word prediction\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Define the `WordRNNLM` model class (similar to CharRNNLM but for words). **(1 point)**\n",
        "2. Train for multiple epochs on the full corpus and save model checkpoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmlCO9-nJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.2: Word-Level RNN Language Model\n",
        "\n",
        "\n",
        "class WordRNNLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Word-level RNN Language Model.\n",
        "\n",
        "    Similar architecture to CharRNNLM but with:\n",
        "    - Larger embedding dimensions (words need more representation capacity)\n",
        "    - Multiple RNN layers for better modeling\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_size: int, num_layers: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Number of words in vocabulary\n",
        "            emb_dim: Dimension of word embeddings\n",
        "            hidden_size: Number of hidden units in RNN\n",
        "            num_layers: Number of stacked RNN layers\n",
        "        \"\"\"\n",
        "        # Task 2.4: START STUDENT CODE\n",
        "\n",
        "        # HINT: Build a word-level RNN model (similar to CharRNNLM but with num_layers param)\n",
        "        pass\n",
        "\n",
        "        # Task 2.4: END STUDENT CODE\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len)\n",
        "            hidden: Optional initial hidden state\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits of shape (batch, seq_len, vocab_size)\n",
        "            hidden: Final hidden state\n",
        "        \"\"\"\n",
        "        # Task 2.5: START STUDENT CODE\n",
        "\n",
        "        # HINT: Forward pass through RNN (same as CharRNNLM)\n",
        "        pass\n",
        "\n",
        "        # Task 2.5: END STUDENT CODE\n",
        "\n",
        "print(\"--- Exercise 2.2: Word-Level RNN Model ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_hZTPptJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.2 (continued): Training Word RNN\n",
        "\n",
        "# Model hyperparameters - larger than char model\n",
        "WORD_EMB_DIM = 300\n",
        "WORD_HIDDEN_SIZE = 512\n",
        "WORD_NUM_LAYERS = 3\n",
        "WORD_EPOCHS = 3  # Fewer epochs due to larger corpus\n",
        "WORD_LR = 0.001\n",
        "\n",
        "# Create model\n",
        "word_model = WordRNNLM(\n",
        "    vocab_size=word_dataset.vocab_size,\n",
        "    emb_dim=WORD_EMB_DIM,\n",
        "    hidden_size=WORD_HIDDEN_SIZE,\n",
        "    num_layers=WORD_NUM_LAYERS\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in word_model.parameters()):,} parameters\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(word_model.parameters(), lr=WORD_LR)\n",
        "\n",
        "# Training loop\n",
        "total_batches = len(word_dataloader)\n",
        "word_model.train()\n",
        "\n",
        "for epoch in range(WORD_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(word_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = word_model(inputs)\n",
        "        loss = criterion(logits.view(-1, word_dataset.vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(word_model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Progress logging\n",
        "        if i % 200 == 0:\n",
        "            pct = (i / total_batches) * 100\n",
        "            print(f\"Epoch {epoch+1}/{WORD_EPOCHS} | Batch {i}/{total_batches} ({pct:.1f}%) | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(word_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{WORD_EPOCHS} Complete | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save model\n",
        "word_model_path = \"word_rnn_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': word_model.state_dict(),\n",
        "    'vocab_size': word_dataset.vocab_size,\n",
        "    'emb_dim': WORD_EMB_DIM,\n",
        "    'hidden_size': WORD_HIDDEN_SIZE,\n",
        "    'num_layers': WORD_NUM_LAYERS,\n",
        "    'word_to_idx': word_dataset.word_to_idx,\n",
        "    'idx_to_word': word_dataset.idx_to_word,\n",
        "}, word_model_path)\n",
        "print(f\"\\nModel saved to {word_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujK9u-hdJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 2.3 – Theatrical Chat Interface with a Word-Level RNN\n",
        "\n",
        "### Description\n",
        "\n",
        "In this exercise, you will use your trained **word-level RNN language model** to build a simple **theatrical chat interface**. The model will be prompted with a speaker name and dialog, then continue the text in Shakespearean style.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Use a word-level language model for **prompt-based generation**\n",
        "- Design a simple **chat-style interface** around a language model\n",
        "- Control text generation via temperature sampling at the word level\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Implement a word-level generation function that detokenizes the generated text after generation. **(1 point)**\n",
        "2. Test with different theatrical prompts (ROMEO., JULIET., etc.) and different temperatures. **(0.5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I5a2L3YJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.3: Theatrical Chat Interface\n",
        "\n",
        "def detokenize(tokens: list) -> str:\n",
        "    \"\"\"\n",
        "    Convert a list of tokens back into readable text.\n",
        "    Handles punctuation attachment (no space before punctuation).\n",
        "\n",
        "    Args:\n",
        "        tokens: List of token strings\n",
        "\n",
        "    Returns:\n",
        "        Reconstructed text string\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    for t in tokens:\n",
        "        # Attach punctuation without leading space\n",
        "        if t in [\".\", \",\", \"?\", \"!\", \":\", \";\", \"'\"] or t.startswith(\"'\"):\n",
        "            if text:\n",
        "                text = text.rstrip() + t + \" \"\n",
        "            else:\n",
        "                text += t + \" \"\n",
        "        else:\n",
        "            text += t + \" \"\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def generate_words(model, start_text: str, max_tokens: int, temperature: float,\n",
        "                   word_to_idx: dict, idx_to_word: dict, device='cpu') -> str:\n",
        "    \"\"\"\n",
        "    Generate text at the word level with temperature sampling.\n",
        "\n",
        "    Args:\n",
        "        model: Trained WordRNNLM model\n",
        "        start_text: Initial prompt text\n",
        "        max_tokens: Maximum number of words to generate\n",
        "        temperature: Sampling temperature\n",
        "        word_to_idx: Word to index mapping\n",
        "        idx_to_word: Index to word mapping\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Generated text string (including prompt)\n",
        "    \"\"\"\n",
        "    # Task 2.6: START STUDENT CODE\n",
        "\n",
        "    # HINT: Word-level generation is similar to character-level but:\n",
        "    # 1. Tokenize the prompt using the tokenize() function\n",
        "    # 2. Map tokens to indices (handle UNK)\n",
        "    # 3. Generate words with temperature sampling (like Task 1.9)\n",
        "    # 4. Detokenize the result (reconstruct readable text with proper spacing)\n",
        "    pass\n",
        "\n",
        "    # Task 2.6: END STUDENT CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAK6CBQsmX6J"
      },
      "outputs": [],
      "source": [
        "# Test theatrical generation\n",
        "print(\"--- Exercise 2.3: Theatrical Chat Interface ---\")\n",
        "\n",
        "examples = [\n",
        "    (\"ROMEO\", \"My heart is heavy with unspoken words.\"),\n",
        "    (\"JULIET\", \"Good even, my lord. Why art thou troubled?\"),\n",
        "    (\"HAMLET\", \"To be, or not to be, that is the question.\")\n",
        "]\n",
        "\n",
        "for speaker, line in examples:\n",
        "    prompt = f\"{speaker}.\\n{line}\\n\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{speaker}: {line}\")\n",
        "    print('='*60)\n",
        "\n",
        "    for temp in [0.5, 0.8, 1.2]:\n",
        "        response = generate_words(\n",
        "            word_model, prompt, 50, temp,\n",
        "            word_dataset.word_to_idx, word_dataset.idx_to_word, device\n",
        "        )\n",
        "        print(f\"\\n[Temperature {temp}]\")\n",
        "        print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7nmjPezJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 2.4 – Turn-Based Modeling with Special Tokens\n",
        "\n",
        "### Description\n",
        "\n",
        "To build a realistic theatrical chat interface, the model needs to understand when a speaker's turn ends. In this exercise, you will create a specialized dataset that inserts a special **End-of-Turn** token (`<EOS>`) before every speaker change.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Preprocess text to explicitly model dialog structure (turns)\n",
        "- Use special tokens (`<EOS>`) to control generation length\n",
        "- Train a language model that learns to stop generating at appropriate times\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "By training with `<EOS>` markers before speaker changes, the model learns:\n",
        "1. When to stop generating (predict `<EOS>`)\n",
        "2. The natural rhythm of theatrical dialogue\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Implement a `insert_turn_markers()` to add `<EOS>` before speaker names. **(1 point)**\n",
        "2. Create a `TurnDataset` with the modified corpus and train a turn-aware model. **(1 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UwEeuDtJQkT"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.4: Turn-Based Modeling with Special Tokens\n",
        "\n",
        "def insert_turn_markers(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Insert <EOS> markers before speaker names in the text.\n",
        "\n",
        "    Speaker detection heuristic:\n",
        "    - Lines that are predominantly uppercase\n",
        "    - Short (< 30 characters)\n",
        "    - Not empty\n",
        "\n",
        "    Args:\n",
        "        text: Original text\n",
        "\n",
        "    Returns:\n",
        "        Text with <EOS> markers inserted before speaker names\n",
        "    \"\"\"\n",
        "    # Task 2.7: START STUDENT CODE\n",
        "\n",
        "    # HINT: Detect speaker names and insert <EOS> markers\n",
        "    # 1. Split text into lines\n",
        "    # 2. For each line, detect if it's a speaker name:\n",
        "    #    - Non-empty, short (< 30 chars), mostly uppercase\n",
        "    # 3. If speaker: insert \"<EOS>\" before the line\n",
        "    # 4. Rejoin lines\n",
        "    pass\n",
        "\n",
        "    # Task 2.7: END STUDENT CODE\n",
        "\n",
        "\n",
        "class TurnDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset with turn markers for dialogue modeling.\n",
        "\n",
        "    Similar to WordDataset but:\n",
        "    - Preprocesses text with <EOS> markers\n",
        "    - Ensures <EOS> token is in vocabulary\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text: str, seq_len: int, vocab_size: int = 30000):\n",
        "        # Task 2.8: START STUDENT CODE\n",
        "\n",
        "        # HINT: Similar to WordDataset but with turn markers:\n",
        "        # 1. Store seq_len\n",
        "        # 2. Insert <EOS> markers before speakers (use insert_turn_markers)\n",
        "        # 3. Replace <EOS> with \"eos_marker\" so tokenizer handles it properly\n",
        "        # 4. Tokenize and build vocabulary (reserve space for UNK and eos_marker)\n",
        "        # 5. Ensure both UNK and EOS tokens are in vocabulary\n",
        "        # 6. Encode all tokens\n",
        "        pass\n",
        "\n",
        "        # Task 2.8: END STUDENT CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        # Task 2.9: START STUDENT CODE\n",
        "\n",
        "        # HINT: Same as WordDataset\n",
        "        pass\n",
        "\n",
        "        # Task 2.9: END STUDENT CODE\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Task 2.10: START STUDENT CODE\n",
        "\n",
        "        # HINT: Same as WordDataset\n",
        "        pass\n",
        "\n",
        "        # Task 2.10: END STUDENT CODE\n",
        "\n",
        "# Create turn-aware dataset\n",
        "print(\"--- Exercise 2.4: Turn-Based Dataset ---\")\n",
        "\n",
        "turn_dataset = TurnDataset(full_corpus, WORD_SEQ_LEN, vocab_size=WORD_VOCAB_SIZE)\n",
        "turn_dataloader = DataLoader(turn_dataset, batch_size=WORD_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print(f\"\\nTurn Dataset Stats:\")\n",
        "print(f\"  Vocabulary size: {turn_dataset.vocab_size}\")\n",
        "print(f\"  EOS token index: {turn_dataset.word_to_idx[turn_dataset.eos_token]}\")\n",
        "print(f\"  Number of samples: {len(turn_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNLV0fxgJQkT"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.4 (continued): Training Turn-Aware Model\n",
        "\n",
        "print(\"--- Training Turn-Aware RNN ---\")\n",
        "\n",
        "# Create turn-aware model (same architecture, different vocab/data)\n",
        "turn_model = WordRNNLM(\n",
        "    vocab_size=turn_dataset.vocab_size,\n",
        "    emb_dim=WORD_EMB_DIM,\n",
        "    hidden_size=WORD_HIDDEN_SIZE,\n",
        "    num_layers=WORD_NUM_LAYERS\n",
        ").to(device)\n",
        "\n",
        "print(f\"Turn model created with {sum(p.numel() for p in turn_model.parameters()):,} parameters\")\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(turn_model.parameters(), lr=WORD_LR)\n",
        "\n",
        "# Training loop\n",
        "total_batches = len(turn_dataloader)\n",
        "turn_model.train()\n",
        "\n",
        "TURN_EPOCHS = 3\n",
        "\n",
        "for epoch in range(TURN_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(turn_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = turn_model(inputs)\n",
        "        loss = criterion(logits.view(-1, turn_dataset.vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(turn_model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            pct = (i / total_batches) * 100\n",
        "            print(f\"Epoch {epoch+1}/{TURN_EPOCHS} | Batch {i}/{total_batches} ({pct:.1f}%) | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(turn_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{TURN_EPOCHS} Complete | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save turn-aware model\n",
        "turn_model_path = \"turn_rnn_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': turn_model.state_dict(),\n",
        "    'vocab_size': turn_dataset.vocab_size,\n",
        "    'emb_dim': WORD_EMB_DIM,\n",
        "    'hidden_size': WORD_HIDDEN_SIZE,\n",
        "    'num_layers': WORD_NUM_LAYERS,\n",
        "    'word_to_idx': turn_dataset.word_to_idx,\n",
        "    'idx_to_word': turn_dataset.idx_to_word,\n",
        "    'eos_token': turn_dataset.eos_token,\n",
        "}, turn_model_path)\n",
        "print(f\"\\nTurn-aware model saved to {turn_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qMZJWxXmX6K"
      },
      "source": [
        "---\n",
        "\n",
        "## Stage 2.5 – Theatrical Chat Interface (Model Comparison)\n",
        "\n",
        "### Description\n",
        "\n",
        "You can now use your trained models to build a chat interface that supports **both** the standard Word-RNN (from Exercise 2.2) and the Turn-Aware RNN (from Exercise 2.4).\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "| Model | Generation Behavior |\n",
        "|-------|---------------------|\n",
        "| Standard Word-RNN | Generates exactly `max_tokens` words |\n",
        "| Turn-Aware RNN | Stops when `<EOS>` is generated |\n",
        "\n",
        "### Tasks\n",
        "\n",
        "No tasks, just execute the code below and play around with it, to get a feeling for the performance of the two variants.\n",
        "\n",
        "**The `generate_with_eos` function will be useful in the next tasks - you can and should use it, and should familiarize yourself with what it does.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6FZNM7_mX6K"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.5: Theatrical Chat Interface (Model Comparison)\n",
        "\n",
        "def generate_with_eos(model, start_text: str, max_tokens: int, temperature: float,\n",
        "                      word_to_idx: dict, idx_to_word: dict, eos_token: str = None,\n",
        "                      device='cpu') -> str:\n",
        "    \"\"\"\n",
        "    Generate text with optional EOS stopping.\n",
        "\n",
        "    Args:\n",
        "        model: Trained language model\n",
        "        start_text: Initial prompt\n",
        "        max_tokens: Maximum tokens to generate\n",
        "        temperature: Sampling temperature\n",
        "        word_to_idx: Word to index mapping\n",
        "        idx_to_word: Index to word mapping\n",
        "        eos_token: If provided, stop when this token is generated\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Generated text (excluding prompt tokens)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    hidden = None\n",
        "\n",
        "    # Preprocess prompt - handle <eos> placeholder\n",
        "    text_to_process = start_text.lower()\n",
        "    if eos_token and \"<eos>\" in text_to_process:\n",
        "        text_to_process = text_to_process.replace(\"<eos>\", eos_token)\n",
        "\n",
        "    tokens = tokenize(text_to_process)\n",
        "    unk_idx = word_to_idx.get(\"<UNK>\", 0)\n",
        "\n",
        "    input_indices = [word_to_idx.get(t, unk_idx) for t in tokens]\n",
        "    input_seq = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    # Get EOS index if applicable\n",
        "    eos_idx = word_to_idx.get(eos_token, -1) if eos_token else -1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, hidden = model(input_seq, hidden)\n",
        "        last_logits = logits[:, -1, :]\n",
        "\n",
        "        for _ in range(max_tokens):\n",
        "            if temperature <= 0:\n",
        "                idx = torch.argmax(last_logits, dim=-1).item()\n",
        "            else:\n",
        "                probs = F.softmax(last_logits / temperature, dim=-1)\n",
        "                idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # Check for EOS\n",
        "            if eos_token and idx == eos_idx:\n",
        "                break\n",
        "\n",
        "            word = idx_to_word.get(idx, \"<UNK>\")\n",
        "            generated_tokens.append(word)\n",
        "\n",
        "            input_seq = torch.tensor([[idx]], dtype=torch.long).to(device)\n",
        "            logits, hidden = model(input_seq, hidden)\n",
        "            last_logits = logits[:, -1, :]\n",
        "\n",
        "    return detokenize(generated_tokens)\n",
        "\n",
        "# Compare models\n",
        "print(\"--- Exercise 2.5: Model Comparison ---\")\n",
        "\n",
        "examples = [\n",
        "    (\"ROMEO\", \"My heart is heavy with unspoken words.\"),\n",
        "    (\"JULIET\", \"Good even, my lord. Why art thou troubled?\"),\n",
        "]\n",
        "\n",
        "for speaker, line in examples:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"PROMPT: {speaker}: {line}\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Standard model (fixed length)\n",
        "    standard_prompt = f\"{speaker}.\\n{line}\\n\"\n",
        "    standard_response = generate_with_eos(\n",
        "        word_model, standard_prompt, 50, 0.8,\n",
        "        word_dataset.word_to_idx, word_dataset.idx_to_word,\n",
        "        eos_token=None, device=device\n",
        "    )\n",
        "    print(f\"\\n[Standard Word-RNN (50 tokens)]\")\n",
        "    print(standard_response[:200] + \"...\" if len(standard_response) > 200 else standard_response)\n",
        "\n",
        "    # Turn-aware model (EOS stopping)\n",
        "    turn_prompt = f\"<eos>\\n{speaker}.\\n{line}\\n<eos>\\n\"\n",
        "    turn_response = generate_with_eos(\n",
        "        turn_model, turn_prompt, 200, 0.8,\n",
        "        turn_dataset.word_to_idx, turn_dataset.idx_to_word,\n",
        "        eos_token=turn_dataset.eos_token, device=device\n",
        "    )\n",
        "    print(f\"\\n[Turn-Aware RNN (EOS stopping)]\")\n",
        "    print(turn_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ih4uoNJQkT"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 3: Word-Level LSTM & RNN–LSTM Comparison\n",
        "\n",
        "In this final stage, you will extend your word-level language model by replacing the RNN with an **LSTM**. You will:\n",
        "\n",
        "- Implement and train a **word-level LSTM language model**\n",
        "- Plug the LSTM into your existing **theatrical chat interface**\n",
        "- Qualitatively compare the behavior of **RNN** vs **LSTM** models\n",
        "\n",
        "## Why LSTM?\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks address the **vanishing gradient problem** in standard RNNs:\n",
        "\n",
        "| Feature | RNN | LSTM |\n",
        "|---------|-----|------|\n",
        "| Memory | Short-term only | Long and short-term |\n",
        "| Gradient flow | Degrades over long sequences | Gates preserve gradients |\n",
        "| Training | Faster per step | More stable |\n",
        "| Parameters | Fewer | ~4x more (3 gates + cell) |\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 3.1 – Word-Level LSTM Language Model in PyTorch\n",
        "\n",
        "### Description\n",
        "\n",
        "You will modify your word-level language model by replacing the RNN layer with an LSTM.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Implement a **word-level LSTM-based language model**\n",
        "- Handle LSTM's dual hidden state `(h, c)`\n",
        "- Train on the same turn-aware dataset for fair comparison\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Define `WordLSTMLM` class with `nn.LSTM`. It has to fulfill the same conditions as normally, except this time, the model should return both the prediction and the LSTM's hidden state. **(1 point)**\n",
        "2. Train on the same dataset as the RNN and compare training behavior in terms of loss, stability, etc. **(1 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3KWm_YCJQkT"
      },
      "outputs": [],
      "source": [
        "# Exercise 3.1: Word-Level LSTM Language Model\n",
        "\n",
        "class WordLSTMLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Word-level LSTM Language Model.\n",
        "\n",
        "    Key difference from RNN:\n",
        "    - Uses nn.LSTM instead of nn.RNN\n",
        "    - Hidden state is a tuple (h_n, c_n) where:\n",
        "      - h_n: hidden state (same as RNN)\n",
        "      - c_n: cell state (LSTM's long-term memory)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_size: int, num_layers: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the LSTM model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Number of words in vocabulary\n",
        "            emb_dim: Dimension of word embeddings\n",
        "            hidden_size: Number of hidden units in LSTM\n",
        "            num_layers: Number of stacked LSTM layers\n",
        "        \"\"\"\n",
        "        # Task 3.1: START STUDENT CODE\n",
        "\n",
        "        # HINT: Build a word-level LSTM model (same as WordRNNLM but use nn.LSTM)\n",
        "        pass\n",
        "\n",
        "        # Task 3.1: END STUDENT CODE\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len)\n",
        "            hidden: Optional initial hidden state tuple (h_0, c_0)\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits of shape (batch, seq_len, vocab_size)\n",
        "            hidden: Final hidden state tuple (h_n, c_n)\n",
        "        \"\"\"\n",
        "        # Task 3.2: START STUDENT CODE\n",
        "\n",
        "        # HINT: Forward pass through LSTM (same as RNN forward pass)\n",
        "        pass\n",
        "\n",
        "        # Task 3.2: END STUDENT CODE\n",
        "\n",
        "\n",
        "print(\"--- Exercise 3.1: Word-Level LSTM Model ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKo0P-WXJQkT"
      },
      "outputs": [],
      "source": [
        "# Exercise 3.1 (continued): Training LSTM Model\n",
        "\n",
        "# Create LSTM model (same hyperparameters as RNN for fair comparison)\n",
        "lstm_model = WordLSTMLM(\n",
        "    vocab_size=turn_dataset.vocab_size,\n",
        "    emb_dim=WORD_EMB_DIM,\n",
        "    hidden_size=WORD_HIDDEN_SIZE,\n",
        "    num_layers=WORD_NUM_LAYERS\n",
        ").to(device)\n",
        "\n",
        "print(f\"LSTM model created with {sum(p.numel() for p in lstm_model.parameters()):,} parameters\")\n",
        "print(f\"(Compare to RNN: ~{sum(p.numel() for p in turn_model.parameters()):,} parameters)\")\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=WORD_LR)\n",
        "\n",
        "# Training loop\n",
        "total_batches = len(turn_dataloader)\n",
        "lstm_model.train()\n",
        "\n",
        "LSTM_EPOCHS = 3\n",
        "\n",
        "for epoch in range(LSTM_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(turn_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = lstm_model(inputs)\n",
        "        loss = criterion(logits.view(-1, turn_dataset.vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            pct = (i / total_batches) * 100\n",
        "            print(f\"Epoch {epoch+1}/{LSTM_EPOCHS} | Batch {i}/{total_batches} ({pct:.1f}%) | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(turn_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{LSTM_EPOCHS} Complete | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save LSTM model\n",
        "lstm_model_path = \"word_lstm_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': lstm_model.state_dict(),\n",
        "    'vocab_size': turn_dataset.vocab_size,\n",
        "    'emb_dim': WORD_EMB_DIM,\n",
        "    'hidden_size': WORD_HIDDEN_SIZE,\n",
        "    'num_layers': WORD_NUM_LAYERS,\n",
        "    'word_to_idx': turn_dataset.word_to_idx,\n",
        "    'idx_to_word': turn_dataset.idx_to_word,\n",
        "    'eos_token': turn_dataset.eos_token,\n",
        "}, lstm_model_path)\n",
        "print(f\"\\nLSTM model saved to {lstm_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MXouAiAJQkT"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 3.2 – LSTM Chat Interface and RNN–LSTM Comparison\n",
        "\n",
        "### Description\n",
        "\n",
        "You will now plug your **word-level LSTM language model** into the theatrical chat interface and compare its behavior to the **word-level RNN** using identical prompts and settings.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Use a word-level LSTM for **prompt-based generation**\n",
        "- Compare outputs of RNN vs LSTM models\n",
        "- Reflect on advantages and limitations of both architectures\n",
        "\n",
        "### Tasks\n",
        "\n",
        "Plug the LSTM model into the chat interface you made and compare to the RNN chat generation, using the following comparison criteria **(1 point)**:\n",
        "\n",
        "1. **Coherence**: Does the text make grammatical sense?\n",
        "2. **Dialog structure**: Does it follow speaker patterns?\n",
        "3. **Repetition**: Does one model repeat itself more?\n",
        "4. **Creativity**: Which produces more varied outputs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVlFecyyJQkT"
      },
      "outputs": [],
      "source": [
        "# Exercise 3.2: RNN vs LSTM Comparison\n",
        "\n",
        "print(\"--- Exercise 3.2: RNN vs LSTM Comparison ---\")\n",
        "\n",
        "# Test prompts\n",
        "comparison_prompts = [\n",
        "    (\"ROMEO\", \"My heart is heavy with unspoken words.\", \"JULIET\"),\n",
        "    (\"JULIET\", \"Good even, my lord. Why art thou troubled?\", \"ROMEO\"),\n",
        "    (\"HAMLET\", \"To be, or not to be, I ask again.\", \"HORATIO\"),\n",
        "]\n",
        "\n",
        "for user_speaker, user_line, model_speaker in comparison_prompts:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{user_speaker}: {user_line}\")\n",
        "    print(f\"(Response from: {model_speaker})\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Construct prompts\n",
        "    prompt = f\"<eos>\\n{user_speaker}.\\n{user_line}\\n<eos>\\n{model_speaker}.\\n\"\n",
        "\n",
        "    # RNN response\n",
        "    # Task 3.3: START STUDENT CODE\n",
        "\n",
        "    # HINT: Generate rnn_response using the turn_model (RNN) with the prompt\n",
        "    pass\n",
        "\n",
        "    # Task 3.3: END STUDENT CODE\n",
        "    print(f\"\\n[RNN] {model_speaker}:\")\n",
        "    print(rnn_response)\n",
        "\n",
        "    # LSTM response\n",
        "    # Task 3.4: START STUDENT CODE\n",
        "\n",
        "    # HINT: Generate lstm_response using the lstm_model (LSTM) with the same prompt\n",
        "    pass\n",
        "\n",
        "    # Task 3.4: END STUDENT CODE\n",
        "    print(f\"\\n[LSTM] {model_speaker}:\")\n",
        "    print(lstm_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Ydx96iJQkT"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 4: Fine-Tuning Modern Language Models with Hugging Face 🤗\n",
        "\n",
        "In this final stage, you will learn to use the **Hugging Face ecosystem** to fine-tune a modern pretrained language model for Shakespearean dialogue generation.\n",
        "\n",
        "## 📚 Introduction to Hugging Face\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) is the leading platform for machine learning, offering:\n",
        "\n",
        "- **🤗 Transformers**: A library with 200,000+ pretrained models\n",
        "- **📦 Datasets**: Easy-to-use datasets for ML\n",
        "- **🏋️ Trainer**: High-level API for training models\n",
        "- **🌐 Hub**: Share and discover models\n",
        "\n",
        "### Essential Resources\n",
        "\n",
        "| Resource | Link | Description |\n",
        "|----------|------|-------------|\n",
        "| HF Course | [huggingface.co/learn](https://huggingface.co/learn/nlp-course) | Free NLP course |\n",
        "| Transformers Docs | [huggingface.co/docs/transformers](https://huggingface.co/docs/transformers) | Official documentation |\n",
        "| Model Hub | [huggingface.co/models](https://huggingface.co/models) | Browse all models |\n",
        "\n",
        "## Model Choice: Qwen2.5-0.5B\n",
        "\n",
        "We use **Qwen2.5-0.5B** (released October 2024) because:\n",
        "- ✅ Modern architecture (late 2024 release)\n",
        "- ✅ Small but capable (494M parameters)\n",
        "- ✅ Fast to fine-tune on consumer GPUs\n",
        "- ✅ Strong base capabilities for text generation\n",
        "\n",
        "**Alternative**: If Qwen is unavailable, you can use `HuggingFaceTB/SmolLM2-360M` (135-360M params, Nov 2024).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30i8DiEjmX6L"
      },
      "source": [
        "## Exercise 4.1 – Introduction to Hugging Face Transformers\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Understand the core Hugging Face abstractions: **Tokenizer**, **Model**, **Trainer**\n",
        "- Load pretrained models from the Hugging Face Hub\n",
        "- Explore tokenization and understand how text becomes numbers\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────┐\n",
        "│                    Hugging Face Pipeline                    │\n",
        "├─────────────────────────────────────────────────────────────┤\n",
        "│  Text → [Tokenizer] → Token IDs → [Model] → Logits → Text  │\n",
        "└─────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "- **Tokenizer**: Converts text ↔ token IDs (integers)\n",
        "- **Model**: Neural network that processes token IDs\n",
        "- **AutoClass**: Automatically selects the right class for any model\n",
        "\n",
        "### 💡 Hint\n",
        "\n",
        "The `Auto` classes (AutoTokenizer, AutoModel) are magic! They detect the model type and load the correct implementation automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7QTpiywmX6L"
      },
      "outputs": [],
      "source": [
        "# Exercise 4.1: Introduction to Hugging Face\n",
        "\n",
        "# Step 1: Install required packages\n",
        "# Uncomment for Colab/fresh environment:\n",
        "# !pip install transformers datasets accelerate -q\n",
        "\n",
        "# Step 2: Import the core libraries\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "device = \"mps\"\n",
        "print(\"✅ Hugging Face Transformers imported successfully!\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "# ============================================================\n",
        "# UNDERSTANDING HUGGING FACE: Loading a Pretrained Model\n",
        "# ============================================================\n",
        "\n",
        "# Model identifier on Hugging Face Hub\n",
        "# Browse models at: https://huggingface.co/models?sort=trending\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"  # 494M params, released Oct 2024\n",
        "\n",
        "# Alternative smaller models (uncomment if needed):\n",
        "# MODEL_NAME = \"HuggingFaceTB/SmolLM2-360M\"  # 360M params, Nov 2024\n",
        "# MODEL_NAME = \"HuggingFaceTB/SmolLM2-135M\"  # 135M params, Nov 2024\n",
        "\n",
        "print(f\"\\n📥 Loading model: {MODEL_NAME}\")\n",
        "print(\"   (First run downloads the model, subsequent runs use cache)\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5p9lS7rmX6M"
      },
      "outputs": [],
      "source": [
        "# Exercise 4.1 (continued): Understanding Tokenizers\n",
        "\n",
        "# ============================================================\n",
        "# TOKENIZER: Converting Text ↔ Numbers\n",
        "# ============================================================\n",
        "\n",
        "# Load the tokenizer - this downloads vocabulary and tokenization rules\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Some models don't have a pad token - we set it to the EOS token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"✅ Tokenizer loaded!\")\n",
        "print(f\"   Vocabulary size: {len(tokenizer):,} tokens\")\n",
        "print(f\"   Model max length: {tokenizer.model_max_length}\")\n",
        "\n",
        "# ============================================================\n",
        "# EXPLORE: How tokenization works\n",
        "# ============================================================\n",
        "\n",
        "sample_text = \"Romeo, Romeo! Wherefore art thou Romeo?\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(sample_text)\n",
        "token_ids = tokenizer.encode(sample_text)\n",
        "\n",
        "print(f\"\\n📝 Sample text: '{sample_text}'\")\n",
        "print(f\"\\n🔤 Tokens ({len(tokens)}): {tokens}\")\n",
        "print(f\"\\n🔢 Token IDs: {token_ids}\")\n",
        "\n",
        "# Decode back to text\n",
        "decoded = tokenizer.decode(token_ids)\n",
        "print(f\"\\n🔙 Decoded: '{decoded}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VChO5yImX6M"
      },
      "outputs": [],
      "source": [
        "# Exercise 4.1 (continued): Loading the Model\n",
        "\n",
        "# ============================================================\n",
        "# MODEL: The Neural Network\n",
        "# ============================================================\n",
        "\n",
        "print(\"📥 Loading model (this may take a minute)...\\n\")\n",
        "\n",
        "# AutoModelForCausalLM = Auto Model for Causal Language Modeling\n",
        "# \"Causal\" means the model predicts the next token (like GPT)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"✅ Model loaded!\")\n",
        "print(f\"   Total parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Device: {next(model.parameters()).device}\")\n",
        "\n",
        "# ============================================================\n",
        "# TEST: Generate some text!\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🎭 Quick test - generating text...\")\n",
        "\n",
        "test_prompt = \"To be, or not to be\"\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=30,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nPrompt: '{test_prompt}'\")\n",
        "print(f\"Generated: '{generated}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urNJvg8tmX6M"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 4.2 – Preparing Data and Fine-Tuning\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Create a dataset compatible with Hugging Face `Trainer`\n",
        "- Add custom special tokens to mark dialogue structure\n",
        "- Configure and run fine-tuning with the `Trainer` API\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "**Special Tokens**: We add custom tokens to help the model understand dialogue:\n",
        "```\n",
        "<|speaker|>ROMEO<|endname|>My text here<|endturn|>\n",
        "```\n",
        "\n",
        "**Dataset**: Hugging Face uses Arrow format for efficient data loading.\n",
        "\n",
        "**Trainer**: High-level API that handles:\n",
        "- Training loop\n",
        "- Gradient accumulation\n",
        "- Mixed precision (FP16)\n",
        "- Checkpointing\n",
        "- Evaluation\n",
        "\n",
        "### 📖 Further Reading\n",
        "\n",
        "- [Fine-tuning tutorial](https://huggingface.co/docs/transformers/training)\n",
        "- [Trainer documentation](https://huggingface.co/docs/transformers/main_classes/trainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmQdD5JcmX6M"
      },
      "outputs": [],
      "source": [
        "# Exercise 4.2: Preparing the Shakespeare Dataset\n",
        "\n",
        "from datasets import Dataset\n",
        "import re\n",
        "\n",
        "# ============================================================\n",
        "# SPECIAL TOKENS: Marking dialogue structure\n",
        "# ============================================================\n",
        "\n",
        "SPEAKER_TOKEN = \"<|speaker|>\"\n",
        "ENDNAME_TOKEN = \"<|endname|>\"\n",
        "ENDTURN_TOKEN = \"<|endturn|>\"\n",
        "\n",
        "# Add special tokens to the tokenizer\n",
        "special_tokens = {\n",
        "    \"additional_special_tokens\": [SPEAKER_TOKEN, ENDNAME_TOKEN, ENDTURN_TOKEN]\n",
        "}\n",
        "num_added = tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "# IMPORTANT: Resize model embeddings to match new vocabulary\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(f\"✅ Added {num_added} special tokens\")\n",
        "print(f\"   New vocabulary size: {len(tokenizer):,}\")\n",
        "print(f\"   Format: {SPEAKER_TOKEN}NAME{ENDNAME_TOKEN}dialogue{ENDTURN_TOKEN}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwR6U11umX6M"
      },
      "outputs": [],
      "source": [
        "# Exercise 4.2 (continued): Extract dialogues from Shakespeare\n",
        "\n",
        "def extract_dialogues(text):\n",
        "    \"\"\"Extract (speaker, dialogue) pairs from Shakespeare text.\"\"\"\n",
        "    dialogues = []\n",
        "    lines = text.split('\\n')\n",
        "    current_speaker, current_lines = None, []\n",
        "\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        # Detect speaker: short, uppercase lines\n",
        "        if stripped and len(stripped) < 40:\n",
        "            alpha = re.sub(r'[^A-Za-z]', '', stripped)\n",
        "            if alpha and alpha.isupper() and len(alpha) > 1:\n",
        "                if current_speaker and current_lines:\n",
        "                    text = ' '.join(current_lines).strip()\n",
        "                    if len(text) > 20:\n",
        "                        dialogues.append((current_speaker, text))\n",
        "                current_speaker = stripped.rstrip('.').strip()\n",
        "                current_lines = []\n",
        "                continue\n",
        "        if current_speaker and stripped:\n",
        "            current_lines.append(stripped)\n",
        "\n",
        "    return dialogues\n",
        "\n",
        "def format_for_training(dialogues, context_turns=2):\n",
        "    \"\"\"Format dialogues with special tokens.\"\"\"\n",
        "    examples = []\n",
        "    for i in range(len(dialogues)):\n",
        "        start = max(0, i - context_turns)\n",
        "        parts = []\n",
        "        for speaker, text in dialogues[start:i+1]:\n",
        "            parts.append(f\"{SPEAKER_TOKEN}{speaker}{ENDNAME_TOKEN}{text}{ENDTURN_TOKEN}\")\n",
        "        full_text = \"\".join(parts)\n",
        "        if len(full_text) > 50:\n",
        "            examples.append(full_text)\n",
        "    return examples\n",
        "\n",
        "# Load Shakespeare works\n",
        "print(\"📚 Loading Shakespeare corpus...\")\n",
        "\n",
        "all_dialogues = []\n",
        "work_files = sorted([f for f in os.listdir(WORKS_DIR) if f.endswith('.txt')])\n",
        "selected = [f for f in work_files if any(x in f for x in ['romeo', 'hamlet', 'macbeth'])][:3]\n",
        "\n",
        "if not selected:\n",
        "    selected = work_files[:3]\n",
        "\n",
        "for fname in selected:\n",
        "    with open(os.path.join(WORKS_DIR, fname), 'r') as f:\n",
        "        dialogues = extract_dialogues(f.read())\n",
        "        all_dialogues.extend(dialogues)\n",
        "        print(f\"   {fname}: {len(dialogues)} turns\")\n",
        "\n",
        "# Format and create dataset\n",
        "training_texts = format_for_training(all_dialogues, context_turns=2)\n",
        "print(f\"\\n✅ Created {len(training_texts)} training examples\")\n",
        "\n",
        "# Show example\n",
        "print(f\"\\n📝 Sample formatted dialogue:\")\n",
        "print(training_texts[5][:200] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQCzmqTtmX6M"
      },
      "outputs": [],
      "source": [
        "# Exercise 4.2 (continued): Fine-tuning with Trainer\n",
        "\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# CREATE DATASET\n",
        "# ============================================================\n",
        "\n",
        "# Split into train/val\n",
        "split_idx = int(len(training_texts) * 0.9)\n",
        "train_texts = training_texts[:split_idx]\n",
        "val_texts = training_texts[split_idx:]\n",
        "\n",
        "# Tokenize all examples\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "def tokenize_texts(texts):\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize_texts(train_texts)\n",
        "val_encodings = tokenize_texts(val_texts)\n",
        "\n",
        "# Create HuggingFace Datasets\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": train_encodings[\"input_ids\"],\n",
        "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
        "})\n",
        "val_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": val_encodings[\"input_ids\"],\n",
        "    \"attention_mask\": val_encodings[\"attention_mask\"],\n",
        "})\n",
        "\n",
        "print(f\"✅ Datasets created\")\n",
        "print(f\"   Train: {len(train_dataset)} | Val: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XrNmJGUmX6N"
      },
      "outputs": [],
      "source": [
        "# Exercise 4.2 (continued): Configure and run Trainer\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./shakespeare_model\",\n",
        "\n",
        "    # Training parameters\n",
        "    num_train_epochs=2,                    # Number of passes through the data\n",
        "    per_device_train_batch_size=2,         # Samples per GPU per step\n",
        "    gradient_accumulation_steps=8,         # Effective batch = 2 * 8 = 16\n",
        "\n",
        "    # Learning rate\n",
        "    learning_rate=2e-5,                    # Fine-tuning uses smaller LR\n",
        "    warmup_steps=50,                       # Gradual LR warmup\n",
        "\n",
        "    # Logging\n",
        "    logging_steps=25,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "\n",
        "    # Saving\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "\n",
        "    # Performance\n",
        "    fp16=torch.cuda.is_available(),        # Mixed precision on GPU\n",
        "\n",
        "    # Misc\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Data collator: handles padding and creates labels for CLM\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal LM, not Masked LM\n",
        ")\n",
        "\n",
        "# Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"✅ Trainer configured!\")\n",
        "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez5IrSgemX6N"
      },
      "outputs": [],
      "source": [
        "# Exercise 4.2 (continued): Run training\n",
        "\n",
        "print(\"🏋️ Starting fine-tuning...\")\n",
        "print(\"   (This may take 5-15 minutes on GPU, longer on CPU)\\n\")\n",
        "\n",
        "# Train!\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"✅ Training complete!\")\n",
        "print(f\"   Final loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "SAVE_PATH = \"./shakespeare_finetuned\"\n",
        "trainer.save_model(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(f\"   Model saved to: {SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDsaaYRvmX6N"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 4.3 – Building the Chat Interface\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Generate text with a fine-tuned model using `model.generate()`\n",
        "- Understand generation parameters: `temperature`, `top_p`, `repetition_penalty`\n",
        "- Build an interactive dialogue system\n",
        "\n",
        "### Generation Parameters Explained\n",
        "\n",
        "| Parameter | Effect | Typical Values |\n",
        "|-----------|--------|----------------|\n",
        "| `temperature` | Randomness (higher = more creative) | 0.7 - 1.0 |\n",
        "| `top_p` | Nucleus sampling (cumulative probability) | 0.9 - 0.95 |\n",
        "| `repetition_penalty` | Penalize repeated tokens | 1.1 - 1.3 |\n",
        "| `max_new_tokens` | Maximum tokens to generate | 50 - 200 |\n",
        "\n",
        "### 📖 Further Reading\n",
        "\n",
        "- [Text Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxbHtRG-mX6N"
      },
      "outputs": [],
      "source": [
        "# Exercise 4.3: Text Generation Function\n",
        "\n",
        "def generate_response(\n",
        "    prompt: str,\n",
        "    responding_speaker: str,\n",
        "    max_new_tokens: int = 100,\n",
        "    temperature: float = 0.8,\n",
        "    top_p: float = 0.9,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate a response from the specified speaker.\n",
        "\n",
        "    💡 Key insight: We format the prompt with special tokens,\n",
        "       then let the model continue from the speaker's name.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Build the full prompt\n",
        "    full_prompt = f\"{prompt}{SPEAKER_TOKEN}{responding_speaker}{ENDNAME_TOKEN}\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Get end token ID for stopping\n",
        "    endturn_id = tokenizer.convert_tokens_to_ids(ENDTURN_TOKEN)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            repetition_penalty=1.2,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=[tokenizer.eos_token_id, endturn_id],\n",
        "        )\n",
        "\n",
        "    # Decode and extract response\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "    # Find the response part\n",
        "    marker = f\"{SPEAKER_TOKEN}{responding_speaker}{ENDNAME_TOKEN}\"\n",
        "    if marker in full_output:\n",
        "        response = full_output.split(marker)[-1]\n",
        "        response = response.replace(ENDTURN_TOKEN, \"\").replace(tokenizer.eos_token, \"\")\n",
        "    else:\n",
        "        response = full_output[len(full_prompt):]\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "# Test generation\n",
        "print(\"🎭 Testing generation...\\n\")\n",
        "\n",
        "test_prompt = f\"{SPEAKER_TOKEN}ROMEO{ENDNAME_TOKEN}What light through yonder window breaks?{ENDTURN_TOKEN}\"\n",
        "response = generate_response(test_prompt, \"JULIET\")\n",
        "\n",
        "print(f\"ROMEO: What light through yonder window breaks?\")\n",
        "print(f\"\\nJULIET: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHALvtSwmX6N"
      },
      "outputs": [],
      "source": [
        "# Exercise 4.3 (continued): Interactive Chat Interface\n",
        "\n",
        "class ShakespeareChat:\n",
        "    \"\"\"Simple chat interface for Shakespearean dialogue.\"\"\"\n",
        "\n",
        "    def __init__(self, user_name=\"ROMEO\", ai_name=\"JULIET\"):\n",
        "        self.user_name = user_name.upper()\n",
        "        self.ai_name = ai_name.upper()\n",
        "        self.history = \"\"\n",
        "\n",
        "    def chat(self, user_message: str, temperature: float = 0.8) -> str:\n",
        "        \"\"\"Send a message and get a response.\"\"\"\n",
        "        # Add user message to history\n",
        "        self.history += f\"{SPEAKER_TOKEN}{self.user_name}{ENDNAME_TOKEN}{user_message}{ENDTURN_TOKEN}\"\n",
        "\n",
        "        # Generate response\n",
        "        response = generate_response(self.history, self.ai_name, temperature=temperature)\n",
        "\n",
        "        # Add AI response to history\n",
        "        self.history += f\"{SPEAKER_TOKEN}{self.ai_name}{ENDNAME_TOKEN}{response}{ENDTURN_TOKEN}\"\n",
        "\n",
        "        # Keep history manageable (last 4 turns)\n",
        "        turns = self.history.split(ENDTURN_TOKEN)\n",
        "        if len(turns) > 8:\n",
        "            self.history = ENDTURN_TOKEN.join(turns[-8:]) + ENDTURN_TOKEN\n",
        "\n",
        "        return response\n",
        "\n",
        "    def reset(self, user_name=None, ai_name=None):\n",
        "        \"\"\"Reset conversation with optional new speakers.\"\"\"\n",
        "        if user_name:\n",
        "            self.user_name = user_name.upper()\n",
        "        if ai_name:\n",
        "            self.ai_name = ai_name.upper()\n",
        "        self.history = \"\"\n",
        "\n",
        "# Demo conversation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎭 SHAKESPEAREAN CHAT DEMO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "chat = ShakespeareChat(\"ROMEO\", \"JULIET\")\n",
        "\n",
        "demo_lines = [\n",
        "    \"But soft! What light through yonder window breaks?\",\n",
        "    \"My love for thee knows no bounds.\",\n",
        "]\n",
        "\n",
        "for line in demo_lines:\n",
        "    print(f\"\\nROMEO: {line}\")\n",
        "    response = chat.chat(line)\n",
        "    print(f\"JULIET: {response}\")\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zur7LHsGmX6O"
      },
      "outputs": [],
      "source": [
        "# Exercise 4.3 (continued): Run Interactive Mode\n",
        "\n",
        "def run_interactive_chat():\n",
        "    \"\"\"Run an interactive chat session.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎭 SHAKESPEAREAN CHAT\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nCharacters: ROMEO, JULIET, HAMLET, OPHELIA, MACBETH, etc.\")\n",
        "\n",
        "    user = input(\"\\nYou are (default ROMEO): \").strip() or \"ROMEO\"\n",
        "    ai = input(\"AI is (default JULIET): \").strip() or \"JULIET\"\n",
        "\n",
        "    chat = ShakespeareChat(user, ai)\n",
        "    print(f\"\\n🎭 {user} speaks with {ai}. Type 'quit' to exit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            msg = input(f\"{chat.user_name}: \").strip()\n",
        "            if msg.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"\\n🎭 Exeunt omnes!\")\n",
        "                break\n",
        "            if msg:\n",
        "                response = chat.chat(msg)\n",
        "                print(f\"{chat.ai_name}: {response}\\n\")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n🎭 Exeunt omnes!\")\n",
        "            break\n",
        "\n",
        "# Uncomment to run interactive chat:\n",
        "# run_interactive_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c__LayhmX6O"
      },
      "source": [
        "---\n",
        "\n",
        "# 🎭 Congratulations!\n",
        "\n",
        "You have completed all stages of this NLP workshop!\n",
        "\n",
        "## What You Learned\n",
        "\n",
        "| Stage | Topics |\n",
        "|-------|--------|\n",
        "| **0** | Corpus processing, tokenization, GloVe embeddings |\n",
        "| **1** | Character-level RNN, PyTorch fundamentals |\n",
        "| **2** | Word-level RNN, vocabulary handling, turn-based modeling |\n",
        "| **3** | LSTM architecture, RNN vs LSTM comparison |\n",
        "| **4** | Hugging Face ecosystem, fine-tuning transformers |\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Evolution of NLP**: RNN → LSTM → Transformer (each solving limitations of the previous)\n",
        "2. **Transfer Learning**: Fine-tuning pretrained models is more efficient than training from scratch\n",
        "3. **Hugging Face**: Industry-standard platform for NLP/ML\n",
        "4. **Special Tokens**: Help models understand structure (dialogue, turns, speakers)\n",
        "\n",
        "## 📚 Continue Learning\n",
        "\n",
        "- [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course)\n",
        "- [Fine-tuning LLMs Guide](https://huggingface.co/docs/transformers/training)\n",
        "- [Text Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)\n",
        "\n",
        "---\n",
        "\n",
        "*\"All the world's a stage, and all the men and women merely players.\"* 🎭\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRhXJt-0mX6O"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}