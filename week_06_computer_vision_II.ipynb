{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a5a61491",
      "metadata": {
        "id": "a5a61491"
      },
      "source": [
        "### Chapter 3 - Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c77f8126",
      "metadata": {
        "id": "c77f8126"
      },
      "source": [
        "**This week's exercise has 3 tasks, for a total of 10.5 points. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "In this chapter, we want you to become proficient at the following tasks:\n",
        "- Building core components of modern PyTorch models\n",
        "- Assembling modern PyTorch models from components\n",
        "- Training a modern model on a real-world task and achieving passable results\n",
        "\n",
        "**Note**: Since you have already proven that you are capable of creating the core components of a typical training loop yourself, we will provide some utility code for this section. This is done so that you can focus on the important parts of this lesson, and to help us debug your code in case you need help."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4f7614",
      "metadata": {
        "id": "8e4f7614"
      },
      "source": [
        "#### Chapter 3.1 - Data Augmentation\n",
        "\n",
        "**What is data augmentation?**\n",
        "Have you ever lost your glasses and then squinted, or tried to look through a rainy window? Or looked at a false color image, maybe a forest where the trees are blue and the sky green? You can usually make an educated guess what you are looking at, even though the image you see is different than usual. This is, essentialy, what data augmentation is. It's the same data, still recognizable, but slightly altered in some way.\n",
        "\n",
        "**What is that useful for?**\n",
        "Let me begin with an anecdote that you've probably heard in the lectures. Say you have pictures of cats and dogs, and want your model to tell the two apart. How many people you know go to the park with their dogs? I imagine many. Hence, many images of dogs are dogs lying on the grass. The same is generally untrue for cats, at least I have never heard of anyone walking their cat to the park. At any rate, here is what happens when I train a neural network on these images: The model takes a shortcut. It sees a lot of green and the correct answer for these pictures is always \"Dog\". It learns \"Green = Dog\". This is what we call overfitting. We have overfitted to the existence of green in the background as a quintessential part of what makes a dog. Sometimes, we get away with this, if our data always has this correlation.\n",
        "\n",
        "Now I get some new data. A bunch of people have taken pictures of their cats, sunbathing on the terrace. The garden is in the background. Lots of green. The model, in its infinite wisdom, will at first guess that these images are of dogs. Clearly, our model's ability to tell apart cats and dogs has not generalized to this new dataset.\n",
        "\n",
        "So how can we prevent the model from taking shortcuts and encourage learning information that generalizes? We force these generalizations in training. If I gave you an image of a dog, but the grass was brown, and the dog green, you could still identify it as a dog, instead of a cat, right? And so should the model, if we can manage it. So let's also make it train using pictures of cats and dogs where the colours are different or removed. Suddenly, the shortcut solution is no longer useful, and the model must rely on shape, texture, or contextual information like a leash. The practice of color change described is a practical and useful data augmentation that is used in state-of-the-art image recognition.\n",
        "\n",
        "In addition to color changes, there is a myriad of other techniques, such as cropping, image rotation or flipping, edge filters, solarization, random noise, and many, many more. Basically, anything that you believe may eventually show up in testing data and that you want the model to generalize to, can be made into a corresponding data augmentation.\n",
        "\n",
        "**How do we use data augmentations in practice?**\n",
        "There are two ways of adding data augmentation during training. Either, you can implement it inside of your dataset, so that it only returns augmented image tensors, or right before feeding your image tensors into your model. Both options are acceptable and come with advantages and disadvantages, although the more common way is to separate dataset and augmentations. We also showcase the native PyTorch way of augmenting data below.\n",
        "\n",
        "If you are particularly eager, or want to try your hand at making image augmentation functions yourself, it can be fun and is definitely good practice. However, PyTorch comes with a large selection of image augmentations right out of the box, and in the following chapter, we will look at how to make use of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e352a55a",
      "metadata": {
        "id": "e352a55a",
        "outputId": "68e21268-4e6e-4941-ea7b-e9d0faac41cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as tt\n",
        "import torchvision.transforms.functional as ttf\n",
        "\n",
        "# Torchvision contains two ways of utilizing transforms:\n",
        "# Functional and Composed.\n",
        "\n",
        "# Functional does what it advertises - it is a function which\n",
        "# you can use on your tensors. Here is an example which performs\n",
        "# a center crop:\n",
        "\n",
        "dummy_images = torch.rand((16, 1, 256, 256))\n",
        "transformed_images = ttf.center_crop(img = dummy_images, output_size = (128, 128))\n",
        "print(transformed_images.size())\n",
        "\n",
        "# Functional transforms have the inherent advantage of giving the\n",
        "# user very fine-grained control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "55a993c1",
      "metadata": {
        "id": "55a993c1",
        "outputId": "6944086b-fb64-4992-ba2f-7a8d1c8b4ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# The alternative is the so-called Composed form, which uses\n",
        "# classes to achieve the same result. We make a Composed Transform\n",
        "# like so:\n",
        "\n",
        "dummy_images = torch.rand((16, 1, 256, 256))\n",
        "transforms = tt.Compose([\n",
        "    tt.RandomCrop(size = (128, 128)),\n",
        "    tt.RandomHorizontalFlip()\n",
        "])\n",
        "transformed_images = transforms(dummy_images)\n",
        "print(transformed_images.size())\n",
        "\n",
        "# As you can see, Compose offers us the option of sequentially\n",
        "# executing multiple transformations in a single line of code.\n",
        "# We also get the option of using randomized augmentations,\n",
        "# where the randomization is already done for us.\n",
        "\n",
        "# In practice, either style of writing transformations is fine.\n",
        "# In fact, they are equivalent, as Compose calls the functional\n",
        "# versions of the transforms under the hood. In the case of\n",
        "# randomized augmentations, the class handles all the randomizing\n",
        "# and then calls the functional transform with the random inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b4a34c",
      "metadata": {
        "id": "b8b4a34c"
      },
      "source": [
        "**Task 1 (1+1 points)**: A complete list of Torchvision's available transforms can be found here: https://docs.pytorch.org/vision/0.9/transforms.html. Consider the task we are working on right now - working with CT images from the LiTS 2017 dataset. Which data augmentations strike you as a good idea to add to our training **(1 point)**? Which do you think are a bad idea or cannot work at all **(1 point)**? Are there any which are missing in Torchvision? If you don't know what they do, try them out and judge for yourselves. Can you think of other image types with other physics behind them? Are the rules for them going to be different?\n",
        "\n",
        "There are no definitely correct or incorrect answers here. The goal for this task is for you to be able to argue your case convincingly (to us) and think closely about your dataset. You can test your assumptions when completing the other tasks.\n",
        "\n",
        "Gute Augmentation:\n",
        "\n",
        "- Kleine Rotationen und Translationen (Verschiebungen)\n",
        "- ROI Cropp (nicht in torch)\n",
        "- Zoom / Veränderung der FOV (Organgrößen leicht verschieden)\n",
        "- minimale Intensitäts Variation (nicht in torch)\n",
        " oder simuliertes Rauschen\n",
        "\n",
        "Schlechte Augmentation:\n",
        "\n",
        "- Collor Jitter (Keine Farbkanäle, geht granicht)\n",
        "- Posterize (Nur kleine Unterschiede zur Unterschiedung)\n",
        "- Greyscale (Ist schon Grey)\n",
        "- Spiegelungen (anatomisch unkorrekt)\n",
        "- starke Intensität varriation (benötigt zu Unterscheidung)\n",
        "- Random Crop (Zu viele Bilder ohen Lesion)\n",
        "- Verzerrungen/Shear (anatomisch unkorrekt)\n",
        "\n",
        "Andere Physik\n",
        "- MRT (Intensitäten sind willkürlicher abhängig von der Pulssequenz/ Modell muss auf Intensitätschwankungen trainiert werden)\n",
        "- Ultra Schall (Bilder sind immer verzerrt/ FoV ganz anders / mehr Rauschen)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "325ff768",
      "metadata": {
        "id": "325ff768"
      },
      "source": [
        "#### Chapter 3.2 - Regularization Techniques\n",
        "\n",
        "While there are multiple definitions or guidelines for what regularization is supposed to do (see lectures), in terms of practical concerns, all regularization techniques have the same aim, expressed through different means: Improving some aspect of the performance of your deep learning models. We differentiate them, broadly, from data augmentations, because regularization techniques generally concern themselves with the learning process, e.g. loss function modifications, learning rate optimizations, temporary model modifications, etc., and *not* the underlying data in our model training.\n",
        "\n",
        "There are a number of different strategies, far too many to list all of them here, but a few particularly successful ones have made it into common use - so much so that they are more prevalent than regularization-less, \"vanilla\" deep learning. These fall into different groups, briefly discussed below.\n",
        "\n",
        "#### Additional Loss Components\n",
        "\n",
        "The loss function for any given modern optimization task is typically continuous and not always smooth everywhere. As a consequence, there are many different parameter configurations in a model that result in the same train-time loss. Not all of these express the same behavior during training or testing, however. When we modify our loss to penalize certain training behaviors, we allow the training process to select for models and parameters that give models that generalize better, converge to a solution faster, etc.,  despite often expressing the same training loss. Let's look at some examples that should be familiar from the lecture:\n",
        "\n",
        "**L1 Loss** - Often also called LASSO, L1 Loss is a penalty term added to the normal loss during training, which is defined as:\n",
        "$L_{LASSO} = \\sum_{p=1}^{P} |\\Theta_{P}|$. Growing linearly with parameter magnitude, we penalize the model. This, in turn, forces the model to use fewer and smaller weights - relying on more weights than it needs, and thus probably overfitting, is disincentivized. Similarly, we just forced our model to stick to weights near zero, which we already know is generally a preferable area for model activations to stick to.\n",
        "\n",
        "**L2 Loss** - L2 Loss is the more popular cousin of the L1 Loss, which does approximately the same thing, except the penalty is equal to the sum over all squared parameter magnitudes: $L_{LASSO} = \\sum_{p=1}^{P} |\\Theta_{P}|^2$ The reasoning behind it is similar, but it has seen far more practical adoption.\n",
        "\n",
        "**Weight Decay** - An operation that effectively performs the same duty, weight decay reduces the magnitude of weights after each backward pass, for example by subtracting a small constant or multiplying with a factor. In essence, this eliminates parameters which are rarely \"used\" and were thus likely involved in overfitting on a small amount of data anyway. Parameters that are regularly updated (and therefore probably useful), will always remain near their optimal value despite weight decay. Interestingly, Weight Decay is mathematically equivalent to L2 Loss in terms of net parameter updates.\n",
        "\n",
        "#### Training Strategies\n",
        "**Early Stopping**: For early stopping, we monitor the performance of the model on a validation set and stop training when the performance stops improving. This ensures that the model does not continue to train on the training data and potentially overfit, while also saving computational resources.\n",
        "\n",
        "**Dropout**: Dropout is a regularization technique where, during training, a random subset of neurons in a layer is \"dropped out\" (set to zero) for each forward pass. This prevents the network from relying too heavily on specific neurons and encourages it to learn more robust and generalized features. During inference, all neurons are used, but their outputs are scaled to account for the dropout during training.\n",
        "\n",
        "**Learning Rate Scheduling (LR Scheduling)**: Learning rate scheduling involves dynamically adjusting the learning rate during training. A high learning rate at the start helps the model converge quickly, while a lower learning rate later allows for fine-tuning. Common strategies include step decay, exponential decay, and cosine annealing. Proper learning rate scheduling can lead to faster convergence and better generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6cd62d1",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "f6cd62d1"
      },
      "source": [
        "**Task 1.5 (3 x 0.5 points)**: Let us implement and compare the effects of different regularization techniques on a simple neural network. To do so, follow these steps:\n",
        "\n",
        "1. Create a small neural network (e.g., 2-3 layers) and train it on the LiTS dataset without any regularization. Record the training and validation accuracy/loss. (P.S.: You can use the model from last week's exercise as a starting point.)\n",
        "2. Add L2 regularization to the model and observe how it affects the training and validation performance (Check the Adam optimizer documentation to find out how to add this regularization). Compare the results with the unregularized model.\n",
        "3. Add dropout to the model and repeat the training process (check the PyTorch documentation to find out how to add this regularization - you do not need to implement it yourself). Compare the results with the previous models.\n",
        "\n",
        "For each step, make sure to copy the relevant code snippets into a new cell, instead of modifying the existing code. This way, we can keep track of the different versions of the model and their performances.\n",
        "\n",
        "You might want to look up the documentation for implementing these techniques in PyTorch.\n",
        "\n",
        "For each regularization technique, explain how it impacts the model's performance and generalization. Which combination of techniques works best for this dataset? Why do you think that is the case?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
        "!rm -rf ./sample_data/\n",
        "!unzip -qq Clean_LiTS.zip\n",
        "!rm ./Clean_LiTS.zi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-TZP1rzWPzn",
        "outputId": "4a458fc6-8d15-4d36-f52e-28f5cfdb54ca"
      },
      "id": "2-TZP1rzWPzn",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
            "From (redirected): https://drive.google.com/uc?id=1TItTaso19GFTPdDnynVnqJvHsCm_RGlI&confirm=t&uuid=f9e48ac9-fc55-44f3-bf8b-6f0ddfa0d9d7\n",
            "To: /content/Clean_LiTS.zip\n",
            "100% 2.56G/2.56G [00:36<00:00, 71.1MB/s]\n",
            "rm: cannot remove './Clean_LiTS.zi': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f91bab93",
      "metadata": {
        "id": "f91bab93"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as ttf\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import PIL\n",
        "\n",
        "class LiTS_Dataset(Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    For our sample solution, we go for the easier variant.\n",
        "\n",
        "    In this specific dataset, we don't load the images until we need them - for a\n",
        "    short training, or limited resources, this is good behavior. If you have the\n",
        "    necessary RAM to pre-load all of your data, you don't have to load the data\n",
        "    multiple times, and save compute costs in the long run. The downside is that\n",
        "    when you are trying to debug, you wait for ages every time, and if you simply\n",
        "    do not have the compute resources, you can't even do it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv: str, mode: str):\n",
        "\n",
        "        self.csv = csv\n",
        "        self.data = pd.read_csv(self.csv)\n",
        "        self.mode = mode\n",
        "        assert mode in [\"train\", \"val\", \"test\"] # has to be train, val, or test data - if not, assert throws an error\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        file = self.data.loc[idx, \"filename\"]\n",
        "        with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "            f = f.convert(\"L\")\n",
        "            image = ttf.pil_to_tensor(f)\n",
        "\n",
        "        image = image.to(dtype = torch.float32)\n",
        "        image -= torch.min(image)\n",
        "        image /= torch.max(image)\n",
        "\n",
        "        liver_visible = self.data.loc[idx, \"liver_visible\"]\n",
        "        lesion_visible = self.data.loc[idx, \"lesion_visible\"]\n",
        "        # Note that targets must have the data type torch.long - a 64-bit integer,\n",
        "        # unlike the image tensor, which is usually a 32-bit float, the default\n",
        "        # dtype for tensors when none is given\n",
        "        if lesion_visible and liver_visible:\n",
        "            target = torch.tensor(2, dtype = torch.long)\n",
        "        elif not lesion_visible and liver_visible:\n",
        "            target = torch.tensor(1, dtype = torch.long)\n",
        "        elif not lesion_visible and not liver_visible:\n",
        "            target = torch.tensor(0, dtype = torch.long)\n",
        "        else:\n",
        "            print(\n",
        "                idx,\n",
        "                lesion_visible,\n",
        "                liver_visible,\n",
        "                self.data.loc[idx, \"liver_visible\"],\n",
        "                self.data.loc[idx, \"lesion_visible\"],\n",
        "                self.data.loc[idx, \"filename\"]\n",
        "                )\n",
        "            raise ValueError(\"Invalid target\")\n",
        "\n",
        "        return image, target\n",
        "\n",
        "train_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/train_classes.csv\", mode=\"train\")\n",
        "val_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/val_classes.csv\", mode=\"val\")\n",
        "test_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/test_classes.csv\", mode=\"test\")\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 0,\n",
        "    shuffle = False,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    drop_last = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bdaccb5c",
      "metadata": {
        "id": "bdaccb5c"
      },
      "outputs": [],
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Insert your model here\n",
        "\n",
        "class YourModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_classes=3):\n",
        "        super(YourModel, self).__init__()\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=(3, 3), padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(3, 3), padding=1)\n",
        "        self.conv3 = torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
        "        self.conv4 = torch.nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.pool = torch.nn.MaxPool2d(2)\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(in_features = 128 * 16 * 16, out_features = 256)\n",
        "        self.fc2 = torch.nn.Linear(in_features = 256, out_features =3)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.pool(self.relu(self.conv4(x)))\n",
        "\n",
        "        x = x.flatten(start_dim=1)\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5efe25fa",
      "metadata": {
        "id": "5efe25fa"
      },
      "outputs": [],
      "source": [
        "# Now for the training loop\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_loop(your_model):\n",
        "    \"\"\"\n",
        "    This function runs your train loop and returns the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = your_model(in_channels = 1, out_classes = 3)\n",
        "    model = model.to(device = device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(),\n",
        "        lr=1e-4,\n",
        "        weight_decay=1e-5  # L2\n",
        "    )\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # This uses the length of the current set - \"train\"\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "            # Validation mode on\n",
        "            model.eval()\n",
        "\n",
        "            # Don't track gradients for validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                hits = 0\n",
        "                losses = []\n",
        "                batch_sizes = []\n",
        "\n",
        "                for step, (data, targets) in enumerate(val_dataloader):\n",
        "\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "                    loss = loss_criterion(predictions, targets)\n",
        "                    losses.append(loss.item())\n",
        "                    batch_sizes.append(data.size()[0])\n",
        "\n",
        "                    class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                    hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "                accuracy = hits / len(val_dataloader.dataset)\n",
        "                avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "                print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9e04157a",
      "metadata": {
        "id": "9e04157a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb500fa-2ae1-46e2-b9c2-b7662d8dd6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.1075\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 0.8566\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 0.7830\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 0.7974\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 0.8077\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 0.4525\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.5593\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 0.6973\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.7749\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 0.5155\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.3345\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 0.5802\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.3502\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 0.3660\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 0.3699\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.2065\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.2946\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.6406\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.5193\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.5941\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 0.2893\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 0.3496\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 0.4496\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.1414\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.5291\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 0.2339\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.3085\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 0.1751\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.3643\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 0.5271\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.1801\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 0.3562\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.2343\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 0.2683\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 0.1152\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 0.3175\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 0.3038\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 0.2031\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 0.1912\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.4685\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 0.1026\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.3303\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.2287\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.1968\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.0686\n",
            "Epoch: 1,\t Validation Loss: 0.3738,\t Accuracy: 0.8210\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.4633\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 0.0462\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.1123\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 0.1057\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 0.2145\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 0.1078\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.3976\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.0802\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.3113\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.2235\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.1056\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.2127\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.0825\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.2421\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 0.0295\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 0.1711\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.0654\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 0.0600\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.0571\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.0470\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.2970\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.1798\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 0.0888\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.1362\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 0.0420\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 0.0790\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.3112\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.2620\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.0452\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 0.3950\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 0.0355\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.3953\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 0.0961\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 0.1230\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.1643\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 0.3361\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 0.0355\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 0.3026\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.1351\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.1022\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.0347\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 0.1320\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 0.0904\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.1161\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.0600\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 0.6002\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.0344\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 0.0346\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 0.1186\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 0.0524\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.0725\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 0.0925\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.0069\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.1487\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.1933\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.1501\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.0535\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.1107\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.0284\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.0342\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 0.0438\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 0.1188\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.0228\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.2073\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.2091\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.1679\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.0252\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 0.2725\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.1063\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.0218\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.0214\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.2336\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.0028\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.0221\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.0425\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.0468\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.2065\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.1598\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.1102\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 0.0952\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.0878\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.0559\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 0.8844\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.0122\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 0.1170\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.0069\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.1344\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.0305\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.0918\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.0420\n",
            "Epoch: 3,\t Validation Loss: 0.4414,\t Accuracy: 0.8782\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.0350\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 0.1674\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.1505\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.4304\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 0.0359\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.0338\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.0875\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.0538\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.1281\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 0.1408\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.1306\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 0.0887\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.0316\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.0334\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.0125\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 0.0456\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 0.0334\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.2271\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.0005\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.0420\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.1152\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.0235\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 0.2846\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.0082\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 0.0395\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.1962\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.3311\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 0.1505\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.0834\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 0.0508\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.0637\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.0033\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 0.1278\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 0.0397\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 0.0105\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.0015\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.0273\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.0177\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.0995\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 0.0473\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.0359\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.0039\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.0073\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 0.0030\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.0577\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.0169\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 0.0293\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.1836\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.0674\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.1578\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.3221\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.0413\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.0373\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.1097\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 0.0198\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 0.1680\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.2406\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 0.0087\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.1159\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.0510\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.0355\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.0464\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.0264\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.0028\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.0036\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.0213\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.0210\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.0081\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.0426\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.0110\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 0.0142\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.0267\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 0.0489\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.0081\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 0.2202\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.0075\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 0.0379\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.0258\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.0749\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.0063\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 0.0068\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.2720\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 0.1368\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 0.0775\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 0.0071\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.0385\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.0307\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.0815\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 0.0286\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 0.0255\n",
            "Epoch: 5,\t Validation Loss: 0.6301,\t Accuracy: 0.8555\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.0256\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 0.0147\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 0.0044\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.0009\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 0.0552\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.0528\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.0237\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.0099\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.0028\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 0.2221\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.2079\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 0.0021\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.0672\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.0083\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.0154\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.1221\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 0.0109\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.0061\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.0858\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.0071\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 0.0416\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.0304\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 0.0002\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.0019\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.0131\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.0450\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.0070\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 0.0600\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 0.0163\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.0597\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.0061\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.0144\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.0070\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.0149\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 0.0243\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.0296\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 0.0039\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 0.0162\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.2297\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.0497\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 0.0117\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 0.0100\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 0.0851\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.0017\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.0336\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 0.0671\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 0.0199\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 0.0140\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.2563\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.0313\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.0617\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 0.3983\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.0336\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.0016\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.0023\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 0.0549\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 0.1808\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 0.0199\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.0420\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.2828\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.0222\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.0622\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.0025\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 0.0024\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 0.0132\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.0991\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 0.0061\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 0.0864\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 0.1113\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 0.0007\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 0.0553\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.0073\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.0011\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 0.0013\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 0.1678\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.0723\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 0.0644\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.0070\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.4174\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 0.0385\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 0.2029\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.0023\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 0.1028\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.0069\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.0090\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.0322\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 0.0779\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.0209\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.0711\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.0017\n",
            "Epoch: 7,\t Validation Loss: 0.7495,\t Accuracy: 0.8388\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.0102\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.0008\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.0790\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.0032\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.0802\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 0.0245\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 0.0016\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.0397\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.1827\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 0.0277\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 0.0031\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 0.2655\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.0338\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.1478\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.1111\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 0.1256\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.0224\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.0388\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.0160\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.0043\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 0.0038\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 0.0898\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 0.0127\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.0317\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.0703\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.0013\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.0444\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 0.0617\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 0.0067\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 0.0024\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 0.0012\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.0124\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.1688\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.0486\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.1586\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.0091\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.0071\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.0066\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.0127\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.0685\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.0424\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.1097\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.0561\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.0055\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.0079\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 0.1827\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.0019\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.0859\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.0367\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.0566\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 0.1251\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.0801\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 0.0141\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.0152\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.0530\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.1060\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.0024\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.0278\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.0066\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 0.0051\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.0006\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.0057\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 0.0978\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.0700\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.0106\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.1115\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.0177\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.2328\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.0830\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.0064\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 0.0167\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.0070\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.0465\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.0073\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 0.0010\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.0488\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.0038\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.1829\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.0346\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.0003\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 0.0872\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.0054\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 0.0694\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.0027\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.0090\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 0.0014\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.0106\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.1027\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 0.0065\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.0171\n",
            "Epoch: 9,\t Validation Loss: 0.8027,\t Accuracy: 0.8394\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.0553\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 0.0356\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.0037\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 0.0738\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 0.0001\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.0403\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.0432\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 0.0000\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.0027\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 0.0064\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 0.0036\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.0012\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.0013\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.0348\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.0078\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.0005\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.0420\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 0.0246\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.0094\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.0007\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 0.0144\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 0.0010\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.0107\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 0.0048\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.0524\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 0.0065\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.0013\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.0504\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.0135\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.0856\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 0.0005\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.0044\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 0.0728\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 0.0038\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.2034\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 0.0092\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.0337\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.0909\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.0276\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 0.0043\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.1775\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.0027\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 0.0021\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.1706\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.0033\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.0980\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.0067\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.0055\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 0.3056\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.1204\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.0039\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 0.0017\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.0049\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 0.0047\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 0.0060\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.0162\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.0000\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.0026\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.0233\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.0028\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 0.0885\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 0.0091\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.0860\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.1064\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.0006\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.0013\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.0001\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.0451\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.0069\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.0181\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.0034\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 0.0059\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 0.0016\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.0076\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.0009\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.2746\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.0224\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.1644\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.0041\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.0094\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 0.0038\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 0.0032\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 0.0005\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.0120\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.0340\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 0.0187\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.0258\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 0.0504\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.0034\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.0017\n",
            "Epoch: 11,\t Validation Loss: 0.9793,\t Accuracy: 0.8427\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.0328\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.0253\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.0006\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.0035\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.0076\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 0.0002\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 0.1320\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 0.0171\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 0.0004\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.0115\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 0.0043\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.0359\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.0068\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.0007\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.0014\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.1962\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.2063\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.0114\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.0607\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.0012\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.0048\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.0045\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.0060\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.3147\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.0143\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.0024\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.0085\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.0066\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.0264\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.2312\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.0038\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 0.0007\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.0411\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.1321\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 0.0001\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.0002\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 0.1867\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.0956\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 0.0283\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.0090\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.0008\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.0011\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.0153\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.0467\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 0.0171\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.0033\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.0108\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.0230\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 0.1186\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.0041\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.0009\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 0.1071\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.0134\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 0.0004\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.0005\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.0237\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 0.0071\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.0045\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 0.0303\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.0203\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.0000\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 0.0023\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.0015\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.0224\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.0062\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 0.0185\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.0002\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 0.0062\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.0114\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.0054\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.0214\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.0005\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.0032\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.0014\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.0028\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.0706\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.0201\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.0002\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.0003\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.0060\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 0.0002\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.0047\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.0558\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.0082\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 0.0017\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.0023\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.0009\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 0.1889\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 0.0213\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.0346\n",
            "Epoch: 13,\t Validation Loss: 0.8704,\t Accuracy: 0.8417\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.0259\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.0109\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.0002\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.0115\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 0.2819\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 0.0296\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 0.1423\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.0019\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.1106\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.0188\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.0258\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.0019\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 0.0006\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.0050\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.0000\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.0200\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.1734\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 0.0011\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.0252\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.0073\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.0040\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 0.1047\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 0.0137\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.0044\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.0072\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.0039\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.0261\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.0006\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.0184\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 0.0079\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.2213\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 0.0072\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.1741\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.1205\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.0005\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.1363\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.0586\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.0009\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.0498\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.0056\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.0780\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.0043\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.0038\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.0101\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 0.0081\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.0838\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.0051\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.0189\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.0006\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.0003\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.0038\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 0.0119\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.0225\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.0783\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.0033\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.0212\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.0018\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.0011\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.0083\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 0.0012\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 0.0023\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.0017\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.0017\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.0018\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 0.0029\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.0184\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.0001\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.1743\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 0.0019\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.0293\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.0008\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.0677\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 0.0078\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.1667\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.0006\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.0217\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 0.1964\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.0040\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 0.0094\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 0.0007\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.0013\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 0.0035\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 0.0017\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.0240\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 0.0103\n",
            "Epoch: 15,\t Validation Loss: 0.9800,\t Accuracy: 0.8529\n"
          ]
        }
      ],
      "source": [
        "trained_model = train_loop(YourModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "10ff035d",
      "metadata": {
        "id": "10ff035d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c053d889-6e73-4751-b08c-dbf38d569657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.5275,\t Accuracy: 0.7574\n"
          ]
        }
      ],
      "source": [
        "# Standard\n",
        "\n",
        "trained_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "605399f3",
      "metadata": {
        "id": "605399f3"
      },
      "source": [
        "### L2-Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1befc94e",
      "metadata": {
        "id": "1befc94e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024e9c76-93d4-4f07-c3e2-c29e059a0a35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.0972\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 0.8323\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 1.0817\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 0.6268\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 0.4279\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 0.3117\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.4290\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 0.2703\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.7447\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 0.3775\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.2129\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 0.1579\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.3333\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 0.1405\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 0.2533\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.4812\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.3356\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.4838\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.8975\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.4541\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 0.1665\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 0.4049\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 0.5057\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.2664\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.1968\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 0.2770\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.1471\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 0.4732\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.2510\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 0.5792\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.4297\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 0.2076\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.1850\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 0.1360\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 0.5341\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 0.7085\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 0.2512\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 0.1599\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 0.2412\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.6160\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 0.1479\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.3832\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.0626\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.3258\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.2930\n",
            "Epoch: 1,\t Validation Loss: 0.4341,\t Accuracy: 0.7966\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.0997\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 0.4686\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.4746\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 0.2070\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 0.1875\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 0.0869\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.1468\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.1044\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.0953\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.1804\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.5556\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.2552\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.2453\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.5506\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 0.4203\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 0.4817\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.2817\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 0.1155\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.0593\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.1379\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.2283\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.1579\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 0.0694\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.2642\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 0.4343\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 0.0555\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.2417\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.1602\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.0625\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 0.1049\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 0.1192\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.1698\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 0.0878\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 0.2876\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.4132\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 0.1894\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 0.0978\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 0.2060\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.2262\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.1390\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.1145\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 0.1949\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 0.0558\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.1484\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.0782\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 0.3131\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.2481\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 0.0466\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 0.0531\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 0.1179\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.1328\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 0.1113\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.2663\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.2150\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.3190\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.1221\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.0875\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.0919\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.2686\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.1772\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 0.0289\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 0.0270\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.1236\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.0752\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.1373\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.0021\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.0024\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 0.0998\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.0887\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.0641\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.0705\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.0517\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.1397\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.0226\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.1497\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.0223\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.1565\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.0304\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.1819\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 0.0440\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.0802\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.1003\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 0.1373\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.0687\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 0.0774\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.0193\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.0127\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.2829\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.2302\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.1050\n",
            "Epoch: 3,\t Validation Loss: 0.4426,\t Accuracy: 0.8506\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.0402\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 0.0604\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.0479\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.2011\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 0.0399\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.0197\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.0280\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.1379\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.1166\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 0.0627\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.0637\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 0.1345\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.1359\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.0180\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.0419\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 0.0931\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 0.2518\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.0645\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.0545\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.3491\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.0534\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.0625\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 0.0742\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.0292\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 0.0216\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.0930\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.0887\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 0.0294\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.0042\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 0.1024\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.0718\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.1095\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 0.0699\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 0.0025\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 0.4298\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.0559\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.0346\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.0357\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.0105\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 0.0127\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.1110\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.0366\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.0247\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 0.1260\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.2116\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.0822\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 0.0148\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.0412\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.0500\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.0154\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.0256\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.1492\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.3545\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.0812\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 0.0197\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 0.0681\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.0371\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 0.1234\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.0228\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.0595\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.0227\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.0087\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.0058\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.1383\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.7741\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.0893\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.0177\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.0057\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.0529\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.0154\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 0.0552\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.0355\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 0.0469\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.0323\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 0.0428\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.0067\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 0.2801\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.0250\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.0073\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.1308\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 0.0100\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.0700\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 0.0163\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 0.0251\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 0.0318\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.0281\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.0119\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.1282\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 0.0344\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 0.0586\n",
            "Epoch: 5,\t Validation Loss: 0.4858,\t Accuracy: 0.8631\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.0229\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 0.1978\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 0.0056\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.0242\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 0.1644\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.1012\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.1314\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.1936\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.0173\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 0.0112\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.0677\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 0.1705\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.0282\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.0713\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.0884\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.1176\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 0.0465\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.0127\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.0527\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.0007\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 0.0771\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.0164\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 0.0092\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.0835\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.0082\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.0316\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.0183\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 0.0914\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 0.1072\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.0497\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.0026\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.0071\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.0008\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.1373\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 0.5145\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.0732\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 0.1505\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 0.0353\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.0300\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.0435\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 0.0065\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 0.0040\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 0.0063\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.0063\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.0039\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 0.0182\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 0.0316\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 0.0416\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.0810\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.0214\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.0017\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 0.0473\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.0784\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.0045\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.1099\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 0.0005\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 0.0019\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 0.0476\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.0184\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.0105\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.0404\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.0068\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.0241\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 0.0999\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 0.0081\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.0577\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 0.1989\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 0.0195\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 0.0065\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 0.0168\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 0.1166\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.0428\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.0319\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 0.1318\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 0.0151\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.0009\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 0.0412\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.0230\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.0192\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 0.0268\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 0.0025\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.0444\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 0.3677\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.0045\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.0334\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.0604\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 0.0695\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.0467\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.0458\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.0011\n",
            "Epoch: 7,\t Validation Loss: 0.6801,\t Accuracy: 0.8509\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.0002\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.0026\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.0744\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.1598\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.0129\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 0.1502\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 0.0741\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.0445\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.0236\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 0.0884\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 0.1735\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 0.2823\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.0396\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.0013\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.0010\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 0.0948\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.1044\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.0133\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.0332\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.0097\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 0.1103\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 0.1368\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 0.0069\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.1641\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.0780\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.0892\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.0857\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 0.0032\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 0.0201\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 0.0245\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 0.0121\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.0017\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.1509\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.0076\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.0200\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.1657\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.0447\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.0062\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.0092\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.0271\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.0268\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.0006\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.0197\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.0147\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.0024\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 0.0395\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.0376\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.0050\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.0053\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.0594\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 0.0153\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.0036\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 0.0282\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.0093\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.0443\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.3857\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.0032\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.0053\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.0309\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 0.0430\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.0002\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.0564\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 0.0135\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.0064\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.0026\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.0083\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.0604\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.0350\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.0205\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.3198\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 0.0524\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.0113\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.0906\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.0036\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 0.0248\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.0956\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.0472\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.0244\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.0538\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.0212\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 0.0729\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.0528\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 0.0005\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.0537\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.0080\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 0.0018\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.0130\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.2045\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 0.0014\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.0093\n",
            "Epoch: 9,\t Validation Loss: 0.7522,\t Accuracy: 0.8414\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.0184\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 0.0346\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.0909\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 0.0219\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 0.0088\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.0110\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.0021\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 0.0287\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.0262\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 0.0058\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 0.0086\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.0238\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.0325\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.0395\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.0028\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.0102\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.0251\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 0.0156\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.0306\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.0182\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 0.2505\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 0.0284\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.0093\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 0.0107\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.0140\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 0.0042\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.0057\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.0112\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.0033\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.0021\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 0.0035\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.0407\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 0.0013\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 0.1458\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.0060\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 0.0035\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.0003\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.0076\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.0016\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 0.0098\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.0043\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.3117\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 0.0025\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.0529\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.0914\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.0017\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.0085\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.0006\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 0.0065\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.0055\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.0000\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 0.0083\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.0901\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 0.1084\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 0.0082\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.0237\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.0146\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.0003\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.0004\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.0040\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 0.0063\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 0.0947\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.0058\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.0094\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.0309\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.0026\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.0066\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.3179\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.0346\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.1950\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.0048\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 0.0228\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 0.0122\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.0234\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.0001\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.0016\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.0049\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.0209\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.0038\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.0253\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 0.0231\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 0.0145\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 0.0025\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.0031\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.0128\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 0.0656\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.0397\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 0.0014\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.0532\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.1296\n",
            "Epoch: 11,\t Validation Loss: 0.7145,\t Accuracy: 0.8463\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.0093\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.1897\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.0075\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.0994\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.0653\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 0.0118\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 0.0115\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 0.0036\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 0.0306\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.1595\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 0.1678\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.0333\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.0007\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.0678\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.0010\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.1178\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.2427\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.0025\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.0139\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.0291\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.0012\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.0115\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.0009\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.0796\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.0123\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.1746\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.0214\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.0050\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.0056\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.0135\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.0009\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 0.0007\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.0074\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.0034\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 0.0076\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.0071\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 0.0856\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.0046\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 0.0024\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.0041\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.0888\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.0009\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.1538\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.0022\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 0.0177\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.2432\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.0124\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.1780\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 0.0040\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.0030\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.0102\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 0.2379\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.0018\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 0.0062\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.0036\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.0003\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 0.0004\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.0551\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 0.0047\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.0004\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.0113\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 0.0008\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.1655\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.0434\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.0795\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 0.0015\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.0014\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 0.0005\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.0320\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.0301\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.0001\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.0991\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.0074\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.0013\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.0008\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.0040\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.0012\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.0160\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.0012\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.0017\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 0.0024\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.0446\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.0417\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.0023\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 0.0013\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.0080\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.0252\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 0.0042\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 0.0008\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.0120\n",
            "Epoch: 13,\t Validation Loss: 0.7920,\t Accuracy: 0.8506\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.0004\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.0028\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.0021\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.0019\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 0.1195\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 0.0001\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 0.0007\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.1709\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.0184\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.0258\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.0145\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.0104\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 0.0001\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.0005\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.0006\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.0000\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.2169\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 0.0072\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.0895\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.0125\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.0046\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 0.0045\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 0.0011\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.0063\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.1349\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.0053\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.0018\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.0172\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.0007\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 0.0069\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.0104\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 0.0175\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.0064\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.0026\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.0039\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.0034\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.0428\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.0138\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.0285\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.0002\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.0265\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.0014\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.0020\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.0004\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 0.0328\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.0304\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.0101\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.0028\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.0053\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.0021\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.0925\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 0.0002\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.0023\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.0044\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.0025\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.0000\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.0079\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.0045\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.0250\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.0298\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.0008\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.0064\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.0098\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 0.0018\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.0021\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.0168\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.0134\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 0.0220\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.0034\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.0016\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.0265\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 0.0069\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.0027\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.0009\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.0063\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 0.1129\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.0037\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.0086\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.0011\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 0.0436\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.0023\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 0.0109\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 0.0426\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.0045\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 0.0039\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 0.0001\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.0014\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 0.0069\n",
            "Epoch: 15,\t Validation Loss: 0.8842,\t Accuracy: 0.8559\n",
            "Test Loss: 1.3043,\t Accuracy: 0.7617\n"
          ]
        }
      ],
      "source": [
        "\n",
        "  # optimizer = torch.optim.Adam(\n",
        "      #  params=model.parameters(),\n",
        "       # lr=1e-4,\n",
        "       # weight_decay=1e-5  # L2\n",
        "  # )\n",
        "\n",
        "trained_model1 = train_loop(YourModel)\n",
        "\n",
        "trained_model1.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model1(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e200dbd0",
      "metadata": {
        "id": "e200dbd0"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "811813fc",
      "metadata": {
        "id": "811813fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49bfbc6-e06e-43b8-ec1d-69e3adca4196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.1149\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 1.2823\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 0.8730\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 0.6028\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 0.5060\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 0.8361\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.4397\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 0.4814\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.3721\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 0.1336\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.2689\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 0.4442\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.2130\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 0.3680\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 0.2229\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.3230\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.2105\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.4599\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.4275\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.9152\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 0.4587\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 0.5262\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 0.2687\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.6977\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.2957\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 0.2359\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.3427\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 0.4487\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.2068\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 0.3059\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.7374\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 0.2780\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.3354\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 0.4805\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 0.0970\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 0.1873\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 0.3541\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 0.2331\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 0.3482\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.4552\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 0.2208\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.2523\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.4565\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.4070\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.2699\n",
            "Epoch: 1,\t Validation Loss: 0.3674,\t Accuracy: 0.8496\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.1993\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 0.2434\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.1484\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 0.0320\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 0.2110\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 0.1901\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.3943\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.4611\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.1407\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.1495\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.2380\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.0504\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.2444\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.5884\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 0.1513\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 0.2559\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.2596\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 0.2020\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.1042\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.1979\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.2694\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.0932\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 0.2952\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.0709\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 0.1113\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 0.1219\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.2967\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.2418\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.5860\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 0.0682\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 0.1732\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.2199\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 0.1762\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 0.0949\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.0919\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 0.0770\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 0.0548\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 0.2149\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.2462\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.2797\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.0127\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 0.0353\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 0.0605\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.2063\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.0675\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 0.0699\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.2213\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 0.4391\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 0.0494\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 0.0848\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.0121\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 0.1443\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.2986\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.2082\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.0658\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.0081\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.0508\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.2455\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.0517\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.0670\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 0.2268\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 0.1143\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.0696\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.1682\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.0663\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.0849\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.1569\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 0.1256\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.1711\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.0508\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.0443\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.0795\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.2195\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.0615\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.0463\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.0071\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.0606\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.0267\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.0889\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 0.0385\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.0713\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.2281\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 0.0310\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.0808\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 0.0708\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.1807\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.2612\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.0660\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.0880\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.2463\n",
            "Epoch: 3,\t Validation Loss: 0.3587,\t Accuracy: 0.8684\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.1308\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 0.0869\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.0535\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.0182\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 0.0074\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.0760\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.0335\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.0401\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.0518\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 0.0912\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.0221\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 0.0999\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.0277\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.0401\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.0723\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 0.0654\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 0.1367\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.1084\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.1464\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.0158\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.2645\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.0157\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 0.0794\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.0097\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 0.0714\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.0003\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.0121\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 0.0745\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.0533\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 0.0265\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.1953\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.0569\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 0.0184\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 0.0267\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 0.0778\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.1430\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.0666\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.0425\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.0121\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 0.0132\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.1215\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.2463\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.1613\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 0.1021\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.0580\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.1506\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 0.0046\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.1037\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.1474\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.0919\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.0935\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.2689\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.0013\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.1317\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 0.0219\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 0.0478\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.0274\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 0.0314\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.0002\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.0636\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.0064\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.0802\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.0530\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.0710\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.0991\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.1732\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.1876\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.1782\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.0422\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.0266\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 0.0256\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.0820\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 0.0411\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.1585\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 0.0534\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.0145\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 0.2579\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.1091\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.0041\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.0402\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 0.3873\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.0679\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 0.3034\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 0.0640\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 0.0259\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.0393\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.0032\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.0685\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 0.0324\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 0.0003\n",
            "Epoch: 5,\t Validation Loss: 0.5089,\t Accuracy: 0.8664\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.1103\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 0.0134\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 0.0177\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.0029\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 0.1802\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.0895\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.2332\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.0861\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.0083\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 0.0013\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.1252\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 0.0319\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.0304\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.0779\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.0086\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.0202\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 0.0922\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.1284\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.0139\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.0009\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 0.0164\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.0304\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 0.0254\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.0022\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.0111\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.0013\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.0069\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 0.0140\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 0.0103\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.0295\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.2119\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.0058\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.0261\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.0004\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 0.0066\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.0066\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 0.0331\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 0.0028\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.2731\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.0009\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 0.0192\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 0.0948\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 0.0088\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.0114\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.0951\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 0.1346\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 0.0025\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 0.0025\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.0164\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.0223\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.0420\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 0.0510\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.2724\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.0000\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.0227\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 0.0090\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 0.0055\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 0.0214\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.0489\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.3026\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.0954\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.1233\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.1386\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 0.0309\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 0.0141\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.0806\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 0.0251\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 0.0270\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 0.0034\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 0.0527\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 0.2106\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.0759\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.0277\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 0.0908\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 0.1706\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.0229\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 0.0271\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.0011\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.1368\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 0.5651\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 0.0148\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.0212\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 0.0277\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.1439\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.0855\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.0010\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 0.0014\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.2255\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.0518\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.1519\n",
            "Epoch: 7,\t Validation Loss: 0.6503,\t Accuracy: 0.8233\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.0223\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.0001\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.0012\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.0293\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.0662\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 0.0085\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 0.0018\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.0003\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.0077\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 0.0048\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 0.0020\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 0.0049\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.0420\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.0220\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.3590\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 0.1337\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.1747\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.0356\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.0506\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.0653\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 0.0670\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 0.1716\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 0.0085\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.0410\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.0180\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.0916\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.0026\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 0.0003\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 0.0496\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 0.0045\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 0.0187\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.1354\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.0796\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.0063\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.0066\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.1053\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.1663\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.0020\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.0378\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.0006\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.3636\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.0019\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.0894\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.0397\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.0033\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 0.0270\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.0339\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.0069\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.0254\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.0441\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 0.0077\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.0273\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 0.0842\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.0009\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.0410\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.0033\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.0177\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.0051\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.0920\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 0.0015\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.0600\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.0004\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 0.0521\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.0078\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.1013\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.0047\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.0854\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.0592\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.0163\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.0131\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 0.0737\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.0155\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.1630\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.0012\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 0.0292\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.2196\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.1617\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.0218\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.0308\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.0093\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 0.0165\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.0060\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 0.0015\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.0003\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.0021\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 0.0327\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.1306\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.0013\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 0.0131\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.0008\n",
            "Epoch: 9,\t Validation Loss: 0.7162,\t Accuracy: 0.8483\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.0014\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 0.0211\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.0154\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 0.0018\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 0.0058\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.0086\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.0009\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 0.1190\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.0264\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 0.0077\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 0.0024\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.1022\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.0027\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.0112\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.0031\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.0215\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.0943\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 0.0016\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.0038\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.0076\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 0.0138\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 0.0003\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.0317\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 0.0044\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.1382\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 0.0004\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.0617\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.0071\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.0425\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.0828\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 0.0102\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.0834\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 0.0832\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 0.1592\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.0649\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 0.0286\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.0184\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.1238\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.0002\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 0.0043\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.0005\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.0254\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 0.0009\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.0808\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.0349\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.2672\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.0519\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.0082\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 0.0005\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.0032\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.1187\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 0.1685\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.0165\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 0.0869\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 0.0238\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.0009\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.0186\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.0030\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.1023\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.0190\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 0.0063\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 0.0018\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.0416\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.0130\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.0002\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.0017\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.0529\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.0021\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.0020\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.0156\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.0102\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 0.0049\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 0.0173\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.0125\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.0016\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.0208\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.3299\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.1955\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.1045\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.0024\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 0.0062\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 0.0040\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 0.0055\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.0012\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.0099\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 0.0272\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.0572\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 0.0374\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.0014\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.0115\n",
            "Epoch: 11,\t Validation Loss: 0.7657,\t Accuracy: 0.8322\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.0031\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.0395\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.0062\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.0271\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.0020\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 0.0195\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 0.0185\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 0.0596\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 0.0011\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.0018\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 0.0018\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.3715\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.1456\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.0169\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.0634\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.0010\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.1326\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.1476\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.0000\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.0005\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.0154\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.0095\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.0744\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.0003\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.0629\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.1229\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.0007\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.0006\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.0031\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.0006\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.0028\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 0.0372\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.0015\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.0016\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 0.0706\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.0892\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 0.0008\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.0011\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 0.0535\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.0002\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.0107\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.0615\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.0006\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.0205\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 0.0040\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.0174\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.0040\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.0165\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 0.0914\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.0740\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.0721\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 0.0528\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.0088\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 0.0168\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.0014\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.0248\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 0.0023\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.0336\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 0.0035\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.0040\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.0042\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 0.0306\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.0019\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.1045\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.0023\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 0.0012\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.0835\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 0.1522\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.0730\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.0095\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.0012\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.3631\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.0073\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.0098\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.0770\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.0011\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.0203\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.0005\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.2751\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.1132\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 0.0013\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.0171\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.0145\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.0159\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 0.0332\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.0184\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.0033\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 0.0050\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 0.1257\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.0926\n",
            "Epoch: 13,\t Validation Loss: 0.7625,\t Accuracy: 0.8388\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.0031\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.0036\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.0045\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.0131\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 0.0887\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 0.0085\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 0.0891\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.0463\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.0633\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.0002\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.0007\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.0243\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 0.0605\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.0022\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.0012\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.0008\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.0180\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 0.0189\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.2259\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.0102\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.0003\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 0.0083\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 0.0012\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.0043\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.0170\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.0092\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.0177\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.0003\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.0562\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 0.0031\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.0455\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 0.1568\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.0089\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.0004\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.0614\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.0931\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.0012\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.1128\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.0067\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.0000\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.0143\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.2762\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.0057\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.0150\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 0.1859\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.0049\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.1846\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.0548\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.0201\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.0001\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 0.0098\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.0405\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.0052\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.0178\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.0054\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.0021\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.0510\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.0076\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.0025\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.0014\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.0118\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.0669\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 0.0235\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.0001\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 0.0273\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.0084\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.0007\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.0091\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 0.0002\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.0007\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.0110\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.0402\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 0.0072\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.0054\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.2024\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.0008\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 0.0503\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.0034\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.0080\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 0.0451\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.0003\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 0.0126\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 0.0038\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.0170\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 0.0006\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.0106\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 0.0004\n",
            "Epoch: 15,\t Validation Loss: 0.8414,\t Accuracy: 0.8440\n",
            "Test Loss: 1.4148,\t Accuracy: 0.7640\n"
          ]
        }
      ],
      "source": [
        "   # self.dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "   # x = self.dropout(x)\n",
        "\n",
        "trained_model2 = train_loop(YourModel)\n",
        "\n",
        "trained_model2.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model2(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2: Extra Loss; verhindert, dass das Modell auf einzelne Weights fokussiert (kein Overfitting)\n",
        "Dropout: Es wird eine Anzahl von Neuronen während jedem durchlaufen zufällig deaktiviert. Verhindert Overfitting, Modell generalsisiert besser.\n",
        "\n",
        "Viele Parameter in einem Bild. Modell kann leicht zufällige Details lernen. Dies verhindert die Regularisierung."
      ],
      "metadata": {
        "id": "XYSFPJ13qBRO"
      },
      "id": "XYSFPJ13qBRO"
    },
    {
      "cell_type": "markdown",
      "id": "0a23885a",
      "metadata": {
        "id": "0a23885a"
      },
      "source": [
        "#### Chapter 3.3 - Batch Normalization\n",
        "\n",
        "Batch Normalization is a technique to improve the training of deep neural networks by normalizing the inputs to each layer. It was introduced to address the problem of internal covariate shift, which refers to the change in the distribution of layer inputs during training as the parameters of the previous layers change.\n",
        "\n",
        "**Intuition:**\n",
        "The idea behind Batch Normalization is to normalize the inputs to each layer so that they have a mean of 0 and a standard deviation of 1. This ensures that the inputs to each layer are on a similar scale, which helps the network learn faster and more effectively. By normalizing the inputs, Batch Normalization reduces the sensitivity of the network to the initialization of weights and allows for the use of higher learning rates.\n",
        "\n",
        "**How it works:**\n",
        "1. For each mini-batch, Batch Normalization computes the mean and variance of the inputs.\n",
        "2. The inputs are then normalized using these statistics:\n",
        "    $\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
        "    where $\\mu$ is the mean, $\\sigma^2$ is the variance, and $\\epsilon$ is a small constant added for numerical stability.\n",
        "3. To allow the network to learn the optimal scale and shift for the normalized inputs, two learnable parameters, $\\gamma$ (scale) and $\\beta$ (shift), are introduced:\n",
        "    $y = \\gamma \\hat{x} + \\beta$\n",
        "\n",
        "**Problems it solves:**\n",
        "1. **Internal Covariate Shift:** By normalizing the inputs to each layer, Batch Normalization reduces the changes in the distribution of layer inputs during training, making the optimization process more stable.\n",
        "2. **Faster Training:** Normalized inputs allow for the use of higher learning rates, leading to faster convergence.\n",
        "3. **Regularization Effect:** Batch Normalization introduces some noise due to the mini-batch statistics, which acts as a form of regularization and reduces the need for other regularization techniques like Dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff313393",
      "metadata": {
        "id": "ff313393"
      },
      "outputs": [],
      "source": [
        "# Here is how to add it into your model as a layer:\n",
        "\n",
        "bn = torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95af6dfc",
      "metadata": {
        "id": "95af6dfc"
      },
      "source": [
        "#### Chapter 3.4 - Modern Computer Vision Models\n",
        "\n",
        "AlexNet (original paper: https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) is in many senses the grandfather of modern neural networks, being the first one to successfully combine multiple GPUs for training with a deep neural network. While it is no longer in use today, the lessons learned from AlexNet very much are, and multi-GPU setups and deep convolutional neural networks remain a staple of computer vision methods.\n",
        "\n",
        "Modern Computer Vision uses a number of different models, but perhaps none is as prolific as the original ResNet, in particular the ResNet-50. Even though it is far from the strongest model available today, its flexibility, modest size, and robust performance across tasks makes it a favorite, both in general computer vision and medical computer vision, where it is commonly used as the encoder in segmentation models (more on that later). The original paper (https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) has garnered almost 300'000 citations, and its descendants have dominated challenges and paper submissions in the field for a significant amount of time.\n",
        "\n",
        "**Task 2 (up to 6 points)**: Your task is to write one of these two modern models from scratch. The points are awarded for correctly implementing these models. You can choose your own difficulty here, and can earn fewer or more points, depending on which you feel more comfortable building. AlexNet requires only components that you have already seen last week - convolutions, pooling, and linear layers, while ResNet requires you to build skip connections and bottleneck blocks from scratch.\n",
        "\n",
        "Option 1 - AlexNet **(4 points)**:\n",
        "- Building the model **(2 points)**\n",
        "- You do not have to implement the parts where multiple GPUs are required\n",
        "- Add some type of data augmentation and regularization to the training script **(1 point)**\n",
        "- Do a proper evaluation of the test set, including confusion matrix and precision-recall curves **(1 point)**\n",
        "\n",
        "Option 2 - ResNet-50 **(7 points)**\n",
        "- Correctly implementing Skip Connections **(1 point)**\n",
        "- Correctly implementing Residual/Bottleneck Blocks **(3 points)**\n",
        "- Correctly building the ResNet from these Blocks **(1 point)**\n",
        "- For BatchNorm you are allowed to simply use the existing implementation\n",
        "- Add some type of data augmentation and regularization to the training script **(1 point)**\n",
        "- Do a proper evaluation of the test set, including confusion matrix and precision-recall curves **(1 point)**\n",
        "\n",
        "You must verify that your model actually trains and is capable of solving the classification task on LiTS 2017. You should be able to explain every piece of code to the tutors that grade your solution, so if you use any help in building the model (e.g. Chat-GPT, Cursor, etc.), be prepared to explain what code blocks do what, and why you implemented them in the specific way you did, and not any other. The points are awarded for programming *and* understanding!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c7122f",
      "metadata": {
        "id": "62c7122f"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "        def __init__(self, in_channels, out_channels, stride = 1):\n",
        "            super(ResidualBlock, self).__init__()\n",
        "\n",
        "            self.conv1 = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = 1),\n",
        "                            nn.BatchNorm2d(out_channels),\n",
        "                            nn.ReLU())\n",
        "\n",
        "            self.conv2 = nn.Sequential(\n",
        "                            nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
        "                            nn.BatchNorm2d(out_channels),\n",
        "                            nn.ReLU())\n",
        "\n",
        "            self.conv2 = nn.Sequential(\n",
        "                            nn.Conv2d(out_channels, out_channels *4, kernel_size=1, stride=1, padding=1),\n",
        "                            nn.BatchNorm2d(out_channels),\n",
        "                            nn.ReLU())\n",
        "\n",
        "\n",
        "            self.downsample = None\n",
        "            if stride != 1 or in_channels != out_channels *4:\n",
        "               self.downsample = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels * 4, kernel_size=1, stride=stride, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels)\n",
        "               )\n",
        "\n",
        "            self.relu = nn.ReLU()\n",
        "\n",
        "        def forward(self, x):\n",
        "            identity = x\n",
        "\n",
        "            out = self.conv1(x)\n",
        "            out = self.conv2(out)\n",
        "            out= self.conv3(out)\n",
        "\n",
        "            if self.downsample is not None:\n",
        "                identity = self.downsample(x)\n",
        "            out += identity\n",
        "            out = self.relu(out)\n",
        "            return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_classes=3):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            ResidualBlock(64, 64),\n",
        "            ResidualBlock(256, 64),\n",
        "            ResidualBlock(256, 64)\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ResidualBlock(64, 128, stride=2),\n",
        "            ResidualBlock(128, 128),\n",
        "            ResidualBlock(128, 128),\n",
        "            ResidualBlock(128, 128)\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ResidualBlock(128, 256, stride=2),\n",
        "            ResidualBlock(256, 256),\n",
        "            ResidualBlock(256, 256),\n",
        "            ResidualBlock(256, 256),\n",
        "            ResidualBlock(256, 256),\n",
        "            ResidualBlock(256, 256)\n",
        "        )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            ResidualBlock(256, 512, stride=2),\n",
        "            ResidualBlock(512, 512),\n",
        "            ResidualBlock(512, 512)\n",
        "        )\n",
        "\n",
        "        self.fc = torch.nn.Linear(in_features = 512, out_features =out_classes)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "I9nNkUDCzyIu"
      },
      "id": "I9nNkUDCzyIu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_net = train_loop(ResNet)\n",
        "\n",
        "res_net.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = res_net(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Ae9W4DaLPJwn"
      },
      "id": "Ae9W4DaLPJwn",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}