{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a5a61491",
      "metadata": {
        "id": "a5a61491"
      },
      "source": [
        "### Chapter 3 - Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c77f8126",
      "metadata": {
        "id": "c77f8126"
      },
      "source": [
        "**This week's exercise has 3 tasks, for a total of 10.5 points. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "In this chapter, we want you to become proficient at the following tasks:\n",
        "- Building core components of modern PyTorch models\n",
        "- Assembling modern PyTorch models from components\n",
        "- Training a modern model on a real-world task and achieving passable results\n",
        "\n",
        "**Note**: Since you have already proven that you are capable of creating the core components of a typical training loop yourself, we will provide some utility code for this section. This is done so that you can focus on the important parts of this lesson, and to help us debug your code in case you need help."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4f7614",
      "metadata": {
        "id": "8e4f7614"
      },
      "source": [
        "#### Chapter 3.1 - Data Augmentation\n",
        "\n",
        "**What is data augmentation?**\n",
        "Have you ever lost your glasses and then squinted, or tried to look through a rainy window? Or looked at a false color image, maybe a forest where the trees are blue and the sky green? You can usually make an educated guess what you are looking at, even though the image you see is different than usual. This is, essentialy, what data augmentation is. It's the same data, still recognizable, but slightly altered in some way.\n",
        "\n",
        "**What is that useful for?**\n",
        "Let me begin with an anecdote that you've probably heard in the lectures. Say you have pictures of cats and dogs, and want your model to tell the two apart. How many people you know go to the park with their dogs? I imagine many. Hence, many images of dogs are dogs lying on the grass. The same is generally untrue for cats, at least I have never heard of anyone walking their cat to the park. At any rate, here is what happens when I train a neural network on these images: The model takes a shortcut. It sees a lot of green and the correct answer for these pictures is always \"Dog\". It learns \"Green = Dog\". This is what we call overfitting. We have overfitted to the existence of green in the background as a quintessential part of what makes a dog. Sometimes, we get away with this, if our data always has this correlation.\n",
        "\n",
        "Now I get some new data. A bunch of people have taken pictures of their cats, sunbathing on the terrace. The garden is in the background. Lots of green. The model, in its infinite wisdom, will at first guess that these images are of dogs. Clearly, our model's ability to tell apart cats and dogs has not generalized to this new dataset.\n",
        "\n",
        "So how can we prevent the model from taking shortcuts and encourage learning information that generalizes? We force these generalizations in training. If I gave you an image of a dog, but the grass was brown, and the dog green, you could still identify it as a dog, instead of a cat, right? And so should the model, if we can manage it. So let's also make it train using pictures of cats and dogs where the colours are different or removed. Suddenly, the shortcut solution is no longer useful, and the model must rely on shape, texture, or contextual information like a leash. The practice of color change described is a practical and useful data augmentation that is used in state-of-the-art image recognition.\n",
        "\n",
        "In addition to color changes, there is a myriad of other techniques, such as cropping, image rotation or flipping, edge filters, solarization, random noise, and many, many more. Basically, anything that you believe may eventually show up in testing data and that you want the model to generalize to, can be made into a corresponding data augmentation.\n",
        "\n",
        "**How do we use data augmentations in practice?**\n",
        "There are two ways of adding data augmentation during training. Either, you can implement it inside of your dataset, so that it only returns augmented image tensors, or right before feeding your image tensors into your model. Both options are acceptable and come with advantages and disadvantages, although the more common way is to separate dataset and augmentations. We also showcase the native PyTorch way of augmenting data below.\n",
        "\n",
        "If you are particularly eager, or want to try your hand at making image augmentation functions yourself, it can be fun and is definitely good practice. However, PyTorch comes with a large selection of image augmentations right out of the box, and in the following chapter, we will look at how to make use of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e352a55a",
      "metadata": {
        "id": "e352a55a",
        "outputId": "68e21268-4e6e-4941-ea7b-e9d0faac41cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as tt\n",
        "import torchvision.transforms.functional as ttf\n",
        "\n",
        "# Torchvision contains two ways of utilizing transforms:\n",
        "# Functional and Composed.\n",
        "\n",
        "# Functional does what it advertises - it is a function which\n",
        "# you can use on your tensors. Here is an example which performs\n",
        "# a center crop:\n",
        "\n",
        "dummy_images = torch.rand((16, 1, 256, 256))\n",
        "transformed_images = ttf.center_crop(img = dummy_images, output_size = (128, 128))\n",
        "print(transformed_images.size())\n",
        "\n",
        "# Functional transforms have the inherent advantage of giving the\n",
        "# user very fine-grained control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "55a993c1",
      "metadata": {
        "id": "55a993c1",
        "outputId": "6944086b-fb64-4992-ba2f-7a8d1c8b4ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# The alternative is the so-called Composed form, which uses\n",
        "# classes to achieve the same result. We make a Composed Transform\n",
        "# like so:\n",
        "\n",
        "dummy_images = torch.rand((16, 1, 256, 256))\n",
        "transforms = tt.Compose([\n",
        "    tt.RandomCrop(size = (128, 128)),\n",
        "    tt.RandomHorizontalFlip()\n",
        "])\n",
        "transformed_images = transforms(dummy_images)\n",
        "print(transformed_images.size())\n",
        "\n",
        "# As you can see, Compose offers us the option of sequentially\n",
        "# executing multiple transformations in a single line of code.\n",
        "# We also get the option of using randomized augmentations,\n",
        "# where the randomization is already done for us.\n",
        "\n",
        "# In practice, either style of writing transformations is fine.\n",
        "# In fact, they are equivalent, as Compose calls the functional\n",
        "# versions of the transforms under the hood. In the case of\n",
        "# randomized augmentations, the class handles all the randomizing\n",
        "# and then calls the functional transform with the random inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b4a34c",
      "metadata": {
        "id": "b8b4a34c"
      },
      "source": [
        "**Task 1 (1+1 points)**: A complete list of Torchvision's available transforms can be found here: https://docs.pytorch.org/vision/0.9/transforms.html. Consider the task we are working on right now - working with CT images from the LiTS 2017 dataset. Which data augmentations strike you as a good idea to add to our training **(1 point)**? Which do you think are a bad idea or cannot work at all **(1 point)**? Are there any which are missing in Torchvision? If you don't know what they do, try them out and judge for yourselves. Can you think of other image types with other physics behind them? Are the rules for them going to be different?\n",
        "\n",
        "There are no definitely correct or incorrect answers here. The goal for this task is for you to be able to argue your case convincingly (to us) and think closely about your dataset. You can test your assumptions when completing the other tasks.\n",
        "\n",
        "Gute Augmentation:\n",
        "\n",
        "- Kleine Rotationen und Translationen (Verschiebungen)\n",
        "- ROI Cropp (nicht in torch)\n",
        "- Zoom / Veränderung der FOV (Organgrößen leicht verschieden)\n",
        "- minimale Intensitäts Variation (nicht in torch)\n",
        " oder simuliertes Rauschen\n",
        "\n",
        "Schlechte Augmentation:\n",
        "\n",
        "- Collor Jitter (Keine Farbkanäle, geht granicht)\n",
        "- Posterize (Nur kleine Unterschiede zur Unterschiedung)\n",
        "- Greyscale (Ist schon Grey)\n",
        "- Spiegelungen (anatomisch unkorrekt)\n",
        "- starke Intensität varriation (benötigt zu Unterscheidung)\n",
        "- Random Crop (Zu viele Bilder ohen Lesion)\n",
        "- Verzerrungen/Shear (anatomisch unkorrekt)\n",
        "\n",
        "Andere Physik\n",
        "- MRT (Intensitäten sind willkürlicher abhängig von der Pulssequenz/ Modell muss auf Intensitätschwankungen trainiert werden)\n",
        "- Ultra Schall (Bilder sind immer verzerrt/ FoV ganz anders / mehr Rauschen)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "325ff768",
      "metadata": {
        "id": "325ff768"
      },
      "source": [
        "#### Chapter 3.2 - Regularization Techniques\n",
        "\n",
        "While there are multiple definitions or guidelines for what regularization is supposed to do (see lectures), in terms of practical concerns, all regularization techniques have the same aim, expressed through different means: Improving some aspect of the performance of your deep learning models. We differentiate them, broadly, from data augmentations, because regularization techniques generally concern themselves with the learning process, e.g. loss function modifications, learning rate optimizations, temporary model modifications, etc., and *not* the underlying data in our model training.\n",
        "\n",
        "There are a number of different strategies, far too many to list all of them here, but a few particularly successful ones have made it into common use - so much so that they are more prevalent than regularization-less, \"vanilla\" deep learning. These fall into different groups, briefly discussed below.\n",
        "\n",
        "#### Additional Loss Components\n",
        "\n",
        "The loss function for any given modern optimization task is typically continuous and not always smooth everywhere. As a consequence, there are many different parameter configurations in a model that result in the same train-time loss. Not all of these express the same behavior during training or testing, however. When we modify our loss to penalize certain training behaviors, we allow the training process to select for models and parameters that give models that generalize better, converge to a solution faster, etc.,  despite often expressing the same training loss. Let's look at some examples that should be familiar from the lecture:\n",
        "\n",
        "**L1 Loss** - Often also called LASSO, L1 Loss is a penalty term added to the normal loss during training, which is defined as:\n",
        "$L_{LASSO} = \\sum_{p=1}^{P} |\\Theta_{P}|$. Growing linearly with parameter magnitude, we penalize the model. This, in turn, forces the model to use fewer and smaller weights - relying on more weights than it needs, and thus probably overfitting, is disincentivized. Similarly, we just forced our model to stick to weights near zero, which we already know is generally a preferable area for model activations to stick to.\n",
        "\n",
        "**L2 Loss** - L2 Loss is the more popular cousin of the L1 Loss, which does approximately the same thing, except the penalty is equal to the sum over all squared parameter magnitudes: $L_{LASSO} = \\sum_{p=1}^{P} |\\Theta_{P}|^2$ The reasoning behind it is similar, but it has seen far more practical adoption.\n",
        "\n",
        "**Weight Decay** - An operation that effectively performs the same duty, weight decay reduces the magnitude of weights after each backward pass, for example by subtracting a small constant or multiplying with a factor. In essence, this eliminates parameters which are rarely \"used\" and were thus likely involved in overfitting on a small amount of data anyway. Parameters that are regularly updated (and therefore probably useful), will always remain near their optimal value despite weight decay. Interestingly, Weight Decay is mathematically equivalent to L2 Loss in terms of net parameter updates.\n",
        "\n",
        "#### Training Strategies\n",
        "**Early Stopping**: For early stopping, we monitor the performance of the model on a validation set and stop training when the performance stops improving. This ensures that the model does not continue to train on the training data and potentially overfit, while also saving computational resources.\n",
        "\n",
        "**Dropout**: Dropout is a regularization technique where, during training, a random subset of neurons in a layer is \"dropped out\" (set to zero) for each forward pass. This prevents the network from relying too heavily on specific neurons and encourages it to learn more robust and generalized features. During inference, all neurons are used, but their outputs are scaled to account for the dropout during training.\n",
        "\n",
        "**Learning Rate Scheduling (LR Scheduling)**: Learning rate scheduling involves dynamically adjusting the learning rate during training. A high learning rate at the start helps the model converge quickly, while a lower learning rate later allows for fine-tuning. Common strategies include step decay, exponential decay, and cosine annealing. Proper learning rate scheduling can lead to faster convergence and better generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6cd62d1",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "f6cd62d1"
      },
      "source": [
        "**Task 1.5 (3 x 0.5 points)**: Let us implement and compare the effects of different regularization techniques on a simple neural network. To do so, follow these steps:\n",
        "\n",
        "1. Create a small neural network (e.g., 2-3 layers) and train it on the LiTS dataset without any regularization. Record the training and validation accuracy/loss. (P.S.: You can use the model from last week's exercise as a starting point.)\n",
        "2. Add L2 regularization to the model and observe how it affects the training and validation performance (Check the Adam optimizer documentation to find out how to add this regularization). Compare the results with the unregularized model.\n",
        "3. Add dropout to the model and repeat the training process (check the PyTorch documentation to find out how to add this regularization - you do not need to implement it yourself). Compare the results with the previous models.\n",
        "\n",
        "For each step, make sure to copy the relevant code snippets into a new cell, instead of modifying the existing code. This way, we can keep track of the different versions of the model and their performances.\n",
        "\n",
        "You might want to look up the documentation for implementing these techniques in PyTorch.\n",
        "\n",
        "For each regularization technique, explain how it impacts the model's performance and generalization. Which combination of techniques works best for this dataset? Why do you think that is the case?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
        "!rm -rf ./sample_data/\n",
        "!unzip -qq Clean_LiTS.zip\n",
        "!rm ./Clean_LiTS.zi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-TZP1rzWPzn",
        "outputId": "80d9745d-a69d-42a4-baf0-c02aca9293c1"
      },
      "id": "2-TZP1rzWPzn",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
            "From (redirected): https://drive.google.com/uc?id=1TItTaso19GFTPdDnynVnqJvHsCm_RGlI&confirm=t&uuid=9252cbdb-3481-441e-b075-5cb8cc37fdbd\n",
            "To: /content/Clean_LiTS.zip\n",
            "100% 2.56G/2.56G [00:54<00:00, 46.6MB/s]\n",
            "rm: cannot remove './Clean_LiTS.zi': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f91bab93",
      "metadata": {
        "id": "f91bab93"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as ttf\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import PIL\n",
        "\n",
        "class LiTS_Dataset(Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    For our sample solution, we go for the easier variant.\n",
        "\n",
        "    In this specific dataset, we don't load the images until we need them - for a\n",
        "    short training, or limited resources, this is good behavior. If you have the\n",
        "    necessary RAM to pre-load all of your data, you don't have to load the data\n",
        "    multiple times, and save compute costs in the long run. The downside is that\n",
        "    when you are trying to debug, you wait for ages every time, and if you simply\n",
        "    do not have the compute resources, you can't even do it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv: str, mode: str):\n",
        "\n",
        "        self.csv = csv\n",
        "        self.data = pd.read_csv(self.csv)\n",
        "        self.mode = mode\n",
        "        assert mode in [\"train\", \"val\", \"test\"] # has to be train, val, or test data - if not, assert throws an error\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        file = self.data.loc[idx, \"filename\"]\n",
        "        with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "            f = f.convert(\"L\")\n",
        "            image = ttf.pil_to_tensor(f)\n",
        "\n",
        "        image = image.to(dtype = torch.float32)\n",
        "        image -= torch.min(image)\n",
        "        image /= torch.max(image)\n",
        "\n",
        "        liver_visible = self.data.loc[idx, \"liver_visible\"]\n",
        "        lesion_visible = self.data.loc[idx, \"lesion_visible\"]\n",
        "        # Note that targets must have the data type torch.long - a 64-bit integer,\n",
        "        # unlike the image tensor, which is usually a 32-bit float, the default\n",
        "        # dtype for tensors when none is given\n",
        "        if lesion_visible and liver_visible:\n",
        "            target = torch.tensor(2, dtype = torch.long)\n",
        "        elif not lesion_visible and liver_visible:\n",
        "            target = torch.tensor(1, dtype = torch.long)\n",
        "        elif not lesion_visible and not liver_visible:\n",
        "            target = torch.tensor(0, dtype = torch.long)\n",
        "        else:\n",
        "            print(\n",
        "                idx,\n",
        "                lesion_visible,\n",
        "                liver_visible,\n",
        "                self.data.loc[idx, \"liver_visible\"],\n",
        "                self.data.loc[idx, \"lesion_visible\"],\n",
        "                self.data.loc[idx, \"filename\"]\n",
        "                )\n",
        "            raise ValueError(\"Invalid target\")\n",
        "\n",
        "        return image, target\n",
        "\n",
        "train_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/train_classes.csv\", mode=\"train\")\n",
        "val_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/val_classes.csv\", mode=\"val\")\n",
        "test_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/test_classes.csv\", mode=\"test\")\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 0,\n",
        "    shuffle = False,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    drop_last = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bdaccb5c",
      "metadata": {
        "id": "bdaccb5c"
      },
      "outputs": [],
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Insert your model here\n",
        "\n",
        "class YourModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_classes=3):\n",
        "        super(YourModel, self).__init__()\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=(3, 3), padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(3, 3), padding=1)\n",
        "        self.conv3 = torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
        "        self.conv4 = torch.nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.pool = torch.nn.MaxPool2d(2)\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(in_features = 128 * 16 * 16, out_features = 256)\n",
        "        self.fc2 = torch.nn.Linear(in_features = 256, out_features =3)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.pool(self.relu(self.conv4(x)))\n",
        "\n",
        "        x = x.flatten(start_dim=1)\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5efe25fa",
      "metadata": {
        "id": "5efe25fa"
      },
      "outputs": [],
      "source": [
        "# Now for the training loop\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_loop(your_model):\n",
        "    \"\"\"\n",
        "    This function runs your train loop and returns the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = your_model(in_channels = 1, out_classes = 3)\n",
        "    model = model.to(device = device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(),\n",
        "        lr=1e-4,\n",
        "        weight_decay=1e-5  # L2\n",
        "    )\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # This uses the length of the current set - \"train\"\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "            # Validation mode on\n",
        "            model.eval()\n",
        "\n",
        "            # Don't track gradients for validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                hits = 0\n",
        "                losses = []\n",
        "                batch_sizes = []\n",
        "\n",
        "                for step, (data, targets) in enumerate(val_dataloader):\n",
        "\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "                    loss = loss_criterion(predictions, targets)\n",
        "                    losses.append(loss.item())\n",
        "                    batch_sizes.append(data.size()[0])\n",
        "\n",
        "                    class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                    hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "                accuracy = hits / len(val_dataloader.dataset)\n",
        "                avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "                print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9e04157a",
      "metadata": {
        "id": "9e04157a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb500fa-2ae1-46e2-b9c2-b7662d8dd6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.1075\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 0.8566\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 0.7830\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 0.7974\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 0.8077\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 0.4525\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.5593\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 0.6973\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.7749\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 0.5155\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.3345\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 0.5802\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.3502\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 0.3660\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 0.3699\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.2065\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.2946\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.6406\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.5193\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.5941\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 0.2893\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 0.3496\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 0.4496\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.1414\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.5291\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 0.2339\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.3085\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 0.1751\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.3643\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 0.5271\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.1801\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 0.3562\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.2343\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 0.2683\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 0.1152\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 0.3175\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 0.3038\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 0.2031\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 0.1912\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.4685\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 0.1026\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.3303\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.2287\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.1968\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.0686\n",
            "Epoch: 1,\t Validation Loss: 0.3738,\t Accuracy: 0.8210\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.4633\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 0.0462\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.1123\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 0.1057\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 0.2145\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 0.1078\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.3976\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.0802\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.3113\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.2235\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.1056\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.2127\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.0825\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.2421\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 0.0295\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 0.1711\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.0654\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 0.0600\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.0571\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.0470\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.2970\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.1798\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 0.0888\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.1362\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 0.0420\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 0.0790\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.3112\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.2620\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.0452\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 0.3950\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 0.0355\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.3953\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 0.0961\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 0.1230\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.1643\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 0.3361\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 0.0355\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 0.3026\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.1351\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.1022\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.0347\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 0.1320\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 0.0904\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.1161\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.0600\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 0.6002\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.0344\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 0.0346\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 0.1186\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 0.0524\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.0725\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 0.0925\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.0069\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.1487\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.1933\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.1501\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.0535\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.1107\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.0284\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.0342\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 0.0438\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 0.1188\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.0228\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.2073\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.2091\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.1679\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.0252\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 0.2725\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.1063\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.0218\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.0214\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.2336\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.0028\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.0221\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.0425\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.0468\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.2065\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.1598\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.1102\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 0.0952\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.0878\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.0559\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 0.8844\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.0122\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 0.1170\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.0069\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.1344\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.0305\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.0918\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.0420\n",
            "Epoch: 3,\t Validation Loss: 0.4414,\t Accuracy: 0.8782\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.0350\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 0.1674\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.1505\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.4304\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 0.0359\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.0338\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.0875\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.0538\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.1281\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 0.1408\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.1306\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 0.0887\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.0316\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.0334\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.0125\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 0.0456\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 0.0334\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.2271\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.0005\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.0420\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.1152\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.0235\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 0.2846\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.0082\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 0.0395\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.1962\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.3311\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 0.1505\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.0834\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 0.0508\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.0637\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.0033\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 0.1278\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 0.0397\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 0.0105\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.0015\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.0273\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.0177\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.0995\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 0.0473\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.0359\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.0039\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.0073\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 0.0030\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.0577\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.0169\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 0.0293\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.1836\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.0674\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.1578\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.3221\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.0413\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.0373\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.1097\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 0.0198\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 0.1680\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.2406\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 0.0087\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.1159\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.0510\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.0355\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.0464\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.0264\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.0028\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.0036\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.0213\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.0210\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.0081\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.0426\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.0110\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 0.0142\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.0267\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 0.0489\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.0081\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 0.2202\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.0075\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 0.0379\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.0258\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.0749\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.0063\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 0.0068\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.2720\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 0.1368\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 0.0775\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 0.0071\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.0385\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.0307\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.0815\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 0.0286\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 0.0255\n",
            "Epoch: 5,\t Validation Loss: 0.6301,\t Accuracy: 0.8555\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.0256\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 0.0147\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 0.0044\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.0009\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 0.0552\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.0528\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.0237\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.0099\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.0028\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 0.2221\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.2079\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 0.0021\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.0672\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.0083\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.0154\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.1221\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 0.0109\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.0061\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.0858\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.0071\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 0.0416\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.0304\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 0.0002\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.0019\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.0131\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.0450\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.0070\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 0.0600\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 0.0163\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.0597\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.0061\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.0144\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.0070\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.0149\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 0.0243\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.0296\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 0.0039\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 0.0162\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.2297\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.0497\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 0.0117\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 0.0100\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 0.0851\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.0017\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.0336\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 0.0671\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 0.0199\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 0.0140\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.2563\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.0313\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.0617\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 0.3983\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.0336\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.0016\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.0023\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 0.0549\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 0.1808\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 0.0199\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.0420\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.2828\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.0222\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.0622\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.0025\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 0.0024\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 0.0132\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.0991\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 0.0061\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 0.0864\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 0.1113\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 0.0007\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 0.0553\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.0073\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.0011\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 0.0013\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 0.1678\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.0723\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 0.0644\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.0070\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.4174\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 0.0385\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 0.2029\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.0023\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 0.1028\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.0069\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.0090\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.0322\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 0.0779\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.0209\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.0711\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.0017\n",
            "Epoch: 7,\t Validation Loss: 0.7495,\t Accuracy: 0.8388\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.0102\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.0008\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.0790\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.0032\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.0802\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 0.0245\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 0.0016\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.0397\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.1827\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 0.0277\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 0.0031\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 0.2655\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.0338\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.1478\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.1111\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 0.1256\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.0224\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.0388\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.0160\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.0043\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 0.0038\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 0.0898\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 0.0127\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.0317\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.0703\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.0013\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.0444\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 0.0617\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 0.0067\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 0.0024\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 0.0012\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.0124\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.1688\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.0486\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.1586\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.0091\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.0071\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.0066\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.0127\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.0685\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.0424\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.1097\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.0561\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.0055\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.0079\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 0.1827\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.0019\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.0859\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.0367\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.0566\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 0.1251\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.0801\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 0.0141\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.0152\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.0530\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.1060\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.0024\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.0278\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.0066\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 0.0051\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.0006\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.0057\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 0.0978\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.0700\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.0106\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.1115\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.0177\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.2328\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.0830\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.0064\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 0.0167\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.0070\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.0465\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.0073\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 0.0010\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.0488\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.0038\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.1829\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.0346\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.0003\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 0.0872\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.0054\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 0.0694\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.0027\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.0090\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 0.0014\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.0106\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.1027\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 0.0065\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.0171\n",
            "Epoch: 9,\t Validation Loss: 0.8027,\t Accuracy: 0.8394\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.0553\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 0.0356\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.0037\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 0.0738\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 0.0001\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.0403\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.0432\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 0.0000\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.0027\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 0.0064\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 0.0036\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.0012\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.0013\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.0348\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.0078\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.0005\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.0420\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 0.0246\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.0094\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.0007\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 0.0144\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 0.0010\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.0107\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 0.0048\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.0524\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 0.0065\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.0013\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.0504\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.0135\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.0856\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 0.0005\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.0044\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 0.0728\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 0.0038\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.2034\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 0.0092\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.0337\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.0909\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.0276\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 0.0043\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.1775\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.0027\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 0.0021\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.1706\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.0033\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.0980\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.0067\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.0055\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 0.3056\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.1204\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.0039\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 0.0017\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.0049\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 0.0047\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 0.0060\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.0162\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.0000\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.0026\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.0233\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.0028\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 0.0885\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 0.0091\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.0860\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.1064\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.0006\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.0013\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.0001\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.0451\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.0069\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.0181\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.0034\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 0.0059\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 0.0016\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.0076\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.0009\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.2746\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.0224\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.1644\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.0041\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.0094\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 0.0038\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 0.0032\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 0.0005\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.0120\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.0340\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 0.0187\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.0258\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 0.0504\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.0034\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.0017\n",
            "Epoch: 11,\t Validation Loss: 0.9793,\t Accuracy: 0.8427\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.0328\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.0253\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.0006\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.0035\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.0076\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 0.0002\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 0.1320\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 0.0171\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 0.0004\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.0115\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 0.0043\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.0359\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.0068\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.0007\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.0014\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.1962\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.2063\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.0114\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.0607\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.0012\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.0048\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.0045\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.0060\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.3147\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.0143\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.0024\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.0085\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.0066\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.0264\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.2312\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.0038\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 0.0007\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.0411\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.1321\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 0.0001\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.0002\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 0.1867\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.0956\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 0.0283\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.0090\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.0008\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.0011\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.0153\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.0467\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 0.0171\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.0033\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.0108\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.0230\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 0.1186\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.0041\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.0009\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 0.1071\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.0134\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 0.0004\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.0005\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.0237\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 0.0071\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.0045\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 0.0303\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.0203\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.0000\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 0.0023\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.0015\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.0224\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.0062\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 0.0185\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.0002\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 0.0062\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.0114\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.0054\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.0214\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.0005\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.0032\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.0014\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.0028\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.0706\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.0201\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.0002\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.0003\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.0060\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 0.0002\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.0047\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.0558\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.0082\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 0.0017\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.0023\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.0009\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 0.1889\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 0.0213\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.0346\n",
            "Epoch: 13,\t Validation Loss: 0.8704,\t Accuracy: 0.8417\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.0259\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.0109\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.0002\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.0115\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 0.2819\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 0.0296\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 0.1423\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.0019\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.1106\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.0188\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.0258\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.0019\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 0.0006\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.0050\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.0000\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.0200\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.1734\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 0.0011\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.0252\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.0073\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.0040\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 0.1047\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 0.0137\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.0044\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.0072\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.0039\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.0261\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.0006\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.0184\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 0.0079\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.2213\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 0.0072\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.1741\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.1205\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.0005\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.1363\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.0586\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.0009\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.0498\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.0056\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.0780\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.0043\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.0038\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.0101\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 0.0081\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.0838\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.0051\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.0189\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.0006\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.0003\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.0038\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 0.0119\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.0225\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.0783\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.0033\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.0212\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.0018\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.0011\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.0083\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 0.0012\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 0.0023\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.0017\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.0017\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.0018\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 0.0029\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.0184\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.0001\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.1743\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 0.0019\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.0293\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.0008\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.0677\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 0.0078\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.1667\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.0006\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.0217\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 0.1964\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.0040\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 0.0094\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 0.0007\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.0013\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 0.0035\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 0.0017\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.0240\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 0.0103\n",
            "Epoch: 15,\t Validation Loss: 0.9800,\t Accuracy: 0.8529\n"
          ]
        }
      ],
      "source": [
        "trained_model = train_loop(YourModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "10ff035d",
      "metadata": {
        "id": "10ff035d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c053d889-6e73-4751-b08c-dbf38d569657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.5275,\t Accuracy: 0.7574\n"
          ]
        }
      ],
      "source": [
        "# Standard\n",
        "\n",
        "trained_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "605399f3",
      "metadata": {
        "id": "605399f3"
      },
      "source": [
        "### L2-Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1befc94e",
      "metadata": {
        "id": "1befc94e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024e9c76-93d4-4f07-c3e2-c29e059a0a35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.0972\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 0.8323\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 1.0817\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 0.6268\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 0.4279\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 0.3117\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.4290\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 0.2703\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.7447\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 0.3775\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.2129\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 0.1579\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.3333\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 0.1405\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 0.2533\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.4812\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.3356\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.4838\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.8975\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.4541\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 0.1665\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 0.4049\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 0.5057\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.2664\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.1968\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 0.2770\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.1471\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 0.4732\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.2510\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 0.5792\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.4297\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 0.2076\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.1850\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 0.1360\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 0.5341\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 0.7085\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 0.2512\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 0.1599\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 0.2412\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.6160\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 0.1479\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.3832\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.0626\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.3258\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.2930\n",
            "Epoch: 1,\t Validation Loss: 0.4341,\t Accuracy: 0.7966\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.0997\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 0.4686\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.4746\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 0.2070\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 0.1875\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 0.0869\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.1468\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.1044\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.0953\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.1804\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.5556\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.2552\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.2453\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.5506\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 0.4203\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 0.4817\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.2817\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 0.1155\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.0593\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.1379\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.2283\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.1579\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 0.0694\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.2642\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 0.4343\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 0.0555\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.2417\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.1602\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.0625\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 0.1049\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 0.1192\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.1698\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 0.0878\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 0.2876\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.4132\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 0.1894\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 0.0978\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 0.2060\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.2262\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.1390\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.1145\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 0.1949\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 0.0558\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.1484\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.0782\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 0.3131\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.2481\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 0.0466\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 0.0531\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 0.1179\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.1328\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 0.1113\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.2663\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.2150\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.3190\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.1221\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.0875\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.0919\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.2686\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.1772\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 0.0289\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 0.0270\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.1236\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.0752\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.1373\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.0021\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.0024\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 0.0998\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.0887\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.0641\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.0705\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.0517\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.1397\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.0226\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.1497\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.0223\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.1565\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.0304\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.1819\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 0.0440\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.0802\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.1003\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 0.1373\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.0687\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 0.0774\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.0193\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.0127\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.2829\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.2302\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.1050\n",
            "Epoch: 3,\t Validation Loss: 0.4426,\t Accuracy: 0.8506\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.0402\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 0.0604\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.0479\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.2011\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 0.0399\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.0197\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.0280\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.1379\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.1166\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 0.0627\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.0637\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 0.1345\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.1359\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.0180\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.0419\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 0.0931\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 0.2518\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.0645\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.0545\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.3491\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.0534\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.0625\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 0.0742\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.0292\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 0.0216\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.0930\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.0887\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 0.0294\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.0042\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 0.1024\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.0718\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.1095\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 0.0699\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 0.0025\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 0.4298\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.0559\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.0346\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.0357\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.0105\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 0.0127\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.1110\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.0366\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.0247\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 0.1260\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.2116\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.0822\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 0.0148\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.0412\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.0500\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.0154\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.0256\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.1492\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.3545\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.0812\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 0.0197\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 0.0681\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.0371\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 0.1234\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.0228\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.0595\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.0227\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.0087\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.0058\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.1383\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.7741\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.0893\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.0177\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.0057\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.0529\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.0154\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 0.0552\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.0355\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 0.0469\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.0323\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 0.0428\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.0067\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 0.2801\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.0250\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.0073\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.1308\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 0.0100\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.0700\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 0.0163\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 0.0251\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 0.0318\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.0281\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.0119\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.1282\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 0.0344\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 0.0586\n",
            "Epoch: 5,\t Validation Loss: 0.4858,\t Accuracy: 0.8631\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.0229\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 0.1978\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 0.0056\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.0242\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 0.1644\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.1012\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.1314\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.1936\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.0173\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 0.0112\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.0677\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 0.1705\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.0282\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.0713\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.0884\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.1176\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 0.0465\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.0127\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.0527\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.0007\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 0.0771\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.0164\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 0.0092\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.0835\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.0082\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.0316\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.0183\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 0.0914\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 0.1072\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.0497\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.0026\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.0071\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.0008\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.1373\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 0.5145\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.0732\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 0.1505\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 0.0353\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.0300\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.0435\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 0.0065\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 0.0040\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 0.0063\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.0063\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.0039\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 0.0182\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 0.0316\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 0.0416\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.0810\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.0214\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.0017\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 0.0473\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.0784\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.0045\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.1099\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 0.0005\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 0.0019\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 0.0476\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.0184\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.0105\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.0404\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.0068\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.0241\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 0.0999\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 0.0081\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.0577\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 0.1989\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 0.0195\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 0.0065\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 0.0168\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 0.1166\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.0428\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.0319\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 0.1318\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 0.0151\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.0009\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 0.0412\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.0230\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.0192\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 0.0268\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 0.0025\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.0444\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 0.3677\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.0045\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.0334\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.0604\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 0.0695\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.0467\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.0458\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.0011\n",
            "Epoch: 7,\t Validation Loss: 0.6801,\t Accuracy: 0.8509\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.0002\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.0026\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.0744\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.1598\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.0129\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 0.1502\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 0.0741\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.0445\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.0236\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 0.0884\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 0.1735\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 0.2823\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.0396\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.0013\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.0010\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 0.0948\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.1044\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.0133\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.0332\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.0097\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 0.1103\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 0.1368\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 0.0069\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.1641\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.0780\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.0892\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.0857\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 0.0032\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 0.0201\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 0.0245\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 0.0121\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.0017\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.1509\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.0076\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.0200\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.1657\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.0447\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.0062\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.0092\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.0271\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.0268\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.0006\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.0197\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.0147\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.0024\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 0.0395\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.0376\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.0050\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.0053\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.0594\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 0.0153\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.0036\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 0.0282\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.0093\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.0443\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.3857\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.0032\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.0053\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.0309\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 0.0430\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.0002\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.0564\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 0.0135\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.0064\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.0026\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.0083\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.0604\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.0350\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.0205\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.3198\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 0.0524\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.0113\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.0906\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.0036\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 0.0248\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.0956\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.0472\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.0244\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.0538\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.0212\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 0.0729\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.0528\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 0.0005\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.0537\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.0080\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 0.0018\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.0130\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.2045\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 0.0014\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.0093\n",
            "Epoch: 9,\t Validation Loss: 0.7522,\t Accuracy: 0.8414\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.0184\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 0.0346\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.0909\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 0.0219\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 0.0088\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.0110\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.0021\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 0.0287\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.0262\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 0.0058\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 0.0086\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.0238\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.0325\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.0395\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.0028\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.0102\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.0251\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 0.0156\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.0306\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.0182\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 0.2505\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 0.0284\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.0093\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 0.0107\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.0140\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 0.0042\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.0057\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.0112\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.0033\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.0021\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 0.0035\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.0407\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 0.0013\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 0.1458\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.0060\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 0.0035\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.0003\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.0076\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.0016\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 0.0098\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.0043\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.3117\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 0.0025\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.0529\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.0914\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.0017\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.0085\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.0006\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 0.0065\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.0055\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.0000\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 0.0083\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.0901\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 0.1084\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 0.0082\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.0237\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.0146\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.0003\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.0004\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.0040\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 0.0063\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 0.0947\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.0058\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.0094\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.0309\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.0026\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.0066\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.3179\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.0346\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.1950\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.0048\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 0.0228\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 0.0122\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.0234\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.0001\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.0016\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.0049\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.0209\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.0038\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.0253\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 0.0231\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 0.0145\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 0.0025\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.0031\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.0128\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 0.0656\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.0397\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 0.0014\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.0532\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.1296\n",
            "Epoch: 11,\t Validation Loss: 0.7145,\t Accuracy: 0.8463\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.0093\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.1897\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.0075\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.0994\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.0653\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 0.0118\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 0.0115\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 0.0036\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 0.0306\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.1595\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 0.1678\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.0333\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.0007\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.0678\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.0010\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.1178\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.2427\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.0025\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.0139\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.0291\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.0012\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.0115\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.0009\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.0796\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.0123\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.1746\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.0214\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.0050\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.0056\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.0135\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.0009\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 0.0007\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.0074\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.0034\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 0.0076\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.0071\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 0.0856\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.0046\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 0.0024\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.0041\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.0888\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.0009\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.1538\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.0022\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 0.0177\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.2432\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.0124\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.1780\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 0.0040\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.0030\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.0102\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 0.2379\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.0018\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 0.0062\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.0036\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.0003\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 0.0004\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.0551\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 0.0047\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.0004\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.0113\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 0.0008\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.1655\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.0434\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.0795\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 0.0015\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.0014\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 0.0005\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.0320\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.0301\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.0001\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.0991\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.0074\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.0013\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.0008\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.0040\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.0012\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.0160\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.0012\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.0017\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 0.0024\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.0446\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.0417\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.0023\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 0.0013\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.0080\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.0252\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 0.0042\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 0.0008\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.0120\n",
            "Epoch: 13,\t Validation Loss: 0.7920,\t Accuracy: 0.8506\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.0004\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.0028\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.0021\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.0019\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 0.1195\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 0.0001\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 0.0007\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.1709\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.0184\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.0258\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.0145\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.0104\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 0.0001\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.0005\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.0006\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.0000\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.2169\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 0.0072\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.0895\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.0125\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.0046\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 0.0045\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 0.0011\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.0063\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.1349\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.0053\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.0018\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.0172\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.0007\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 0.0069\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.0104\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 0.0175\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.0064\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.0026\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.0039\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.0034\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.0428\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.0138\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.0285\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.0002\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.0265\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.0014\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.0020\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.0004\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 0.0328\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.0304\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.0101\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.0028\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.0053\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.0021\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.0925\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 0.0002\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.0023\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.0044\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.0025\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.0000\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.0079\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.0045\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.0250\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.0298\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.0008\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.0064\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.0098\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 0.0018\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.0021\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.0168\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.0134\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 0.0220\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.0034\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.0016\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.0265\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 0.0069\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.0027\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.0009\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.0063\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 0.1129\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.0037\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.0086\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.0011\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 0.0436\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.0023\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 0.0109\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 0.0426\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.0045\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 0.0039\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 0.0001\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.0014\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 0.0069\n",
            "Epoch: 15,\t Validation Loss: 0.8842,\t Accuracy: 0.8559\n",
            "Test Loss: 1.3043,\t Accuracy: 0.7617\n"
          ]
        }
      ],
      "source": [
        "\n",
        "  # optimizer = torch.optim.Adam(\n",
        "      #  params=model.parameters(),\n",
        "       # lr=1e-4,\n",
        "       # weight_decay=1e-5  # L2\n",
        "  # )\n",
        "\n",
        "trained_model1 = train_loop(YourModel)\n",
        "\n",
        "trained_model1.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model1(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e200dbd0",
      "metadata": {
        "id": "e200dbd0"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "811813fc",
      "metadata": {
        "id": "811813fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49bfbc6-e06e-43b8-ec1d-69e3adca4196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.1149\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 1.2823\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 0.8730\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 0.6028\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 0.5060\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 0.8361\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.4397\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 0.4814\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.3721\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 0.1336\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.2689\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 0.4442\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.2130\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 0.3680\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 0.2229\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.3230\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.2105\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.4599\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.4275\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.9152\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 0.4587\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 0.5262\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 0.2687\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.6977\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.2957\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 0.2359\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.3427\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 0.4487\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.2068\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 0.3059\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.7374\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 0.2780\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.3354\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 0.4805\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 0.0970\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 0.1873\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 0.3541\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 0.2331\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 0.3482\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.4552\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 0.2208\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.2523\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.4565\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.4070\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.2699\n",
            "Epoch: 1,\t Validation Loss: 0.3674,\t Accuracy: 0.8496\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.1993\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 0.2434\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.1484\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 0.0320\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 0.2110\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 0.1901\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.3943\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.4611\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.1407\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.1495\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.2380\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.0504\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.2444\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.5884\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 0.1513\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 0.2559\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.2596\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 0.2020\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.1042\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.1979\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.2694\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.0932\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 0.2952\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.0709\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 0.1113\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 0.1219\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.2967\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.2418\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.5860\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 0.0682\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 0.1732\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.2199\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 0.1762\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 0.0949\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.0919\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 0.0770\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 0.0548\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 0.2149\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.2462\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.2797\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.0127\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 0.0353\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 0.0605\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.2063\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.0675\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 0.0699\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.2213\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 0.4391\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 0.0494\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 0.0848\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.0121\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 0.1443\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.2986\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.2082\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.0658\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.0081\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.0508\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.2455\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.0517\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.0670\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 0.2268\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 0.1143\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.0696\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.1682\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.0663\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.0849\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.1569\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 0.1256\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.1711\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.0508\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.0443\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.0795\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.2195\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.0615\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.0463\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.0071\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.0606\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.0267\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.0889\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 0.0385\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.0713\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.2281\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 0.0310\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.0808\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 0.0708\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.1807\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.2612\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.0660\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.0880\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.2463\n",
            "Epoch: 3,\t Validation Loss: 0.3587,\t Accuracy: 0.8684\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.1308\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 0.0869\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.0535\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.0182\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 0.0074\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.0760\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.0335\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.0401\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.0518\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 0.0912\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.0221\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 0.0999\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.0277\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.0401\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.0723\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 0.0654\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 0.1367\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.1084\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.1464\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.0158\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.2645\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.0157\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 0.0794\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.0097\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 0.0714\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.0003\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.0121\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 0.0745\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.0533\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 0.0265\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.1953\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.0569\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 0.0184\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 0.0267\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 0.0778\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.1430\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.0666\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.0425\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.0121\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 0.0132\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.1215\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.2463\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.1613\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 0.1021\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.0580\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.1506\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 0.0046\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.1037\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.1474\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.0919\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.0935\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.2689\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.0013\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.1317\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 0.0219\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 0.0478\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.0274\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 0.0314\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.0002\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.0636\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.0064\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.0802\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.0530\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.0710\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.0991\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.1732\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.1876\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.1782\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.0422\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.0266\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 0.0256\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.0820\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 0.0411\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.1585\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 0.0534\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.0145\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 0.2579\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.1091\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.0041\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.0402\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 0.3873\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.0679\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 0.3034\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 0.0640\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 0.0259\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.0393\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.0032\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.0685\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 0.0324\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 0.0003\n",
            "Epoch: 5,\t Validation Loss: 0.5089,\t Accuracy: 0.8664\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.1103\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 0.0134\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 0.0177\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.0029\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 0.1802\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.0895\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.2332\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.0861\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.0083\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 0.0013\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.1252\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 0.0319\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.0304\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.0779\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.0086\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.0202\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 0.0922\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.1284\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.0139\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.0009\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 0.0164\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.0304\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 0.0254\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.0022\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.0111\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.0013\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.0069\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 0.0140\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 0.0103\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.0295\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.2119\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.0058\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.0261\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.0004\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 0.0066\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.0066\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 0.0331\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 0.0028\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.2731\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.0009\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 0.0192\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 0.0948\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 0.0088\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.0114\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.0951\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 0.1346\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 0.0025\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 0.0025\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.0164\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.0223\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.0420\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 0.0510\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.2724\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.0000\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.0227\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 0.0090\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 0.0055\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 0.0214\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.0489\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.3026\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.0954\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.1233\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.1386\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 0.0309\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 0.0141\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.0806\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 0.0251\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 0.0270\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 0.0034\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 0.0527\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 0.2106\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.0759\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.0277\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 0.0908\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 0.1706\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.0229\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 0.0271\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.0011\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.1368\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 0.5651\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 0.0148\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.0212\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 0.0277\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.1439\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.0855\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.0010\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 0.0014\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.2255\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.0518\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.1519\n",
            "Epoch: 7,\t Validation Loss: 0.6503,\t Accuracy: 0.8233\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.0223\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.0001\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.0012\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.0293\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.0662\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 0.0085\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 0.0018\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.0003\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.0077\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 0.0048\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 0.0020\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 0.0049\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.0420\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.0220\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.3590\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 0.1337\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.1747\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.0356\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.0506\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.0653\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 0.0670\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 0.1716\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 0.0085\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.0410\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.0180\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.0916\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.0026\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 0.0003\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 0.0496\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 0.0045\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 0.0187\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.1354\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.0796\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.0063\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.0066\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.1053\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.1663\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.0020\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.0378\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.0006\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.3636\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.0019\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.0894\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.0397\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.0033\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 0.0270\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.0339\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.0069\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.0254\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.0441\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 0.0077\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.0273\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 0.0842\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.0009\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.0410\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.0033\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.0177\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.0051\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.0920\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 0.0015\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.0600\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.0004\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 0.0521\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.0078\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.1013\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.0047\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.0854\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.0592\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.0163\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.0131\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 0.0737\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.0155\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.1630\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.0012\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 0.0292\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.2196\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.1617\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.0218\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.0308\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.0093\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 0.0165\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.0060\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 0.0015\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.0003\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.0021\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 0.0327\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.1306\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.0013\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 0.0131\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.0008\n",
            "Epoch: 9,\t Validation Loss: 0.7162,\t Accuracy: 0.8483\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.0014\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 0.0211\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.0154\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 0.0018\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 0.0058\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.0086\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.0009\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 0.1190\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.0264\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 0.0077\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 0.0024\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.1022\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.0027\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.0112\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.0031\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.0215\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.0943\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 0.0016\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.0038\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.0076\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 0.0138\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 0.0003\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.0317\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 0.0044\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.1382\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 0.0004\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.0617\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.0071\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.0425\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.0828\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 0.0102\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.0834\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 0.0832\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 0.1592\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.0649\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 0.0286\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.0184\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.1238\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.0002\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 0.0043\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.0005\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.0254\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 0.0009\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.0808\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.0349\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.2672\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.0519\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.0082\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 0.0005\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.0032\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.1187\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 0.1685\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.0165\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 0.0869\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 0.0238\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.0009\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.0186\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.0030\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.1023\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.0190\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 0.0063\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 0.0018\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.0416\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.0130\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.0002\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.0017\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.0529\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.0021\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.0020\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.0156\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.0102\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 0.0049\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 0.0173\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.0125\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.0016\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.0208\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.3299\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.1955\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.1045\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.0024\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 0.0062\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 0.0040\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 0.0055\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.0012\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.0099\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 0.0272\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.0572\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 0.0374\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.0014\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.0115\n",
            "Epoch: 11,\t Validation Loss: 0.7657,\t Accuracy: 0.8322\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.0031\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.0395\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.0062\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.0271\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.0020\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 0.0195\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 0.0185\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 0.0596\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 0.0011\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.0018\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 0.0018\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.3715\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.1456\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.0169\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.0634\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.0010\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.1326\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.1476\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.0000\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.0005\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.0154\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.0095\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.0744\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.0003\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.0629\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.1229\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.0007\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.0006\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.0031\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.0006\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.0028\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 0.0372\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.0015\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.0016\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 0.0706\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.0892\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 0.0008\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.0011\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 0.0535\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.0002\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.0107\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.0615\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.0006\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.0205\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 0.0040\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.0174\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.0040\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.0165\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 0.0914\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.0740\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.0721\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 0.0528\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.0088\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 0.0168\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.0014\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.0248\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 0.0023\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.0336\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 0.0035\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.0040\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.0042\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 0.0306\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.0019\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.1045\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.0023\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 0.0012\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.0835\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 0.1522\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.0730\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.0095\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.0012\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.3631\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.0073\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.0098\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.0770\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.0011\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.0203\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.0005\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.2751\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.1132\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 0.0013\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.0171\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.0145\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.0159\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 0.0332\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.0184\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.0033\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 0.0050\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 0.1257\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.0926\n",
            "Epoch: 13,\t Validation Loss: 0.7625,\t Accuracy: 0.8388\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.0031\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.0036\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.0045\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.0131\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 0.0887\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 0.0085\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 0.0891\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.0463\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.0633\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.0002\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.0007\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.0243\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 0.0605\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.0022\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.0012\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.0008\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.0180\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 0.0189\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.2259\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.0102\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.0003\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 0.0083\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 0.0012\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.0043\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.0170\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.0092\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.0177\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.0003\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.0562\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 0.0031\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.0455\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 0.1568\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.0089\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.0004\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.0614\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.0931\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.0012\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.1128\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.0067\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.0000\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.0143\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.2762\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.0057\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.0150\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 0.1859\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.0049\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.1846\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.0548\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.0201\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.0001\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 0.0098\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.0405\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.0052\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.0178\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.0054\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.0021\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.0510\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.0076\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.0025\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.0014\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.0118\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.0669\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 0.0235\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.0001\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 0.0273\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.0084\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.0007\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.0091\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 0.0002\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.0007\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.0110\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.0402\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 0.0072\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.0054\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.2024\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.0008\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 0.0503\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.0034\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.0080\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 0.0451\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.0003\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 0.0126\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 0.0038\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.0170\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 0.0006\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.0106\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 0.0004\n",
            "Epoch: 15,\t Validation Loss: 0.8414,\t Accuracy: 0.8440\n",
            "Test Loss: 1.4148,\t Accuracy: 0.7640\n"
          ]
        }
      ],
      "source": [
        "   # self.dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "   # x = self.dropout(x)\n",
        "\n",
        "trained_model2 = train_loop(YourModel)\n",
        "\n",
        "trained_model2.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model2(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2: Extra Loss; verhindert, dass das Modell auf einzelne Weights fokussiert (kein Overfitting)\n",
        "Dropout: Es wird eine Anzahl von Neuronen während jedem durchlaufen zufällig deaktiviert. Verhindert Overfitting, Modell generalsisiert besser.\n",
        "\n",
        "Viele Parameter in einem Bild. Modell kann leicht zufällige Details lernen. Dies verhindert die Regularisierung."
      ],
      "metadata": {
        "id": "XYSFPJ13qBRO"
      },
      "id": "XYSFPJ13qBRO"
    },
    {
      "cell_type": "markdown",
      "id": "0a23885a",
      "metadata": {
        "id": "0a23885a"
      },
      "source": [
        "#### Chapter 3.3 - Batch Normalization\n",
        "\n",
        "Batch Normalization is a technique to improve the training of deep neural networks by normalizing the inputs to each layer. It was introduced to address the problem of internal covariate shift, which refers to the change in the distribution of layer inputs during training as the parameters of the previous layers change.\n",
        "\n",
        "**Intuition:**\n",
        "The idea behind Batch Normalization is to normalize the inputs to each layer so that they have a mean of 0 and a standard deviation of 1. This ensures that the inputs to each layer are on a similar scale, which helps the network learn faster and more effectively. By normalizing the inputs, Batch Normalization reduces the sensitivity of the network to the initialization of weights and allows for the use of higher learning rates.\n",
        "\n",
        "**How it works:**\n",
        "1. For each mini-batch, Batch Normalization computes the mean and variance of the inputs.\n",
        "2. The inputs are then normalized using these statistics:\n",
        "    $\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
        "    where $\\mu$ is the mean, $\\sigma^2$ is the variance, and $\\epsilon$ is a small constant added for numerical stability.\n",
        "3. To allow the network to learn the optimal scale and shift for the normalized inputs, two learnable parameters, $\\gamma$ (scale) and $\\beta$ (shift), are introduced:\n",
        "    $y = \\gamma \\hat{x} + \\beta$\n",
        "\n",
        "**Problems it solves:**\n",
        "1. **Internal Covariate Shift:** By normalizing the inputs to each layer, Batch Normalization reduces the changes in the distribution of layer inputs during training, making the optimization process more stable.\n",
        "2. **Faster Training:** Normalized inputs allow for the use of higher learning rates, leading to faster convergence.\n",
        "3. **Regularization Effect:** Batch Normalization introduces some noise due to the mini-batch statistics, which acts as a form of regularization and reduces the need for other regularization techniques like Dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff313393",
      "metadata": {
        "id": "ff313393"
      },
      "outputs": [],
      "source": [
        "# Here is how to add it into your model as a layer:\n",
        "\n",
        "bn = torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95af6dfc",
      "metadata": {
        "id": "95af6dfc"
      },
      "source": [
        "#### Chapter 3.4 - Modern Computer Vision Models\n",
        "\n",
        "AlexNet (original paper: https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) is in many senses the grandfather of modern neural networks, being the first one to successfully combine multiple GPUs for training with a deep neural network. While it is no longer in use today, the lessons learned from AlexNet very much are, and multi-GPU setups and deep convolutional neural networks remain a staple of computer vision methods.\n",
        "\n",
        "Modern Computer Vision uses a number of different models, but perhaps none is as prolific as the original ResNet, in particular the ResNet-50. Even though it is far from the strongest model available today, its flexibility, modest size, and robust performance across tasks makes it a favorite, both in general computer vision and medical computer vision, where it is commonly used as the encoder in segmentation models (more on that later). The original paper (https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) has garnered almost 300'000 citations, and its descendants have dominated challenges and paper submissions in the field for a significant amount of time.\n",
        "\n",
        "**Task 2 (up to 6 points)**: Your task is to write one of these two modern models from scratch. The points are awarded for correctly implementing these models. You can choose your own difficulty here, and can earn fewer or more points, depending on which you feel more comfortable building. AlexNet requires only components that you have already seen last week - convolutions, pooling, and linear layers, while ResNet requires you to build skip connections and bottleneck blocks from scratch.\n",
        "\n",
        "Option 1 - AlexNet **(4 points)**:\n",
        "- Building the model **(2 points)**\n",
        "- You do not have to implement the parts where multiple GPUs are required\n",
        "- Add some type of data augmentation and regularization to the training script **(1 point)**\n",
        "- Do a proper evaluation of the test set, including confusion matrix and precision-recall curves **(1 point)**\n",
        "\n",
        "Option 2 - ResNet-50 **(7 points)**\n",
        "- Correctly implementing Skip Connections **(1 point)**\n",
        "- Correctly implementing Residual/Bottleneck Blocks **(3 points)**\n",
        "- Correctly building the ResNet from these Blocks **(1 point)**\n",
        "- For BatchNorm you are allowed to simply use the existing implementation\n",
        "- Add some type of data augmentation and regularization to the training script **(1 point)**\n",
        "- Do a proper evaluation of the test set, including confusion matrix and precision-recall curves **(1 point)**\n",
        "\n",
        "You must verify that your model actually trains and is capable of solving the classification task on LiTS 2017. You should be able to explain every piece of code to the tutors that grade your solution, so if you use any help in building the model (e.g. Chat-GPT, Cursor, etc.), be prepared to explain what code blocks do what, and why you implemented them in the specific way you did, and not any other. The points are awarded for programming *and* understanding!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c7122f",
      "metadata": {
        "id": "62c7122f"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "        def __init__(self, in_channels, out_channels, stride = 1):\n",
        "            super(ResidualBlock, self).__init__()\n",
        "\n",
        "            self.conv1 = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = 1),\n",
        "                            nn.BatchNorm2d(out_channels),\n",
        "                            nn.ReLU())\n",
        "\n",
        "            self.conv2 = nn.Sequential(\n",
        "                            nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
        "                            nn.BatchNorm2d(out_channels),\n",
        "                            nn.ReLU())\n",
        "\n",
        "            self.conv3 = nn.Sequential(\n",
        "                            nn.Conv2d(out_channels, out_channels *4, kernel_size=1, stride=1, padding=0),\n",
        "                            nn.BatchNorm2d(out_channels * 4))\n",
        "\n",
        "\n",
        "\n",
        "            self.downsample = None\n",
        "            if stride != 1 or in_channels != out_channels *4:\n",
        "               self.downsample = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels * 4, kernel_size=1, stride=stride, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels * 4)\n",
        "               )\n",
        "\n",
        "            self.relu = nn.ReLU()\n",
        "\n",
        "        def forward(self, x):\n",
        "            identity = x\n",
        "\n",
        "            out = self.conv1(x)\n",
        "            out = self.conv2(out)\n",
        "            out= self.conv3(out)\n",
        "\n",
        "            if self.downsample is not None:\n",
        "                identity = self.downsample(x)\n",
        "            out += identity\n",
        "            out = self.relu(out)\n",
        "            return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_classes=3):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            ResidualBlock(64, 64),\n",
        "            ResidualBlock(256, 64),\n",
        "            ResidualBlock(256, 64)\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ResidualBlock(256, 128, stride=2),\n",
        "            ResidualBlock(512, 128),\n",
        "            ResidualBlock(512, 128),\n",
        "            ResidualBlock(512, 128)\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ResidualBlock(512, 256, stride=2),\n",
        "            ResidualBlock(1024, 256),\n",
        "            ResidualBlock(1024, 256),\n",
        "            ResidualBlock(1024, 256),\n",
        "            ResidualBlock(1024, 256),\n",
        "            ResidualBlock(1024, 256)\n",
        "        )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            ResidualBlock(1024, 512, stride=2),\n",
        "            ResidualBlock(2048, 512),\n",
        "            ResidualBlock(2048, 512)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(in_features = 2048, out_features =out_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "I9nNkUDCzyIu"
      },
      "id": "I9nNkUDCzyIu",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_net = train_loop(ResNet)\n",
        "\n",
        "res_net.eval()\n",
        "all_targets = []\n",
        "all_predictions = []\n",
        "\n",
        "hits = 0\n",
        "losses = []\n",
        "batch_sizes = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, targets in test_dataloader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        outputs = res_net(data)\n",
        "        loss = loss_criterion(outputs, targets)\n",
        "\n",
        "        # Loss\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size(0))\n",
        "\n",
        "        # Klassenvorhersage\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        hits += (preds == targets).sum().item()\n",
        "\n",
        "        # Für Confusion Matrix & PR\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "# Accuracy & Avg Loss\n",
        "accuracy = hits / len(test_dataloader.dataset)\n",
        "avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "\n",
        "print(f\"Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# --------------------------\n",
        "# Confusion Matrix\n",
        "# --------------------------\n",
        "cm = confusion_matrix(all_targets, all_predictions)\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Optional: Klassifikationsreport\n",
        "print(classification_report(all_targets, all_predictions))\n",
        "\n",
        "# --------------------------\n",
        "# Precision-Recall Kurven\n",
        "# --------------------------\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, _ in test_dataloader:\n",
        "        data = data.to(device)\n",
        "        probs = F.softmax(res_net(data), dim=1).cpu().numpy()\n",
        "        all_probs.append(probs)\n",
        "\n",
        "all_probs = np.vstack(all_probs)\n",
        "all_targets_np = np.array(all_targets)\n",
        "num_classes = all_probs.shape[1]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "for i in range(num_classes):\n",
        "    precision, recall, _ = precision_recall_curve((all_targets_np == i).astype(int), all_probs[:, i])\n",
        "    avg_prec = average_precision_score((all_targets_np == i).astype(int), all_probs[:, i])\n",
        "    plt.plot(recall, precision, label=f\"Class {i} (AP={avg_prec:.2f})\")\n",
        "\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curves\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ae9W4DaLPJwn",
        "outputId": "758bd0c0-c45c-42dd-de2e-927edfe14f68"
      },
      "id": "Ae9W4DaLPJwn",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.3249\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 0.9975\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 0.6169\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 0.9027\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 0.6693\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 0.3980\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.3681\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 0.6220\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.4135\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 0.3155\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.2895\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 0.2942\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.4724\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 0.3502\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 0.2060\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.3836\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.4023\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.3688\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.2875\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.3519\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 0.5233\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 0.2947\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 0.2341\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.5354\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.2916\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 0.1027\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.2245\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 0.1107\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.5436\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 0.3553\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.6635\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 0.0776\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.1844\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 0.1360\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 0.1737\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 0.2735\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 0.2920\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 0.1733\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 0.1017\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.1519\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 0.0780\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.2242\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.1705\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.2147\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.0555\n",
            "Epoch: 1,\t Validation Loss: 0.4316,\t Accuracy: 0.8085\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.0872\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 0.0740\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.0464\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 0.2250\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 0.1627\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 0.1794\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.1574\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.0520\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.1528\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.0386\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.3140\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.4009\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.1368\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.1120\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 0.0614\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 0.1562\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.0257\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 0.2556\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.0313\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.0495\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.2648\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.2743\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 0.0464\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.2397\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 0.2274\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 0.0806\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.1325\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.0369\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.0130\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 0.1765\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 0.1552\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.1262\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 0.0668\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 0.0237\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.0478\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 0.0975\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 0.0740\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 0.0852\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.0697\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.0283\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.0762\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 0.0406\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 0.1374\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.0692\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.2327\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 0.0195\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.0527\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 0.0387\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 0.0021\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 0.0068\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.0148\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 0.0151\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.0091\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.0365\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.1282\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.1257\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.0157\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.0048\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.0260\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.2727\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 0.0146\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 0.4443\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.0425\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.0633\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.1365\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.0438\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.0157\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 0.1694\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.0208\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.0721\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.0125\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.1515\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.0241\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.0300\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.0103\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.0046\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.1449\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.0144\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.0440\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 0.2132\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.2669\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.2174\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 0.0180\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.0330\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 0.0283\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.0959\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.0234\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.0329\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.0019\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.0539\n",
            "Epoch: 3,\t Validation Loss: 0.3489,\t Accuracy: 0.8460\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.0125\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 0.0358\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.1884\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.0098\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 0.0111\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.0241\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.1610\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.0863\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.4251\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 0.0732\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.0617\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 0.1239\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.0219\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.1006\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.0458\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 0.0526\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 0.0181\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.0204\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.0300\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.0704\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.0623\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.0028\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 0.0474\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.3140\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 0.0249\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.0471\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.0212\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 0.0157\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.0712\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 0.0311\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.0488\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.1183\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 0.0492\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 0.2719\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 0.0827\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.0976\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.0155\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.0143\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.0186\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 0.0045\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.0109\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.2680\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.1868\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 0.0282\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.0108\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.0188\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 0.0308\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.0218\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.0394\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.0138\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.1027\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.2283\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.0075\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.2490\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 0.0134\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 0.0619\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.0152\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 0.1327\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.2360\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.0094\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.0507\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.0222\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.1293\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.0084\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.2022\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.2139\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.0381\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.0145\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.0278\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.0117\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 0.0126\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.0385\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 0.0307\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.0354\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 0.0177\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.0404\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 0.0207\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.0025\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.0034\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.0343\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 0.0063\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.0344\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 0.0307\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 0.0073\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 0.0261\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.0165\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.3892\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.2806\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 0.0091\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 0.1486\n",
            "Epoch: 5,\t Validation Loss: 0.4118,\t Accuracy: 0.8190\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.0060\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 0.0076\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 0.2307\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.0145\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 0.0090\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.4205\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.0156\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.0515\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.0383\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 0.0122\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.0650\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 0.0197\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.0248\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.0465\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.0189\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.0440\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 0.0133\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.0529\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.0017\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.0088\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 0.0017\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.1313\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 0.0506\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.0113\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.0061\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.0210\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.0050\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 0.0112\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 0.0237\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.0051\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.0277\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.0699\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.2529\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.0054\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 0.0875\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.1932\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 0.0084\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 0.1032\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.0102\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.1491\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 0.0350\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 0.0205\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 0.1499\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 1.0663\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.0397\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 0.0047\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 0.0131\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 0.0219\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.0172\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.0007\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.0026\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 0.3294\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.0027\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.0139\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.0053\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 0.0096\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 0.0720\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 0.0159\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.0094\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.0094\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.0329\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.0667\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.0605\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 0.0111\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 0.0270\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.0047\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 0.5062\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 0.1371\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 0.0060\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 0.0661\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 0.0256\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.0042\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.0095\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 0.0178\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 0.0060\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.0036\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 0.0133\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.1990\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.0040\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 0.0695\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 0.0101\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.0155\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 0.0020\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.0078\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.0079\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.1539\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 0.0219\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.0013\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.0010\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.0014\n",
            "Epoch: 7,\t Validation Loss: 0.4997,\t Accuracy: 0.8039\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.0026\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.0076\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.0365\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.1089\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.0109\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 0.0009\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 0.0360\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.0012\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.0135\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 0.0064\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 0.0259\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 0.0073\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.0561\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.0018\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.0411\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 0.0087\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.0239\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.0078\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.1521\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.0036\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 0.0052\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 0.0389\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 0.0283\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.0009\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.0032\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.0055\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.0191\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 0.0011\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 0.0035\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 0.0181\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 0.0056\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.0014\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.2685\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.2984\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.0069\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.0054\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.0010\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.0101\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.1760\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.0208\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.0296\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.0057\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.0058\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.0484\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.0027\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 0.0029\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.0032\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.0105\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.0102\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.3122\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 0.0131\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.0047\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 0.0016\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.3510\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.5020\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.0139\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.2254\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.0041\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.0410\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 0.2333\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.0379\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.0085\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 0.0072\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.0300\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.1563\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.0004\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.0009\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.0157\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.0082\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.0026\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 0.0090\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.3219\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.0027\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.2390\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 0.0179\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.0138\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.1915\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.0623\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.1608\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.0165\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 0.0034\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.0324\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 0.0020\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.0041\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.0078\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 0.1870\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.0107\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.0041\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 0.0187\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.0017\n",
            "Epoch: 9,\t Validation Loss: 0.5792,\t Accuracy: 0.8223\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.0020\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 0.0035\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.0019\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 0.0031\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 0.0036\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.1136\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.0222\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 0.0018\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.0711\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 0.0018\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 0.8469\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.0065\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.0428\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.0093\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.0046\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.0054\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.0017\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 0.0007\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.0045\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.0162\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 0.0577\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 0.0893\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.1756\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 0.0065\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.0181\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 0.1225\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.0075\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.0574\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.0258\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.0023\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 0.0012\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.0021\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 0.0004\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 0.0123\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.0069\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 0.0491\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.0089\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.0592\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.2322\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 0.0070\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.0187\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.0106\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 0.0341\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.0382\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.0077\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.0025\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.0004\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.0474\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 0.0024\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.0719\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.0193\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 0.0005\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.0070\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 0.0154\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 0.0263\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.0042\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.0210\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.0348\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.0096\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.0698\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 0.0547\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 0.0135\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.0122\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.0092\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.0004\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.0005\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.0091\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.0492\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.0490\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.0019\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.0008\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 0.2443\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 0.0107\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.0034\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.2073\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.0063\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.0076\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.0135\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.0036\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.0055\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 0.1302\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 0.0395\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 0.0030\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.0146\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.3220\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 0.0036\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.0174\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 0.0033\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.0085\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.3265\n",
            "Epoch: 11,\t Validation Loss: 0.5540,\t Accuracy: 0.8121\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.0086\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.0271\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.0032\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.0627\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.0129\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 0.0012\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 0.0106\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 0.0114\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 0.0131\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.0048\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 0.0014\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.0519\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.0020\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.0012\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.0009\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.0020\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.0051\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.0028\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.0180\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.1089\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.0238\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.1069\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.0019\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.0278\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.0062\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.0663\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.0555\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.0041\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.0156\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.0020\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.0166\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 0.1948\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.0011\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.0019\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 0.0157\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.0588\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 0.0010\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.1428\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 0.0186\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.0090\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.0022\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.0049\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.0842\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.0186\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 0.0230\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.0052\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.0043\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.0041\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 0.0389\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.0024\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.0046\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 0.0885\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.0164\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 0.0081\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.0009\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.0005\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 0.0264\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.0009\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 0.0014\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.0010\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.1370\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 0.0155\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.0013\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.0197\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.0034\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 0.0723\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.0100\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 0.0002\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.0007\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.0200\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.1011\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.0073\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.0082\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.0029\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.0018\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.0094\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.0016\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.0042\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.0040\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.0148\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 0.0787\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.0037\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.0059\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.0161\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 0.0099\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.0075\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.0384\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 0.0034\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 0.0755\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.0171\n",
            "Epoch: 13,\t Validation Loss: 0.6017,\t Accuracy: 0.8197\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.0020\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.0101\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.0052\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.0024\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 0.0004\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 0.0668\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 0.0046\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.0013\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.0009\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.0033\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.0031\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.0165\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 0.0145\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.0038\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.0022\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.0019\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.0068\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 0.0222\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.0031\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.0047\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.0093\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 0.0005\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 0.0060\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.0032\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.0012\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.0181\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.0676\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.0114\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.1712\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 0.0634\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.0263\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 0.0021\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.0333\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.0004\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.0082\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.0377\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.0016\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.1636\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.0432\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.1682\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.0051\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.0375\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.2598\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.0057\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 0.2020\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.0044\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.0052\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.0013\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.0008\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 0.0054\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.0262\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.0141\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.5953\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.0760\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.0027\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.0133\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.0224\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.0029\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.0010\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.0012\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.0010\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 0.0010\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.0017\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 0.0895\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.0010\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.2560\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.0035\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 0.0021\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.0097\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.0090\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.0069\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 0.0009\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.0308\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.0016\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.1037\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 0.0011\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.0017\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.0009\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.0015\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 0.0012\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.0051\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 0.0012\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 0.0013\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.0015\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 0.0007\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.1148\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 0.0528\n",
            "Epoch: 15,\t Validation Loss: 0.5130,\t Accuracy: 0.8230\n",
            "Test Loss: 0.7990, Accuracy: 0.7808\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'confusion_matrix' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10704583.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Confusion Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# --------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --------------------------\n",
        "# Confusion Matrix\n",
        "# --------------------------\n",
        "cm = confusion_matrix(all_targets, all_predictions)\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Optional: Klassifikationsreport\n",
        "print(classification_report(all_targets, all_predictions))\n",
        "\n",
        "# --------------------------\n",
        "# Precision-Recall Kurven\n",
        "# --------------------------\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, _ in test_dataloader:\n",
        "        data = data.to(device)\n",
        "        probs = F.softmax(res_net(data), dim=1).cpu().numpy()\n",
        "        all_probs.append(probs)\n",
        "\n",
        "all_probs = np.vstack(all_probs)\n",
        "all_targets_np = np.array(all_targets)\n",
        "num_classes = all_probs.shape[1]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "for i in range(num_classes):\n",
        "    precision, recall, _ = precision_recall_curve((all_targets_np == i).astype(int), all_probs[:, i])\n",
        "    avg_prec = average_precision_score((all_targets_np == i).astype(int), all_probs[:, i])\n",
        "    plt.plot(recall, precision, label=f\"Class {i} (AP={avg_prec:.2f})\")\n",
        "\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curves\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3ewiBKcuqXMV",
        "outputId": "5fcf67ef-d9ce-4051-9d77-ff89adcbbdfe"
      },
      "id": "3ewiBKcuqXMV",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.95      1824\n",
            "           1       0.61      0.65      0.63       837\n",
            "           2       0.24      0.14      0.18       363\n",
            "\n",
            "    accuracy                           0.78      3024\n",
            "   macro avg       0.59      0.59      0.59      3024\n",
            "weighted avg       0.76      0.78      0.77      3024\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}