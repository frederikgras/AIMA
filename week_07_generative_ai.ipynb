{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d731a498",
      "metadata": {
        "id": "d731a498"
      },
      "source": [
        "### Chapter 3 - Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a668a88",
      "metadata": {
        "id": "4a668a88"
      },
      "source": [
        "**This week's exercise has 4 tasks, for a total of 10 points. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "In this chapter, we want you to become proficient at the following tasks:\n",
        "- Building a modern PyTorch segmentation model\n",
        "- Training a modern model on a real-world segmentation task and achieving passable results\n",
        "\n",
        "**Note**: This is the last exercise concerning pure computer vision. Starting next week, we will begin with Natural Language Processing, i.e. text data. Therefore, don't worry too much if this exercise feels hard or if you can't complete all of it ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e34867",
      "metadata": {
        "id": "96e34867"
      },
      "source": [
        "#### Chapter 3.5 - Segmentation\n",
        "\n",
        "In previous tasks, we solved classification problems - we provide some input(s), typically an image, and get out a few numbers, which are the predicted pseudo-probabilities that our input belongs to some class, such as \"tumor\" or \"no tumor\". For this exercise, we will explore a new task that is extremely common in medical AI research and in clinical practice. This task is called segmentation. In segmentation, the goal is to go from an input image to one or several segmentations (also called *segmentation maps*) of that image. For the example of LiTS, this means that our input remains the same - a 256x256 image with 1 channel. However, our model outputs and targets are now different - they also have the shape 256x256 pixels, times the number of output classes, in our case 3 (background, liver, liver+tumor). Each 256x256 output is basically a map of which pixels in the original image belong to a certain class with what (pseudo-)probability. The training objective, in its simplest form, is also the same; Cross-Entropy Loss, but per pixel, instead of per-image."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "696c73ae",
      "metadata": {
        "id": "696c73ae"
      },
      "source": [
        "To solve today's tasks, we will need to build ourselves a few new things that look almost the same as things we have already built.\n",
        "\n",
        "**Task 1 (2 points)**: We will need a new Dataset class. It is the same as usual, except this time, when we return image and target in the getitem method, our target is now also a multi-dimensional tensor of size.\n",
        "\n",
        "We will return two kinds of targets - class-index targets and one-hot encoded targets. Class-index targets you already know. Every pixel is assigned a class, which can be 0 for background, 1 for liver, and 2 for lesions. The corresponding tensor has the size $H * W$. One-hot encoded targets instead have size $C * H * W$ - each channel is one class (the 0th channel is background, etc.), and the values for each pixel in a channel are 1 if that pixel belongs to that class and 0 if not. We will need both later on - class-index targets because that is the input for the normal CrossEntropyLoss, and one-hot targets because we will use them in this format for our DiceLoss.\n",
        "\n",
        "Since the \"background\" class has no segmentations, you will have to improvise them from the existing segmentations for this task.\n",
        "\n",
        "Your dataset class should return both targets at the end of the \\_\\_getitem\\_\\_ method like this: `return image, c_targets, oh_targets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "N1bQ57_y3-YV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1bQ57_y3-YV",
        "outputId": "bac949b2-cb63-409a-c8c2-fc505c717da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
            "From (redirected): https://drive.google.com/uc?id=1TItTaso19GFTPdDnynVnqJvHsCm_RGlI&confirm=t&uuid=df427c91-495e-4c76-aa63-caf034ebcf03\n",
            "To: /content/Clean_LiTS.zip\n",
            "100% 2.56G/2.56G [00:31<00:00, 81.0MB/s]\n",
            "             filename                  liver_segmentation  liver_visible  \\\n",
            "0    volume-101_0.png    segmentation-101_livermask_0.png          False   \n",
            "1    volume-101_1.png    segmentation-101_livermask_1.png          False   \n",
            "2   volume-101_10.png   segmentation-101_livermask_10.png          False   \n",
            "3  volume-101_100.png  segmentation-101_livermask_100.png          False   \n",
            "4  volume-101_101.png  segmentation-101_livermask_101.png          False   \n",
            "\n",
            "                   lesion_segmentation  lesion_visible  \n",
            "0    segmentation-101_lesionmask_0.png           False  \n",
            "1    segmentation-101_lesionmask_1.png           False  \n",
            "2   segmentation-101_lesionmask_10.png           False  \n",
            "3  segmentation-101_lesionmask_100.png           False  \n",
            "4  segmentation-101_lesionmask_101.png           False  \n"
          ]
        }
      ],
      "source": [
        "!gdown 1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
        "!rm -rf ./sample_data/\n",
        "!unzip -qq Clean_LiTS.zip\n",
        "!rm ./Clean_LiTS.zip\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ca92bcfd",
      "metadata": {
        "id": "ca92bcfd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms.functional as ttf\n",
        "\n",
        "class LiTS_Segmentation_Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, csv: str, mode: str):\n",
        "\n",
        "        self.csv = csv\n",
        "        self.data = pd.read_csv(self.csv)\n",
        "        self.mode = mode\n",
        "        assert mode in [\"train\", \"val\", \"test\"]\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "\n",
        "        file = self.data.loc[idx, \"filename\"]\n",
        "        with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "            f = f.convert(\"L\")\n",
        "            image = ttf.pil_to_tensor(f)\n",
        "        file = self.data.loc[idx, \"liver_segmentation\"]\n",
        "        with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "            f = f.convert(\"L\")\n",
        "            liver = ttf.pil_to_tensor(f)\n",
        "        file = self.data.loc[idx, \"lesion_segmentation\"]\n",
        "        with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "            f = f.convert(\"L\")\n",
        "            lesion = ttf.pil_to_tensor(f)\n",
        "\n",
        "        image = image.to(dtype = torch.float32)\n",
        "        image -= torch.min(image)\n",
        "        image /= torch.max(image)\n",
        "\n",
        "        c_targets = torch.zeros_like(image, dtype=torch.long)\n",
        "        c_targets[liver > 0] = 1\n",
        "        c_targets[lesion > 0] = 2\n",
        "\n",
        "        oh_targets = torch.zeros(3, image.shape[1], image.shape[2], dtype=torch.float32)\n",
        "        oh_targets[0] = (c_targets == 0).float()\n",
        "        oh_targets[1] = (c_targets == 1).float()\n",
        "        oh_targets[2] = (c_targets == 2).float()\n",
        "\n",
        "        return image, c_targets, oh_targets\n",
        "\n",
        "        # We need both the class-index version for the cross-entropy loss and the one-hot version for our dice loss later\n",
        "        # return image, c_targets, oh_targets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms.functional as ttf\n",
        "import pandas as pd\n",
        "import PIL.Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# CSV laden\n",
        "df = pd.read_csv(\"./Clean_LiTS/train_classes.csv\")\n",
        "\n",
        "# Zeilen filtern, wo eine Läsion sichtbar ist\n",
        "lesion_rows = df[df[\"lesion_visible\"] == True]\n",
        "\n",
        "# Ausgabe der ersten paar Zeilen\n",
        "print(\"Erste Zeilen mit Lesion sichtbar:\")\n",
        "print(lesion_rows.head())\n",
        "\n",
        "# Optional: den Index des ersten Samples mit Lesion\n",
        "if not lesion_rows.empty:\n",
        "    first_index = lesion_rows.index[0]\n",
        "    print(f\"\\nErster Index mit sichtbarer Läsion: {first_index}\")\n",
        "else:\n",
        "    print(\"Keine sichtbare Läsion im Dataset gefunden!\")\n",
        "\n",
        "# 1️⃣ Dataset-Instanz erstellen\n",
        "dataset = LiTS_Segmentation_Dataset(csv=\"./Clean_LiTS/train_classes.csv\", mode=\"train\")\n",
        "\n",
        "# 2️⃣ Prüfen, wie viele Samples es gibt\n",
        "print(f\"Anzahl der Samples: {len(dataset)}\")\n",
        "\n",
        "# 3️⃣ Ein Beispiel laden (z. B. erstes Sample)\n",
        "image, c_targets, oh_targets, liver = dataset[340]\n",
        "\n",
        "\n",
        "print(oh_targets)\n",
        "\n",
        "\n",
        "# 4️⃣ c_target untersuchen\n",
        "mask_np = c_targets.squeeze().numpy()  # (H, W) Array\n",
        "\n",
        "total_pixels = mask_np.size\n",
        "non_zero_pixels = (mask_np == 0).sum()\n",
        "liver_pixels = (mask_np == 1).sum()\n",
        "lesion_pixels = (mask_np == 2).sum()\n",
        "\n",
        "print(f\"Total pixels: {total_pixels}\")\n",
        "print(f\"Non-zero pixels (Leber + Läsion): {non_zero_pixels}\")\n",
        "print(f\"Liver pixels (class 1): {liver_pixels}\")\n",
        "print(f\"Lesion pixels (class 2): {lesion_pixels}\")"
      ],
      "metadata": {
        "id": "qZU78lQbtHps",
        "outputId": "d9318428-707e-4d01-86b7-730881a62905",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qZU78lQbtHps",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erste Zeilen mit Lesion sichtbar:\n",
            "               filename                  liver_segmentation  liver_visible  \\\n",
            "340  volume-101_404.png  segmentation-101_livermask_404.png           True   \n",
            "341  volume-101_405.png  segmentation-101_livermask_405.png           True   \n",
            "342  volume-101_406.png  segmentation-101_livermask_406.png           True   \n",
            "343  volume-101_407.png  segmentation-101_livermask_407.png           True   \n",
            "344  volume-101_408.png  segmentation-101_livermask_408.png           True   \n",
            "\n",
            "                     lesion_segmentation  lesion_visible  \n",
            "340  segmentation-101_lesionmask_404.png            True  \n",
            "341  segmentation-101_lesionmask_405.png            True  \n",
            "342  segmentation-101_lesionmask_406.png            True  \n",
            "343  segmentation-101_lesionmask_407.png            True  \n",
            "344  segmentation-101_lesionmask_408.png            True  \n",
            "\n",
            "Erster Index mit sichtbarer Läsion: 340\n",
            "Anzahl der Samples: 35484\n",
            "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
            "Total pixels: 65536\n",
            "Non-zero pixels (Leber + Läsion): 64815\n",
            "Liver pixels (class 1): 642\n",
            "Lesion pixels (class 2): 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LoSJm-I84jI6",
      "metadata": {
        "id": "LoSJm-I84jI6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "train_dataset = LiTS_Segmentation_Dataset(...)\n",
        "val_dataset = LiTS_Segmentation_Dataset(...)\n",
        "test_dataset = LiTS_Segmentation_Dataset(...)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 4,\n",
        "    prefetch_factor = 2,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 4,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 4,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d416cc5",
      "metadata": {
        "id": "2d416cc5"
      },
      "source": [
        "**Task 2 (2 points)**: Plot a few images that contain livers and tumors, as well as their corresponding segmentation maps. Do they look correct? Is there anything special to note?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7888356d",
      "metadata": {
        "id": "7888356d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0d744a45",
      "metadata": {
        "id": "0d744a45"
      },
      "source": [
        "**Task 3 (2 points)**: Next, we need a different loss function. At the bottom, we provide a training/testing loop that already contains cross-entropy loss and a functional segmentation model, plus evaluation. We have learned in the lecture that DICE score, and by extension a DICE-based loss, can be useful for imbalanced classes. We have also discovered that LiTS 2017 contains a class imbalance - slices with tumors are much more rare than slices with livers. Hence, we will make our own DICE loss.\n",
        "\n",
        "The formula for the DICE loss is computed as follows: $1 - \\frac{2 * (|X \\land Y|)+\\epsilon}{|X|+|Y|+\\epsilon}$, where $X$ is the prediction and $Y$ the target.\n",
        "\n",
        "The DICE Loss class you create should fulfill the following criteria:\n",
        "- It subclasses torch.nn.module.\n",
        "- It is a class that implements an \\_\\_init\\_\\_ function.\n",
        "- The loss also implements a \\_\\_forward\\_\\_ function that accepts as inputs a prediction tensor and a target tensor, both of shape B x 3 x 256 x 256 - 3 channels because we will segment background, liver, and liver+tumor again. The output is the computed loss.\n",
        "- You may add class weighting to offset the class imbalance.\n",
        "\n",
        "Your total loss should be `total_loss = ce_loss + dice_loss`, and your backward pass should be `total_loss.backward()`.\n",
        "Run the training for a few epochs, once with and once without DICE loss included as part of the overall loss. In your experiment, which version worked better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c01ba8f4",
      "metadata": {
        "id": "c01ba8f4"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as nnf\n",
        "\n",
        "def compute_dice_score(prediction: torch.Tensor, target: torch.Tensor):\n",
        "\n",
        "    \"\"\"\n",
        "    Computes the dice score for one class.\n",
        "    \"\"\"\n",
        "\n",
        "    prediction = prediction.to(dtype = torch.bool)\n",
        "    target = target.to(dtype = torch.bool)\n",
        "\n",
        "    intersection = torch.sum(prediction * target)   # TP\n",
        "    p_cardinality = torch.sum(prediction)           # TP+FP\n",
        "    t_cardinality = torch.sum(target)               # TP+FN\n",
        "    cardinality = p_cardinality + t_cardinality\n",
        "    eps = 1e-8\n",
        "\n",
        "    if cardinality != 0:\n",
        "        dice = (2 * intersection + eps) / (cardinality + eps) # 2*TP / (2*TP+FP+FN + eps)\n",
        "    else:\n",
        "        dice = None\n",
        "\n",
        "    return dice\n",
        "\n",
        "class BinaryDiceLoss(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Binary Dice Loss. Targets must be one-hot encoded.\n",
        "    Needed to make a full DICE loss, this computes dice loss for one channel.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def forward(self, prediction: torch.Tensor, target: torch.Tensor):\n",
        "\n",
        "        pass\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Dice Loss. Targets must be one-hot encoded.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
        "\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a3f09f",
      "metadata": {
        "id": "66a3f09f"
      },
      "source": [
        "**Task 4 (4 points)**: Finally, we want to make our own model that can handle segmentations. For this course, we will build ourselves a U-Net. The original paper can be found here: https://arxiv.org/pdf/1505.04597.\n",
        "\n",
        "The input dimensions for the network will be the usual B x 1 x 256 x 256. The output dimensions should be B x 3 x 256 x 256. We have three output channels because we will still predict classes 0 (background), 1 (liver) and 2 (liver tumor) - this time, however, we predict the classes on a per-pixel basis.\n",
        "\n",
        "Since our input images have vastly smaller dimensions compared to those used in the original UNet-Paper, we will opt for a different scale of UNet. The general design remains the same as in the paper, except:\n",
        "\n",
        "- We will only downsample 3 times by a factor of 2, using MaxPool (for a minimum resolution 32x32).\n",
        "- Our 3x3 Convolutions will have Padding. Consequently, there will be no cropping during skip connections\n",
        "- We will only have 3 skip connections.\n",
        "- We will go for fewer maximum channels (as we have only 3 downsampling steps, we will have 64, 128, 256, and 512 channels).\n",
        "- Our final output will be 3 channels wide, not 2 (we predict background, liver, and liver tumors).\n",
        "\n",
        "Note that training a segmentation models takes a little while - we do not award points for results here, because it would mean that you would have to wait a long time to see whether your changes helped performance. All we want to see is that your model learns anything useful at all. As a rough guideline, you will probably start seeing ok liver segmentations after 1 epoch, and good liver and ok lesion segmentations after 2 or 3 epochs.\n",
        "\n",
        "If everything works correctly, you can copy the previous training loop and should get some good results. Don't forget to look at some of your predictions! Are they reasonable? Empty? Weird? Can you discover some kind of systemic issues with your predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c43e445",
      "metadata": {
        "id": "6c43e445"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def __forward__(self):\n",
        "\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab0024b",
      "metadata": {
        "id": "0ab0024b"
      },
      "outputs": [],
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNet() # Your model class goes here\n",
        "model = model.to(device)\n",
        "\n",
        "dice_loss = DiceLoss(num_classes = 3) # Your dice loss class goes here\n",
        "ce_loss = nn.CrossEntropyLoss(\n",
        "    weight = torch.tensor([1.0, 5.0, 20.0]).to(device = device),\n",
        "    reduction = \"mean\",\n",
        "    #ignore_index = 0\n",
        "    )\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cX3ZORkH-y2q",
      "metadata": {
        "id": "cX3ZORkH-y2q"
      },
      "outputs": [],
      "source": [
        "# If your model and loss work, this should at least execute successfully.\n",
        "# If you only wish to test your model, just comment out the dice_loss component everywhere.\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "num_epochs = 5\n",
        "avg_liver_dice = 0\n",
        "avg_lesion_dice = 0\n",
        "dice_weight = 1\n",
        "ce_weight = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for step, (data, c_targets, oh_targets) in enumerate(train_dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        data, c_targets, oh_targets = data.to(device), c_targets.to(device), oh_targets.to(device)\n",
        "        predictions = model(data)\n",
        "\n",
        "        loss_1 = dice_loss(predictions, oh_targets)\n",
        "        loss_2 = ce_loss(predictions, c_targets)\n",
        "        total_loss = loss_1 * dice_weight + loss_2 * ce_weight # We could weight contributions from the different loss components here, although 1-to-1 should do just fine\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validate once after every epoch\n",
        "        model.eval()\n",
        "\n",
        "        # Don't track gradients for validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            losses = []\n",
        "            background_dices = []\n",
        "            background_counts = []\n",
        "            liver_dices = []\n",
        "            liver_counts = []\n",
        "            lesion_dices = []\n",
        "            lesion_counts = []\n",
        "            batch_sizes = []\n",
        "\n",
        "            for val_step, (data, c_targets, oh_targets) in enumerate(tqdm(val_dataloader)):\n",
        "\n",
        "                data, c_targets, oh_targets = data.to(device), c_targets.to(device), oh_targets.to(device)\n",
        "                predictions = model(data)\n",
        "                # Choose the likeliest prediction via argmax, then convert to one-hot, and put the new axis in front again\n",
        "                p_arg = nnf.one_hot(torch.argmax(predictions, dim = 1), num_classes = 3).moveaxis(-1, 1)\n",
        "\n",
        "                # loss\n",
        "                loss_1 = dice_loss(predictions, oh_targets)\n",
        "                loss_2 = ce_loss(predictions, c_targets)\n",
        "                total_loss = loss_1 * dice_weight + loss_2 * ce_weight # We could weight contributions from the different loss components here, although 1-to-1 should do just fine\n",
        "\n",
        "                losses.append(total_loss.item())\n",
        "                batch_sizes.append(data.size()[0])\n",
        "\n",
        "                background_seg = oh_targets[:, 0, :, :]\n",
        "                liver_seg = oh_targets[:, 1, :, :]\n",
        "                lesion_seg = oh_targets[:, 2, :, :]\n",
        "\n",
        "                background_dice = compute_dice_score(p_arg[:,0,:,:], background_seg)\n",
        "                background_counts.append(data.size()[0])\n",
        "                background_dices.append(background_dice)\n",
        "\n",
        "                if liver_seg.sum() != 0.0:\n",
        "                    liver_dice = compute_dice_score(p_arg[:,1,:,:], liver_seg)\n",
        "                    liver_counts.append(data.size()[0])\n",
        "                    liver_dices.append(liver_dice)\n",
        "\n",
        "                if lesion_seg.sum() != 0.0:\n",
        "                    lesion_dice = compute_dice_score(p_arg[:,2,:,:], lesion_seg)\n",
        "                    lesion_counts.append(data.size()[0])\n",
        "                    lesion_dices.append(lesion_dice)\n",
        "\n",
        "            avg_background_dice = sum([dice * size for dice, size in zip(background_dices, background_counts)])/sum(background_counts)\n",
        "            avg_liver_dice = sum([dice * size for dice, size in zip(liver_dices, liver_counts)])/sum(liver_counts)\n",
        "            avg_lesion_dice = sum([dice * size for dice, size in zip(lesion_dices, lesion_counts)])/sum(lesion_counts)\n",
        "\n",
        "            avg_loss = sum([l * bs for l, bs in zip(losses, background_counts)]) / sum(background_counts)\n",
        "            print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Liver Dice Score: {avg_liver_dice:.4f}, \\t Lesion Dice Score: {avg_lesion_dice:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fVDuNErcj7T",
      "metadata": {
        "id": "7fVDuNErcj7T"
      },
      "outputs": [],
      "source": [
        "# Test once\n",
        "model.eval()\n",
        "\n",
        "# Don't track gradients for testing\n",
        "with torch.no_grad():\n",
        "\n",
        "    losses = []\n",
        "    background_dices = []\n",
        "    background_counts = []\n",
        "    liver_dices = []\n",
        "    liver_counts = []\n",
        "    lesion_dices = []\n",
        "    lesion_counts = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for test_step, (data, c_targets, oh_targets) in enumerate(tqdm(test_dataloader)):\n",
        "\n",
        "        data, c_targets, oh_targets = data.to(device), c_targets.to(device), oh_targets.to(device)\n",
        "        predictions = model(data)\n",
        "        # Choose the likeliest prediction via argmax, then convert to one-hot, and put the new axis in front again\n",
        "        p_arg = nnf.one_hot(torch.argmax(predictions, dim = 1), num_classes = 3).moveaxis(-1, 1)\n",
        "\n",
        "        # loss\n",
        "        loss_1 = dice_loss(predictions, oh_targets)\n",
        "        loss_2 = ce_loss(predictions, c_targets)\n",
        "        total_loss = loss_1 * dice_weight + loss_2 * ce_weight # We could weight contributions from the different loss components here, although 1-to-1 should do just fine\n",
        "\n",
        "        losses.append(total_loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "\n",
        "        background_seg = oh_targets[:, 0, :, :]\n",
        "        liver_seg = oh_targets[:, 1, :, :]\n",
        "        lesion_seg = oh_targets[:, 2, :, :]\n",
        "\n",
        "        background_dice = compute_dice_score(p_arg[:,0,:,:], background_seg)\n",
        "        background_counts.append(data.size()[0])\n",
        "        background_dices.append(background_dice)\n",
        "\n",
        "        if liver_seg.sum() != 0.0:\n",
        "            liver_dice = compute_dice_score(p_arg[:,1,:,:], liver_seg)\n",
        "            liver_counts.append(data.size()[0])\n",
        "            liver_dices.append(liver_dice)\n",
        "\n",
        "        if lesion_seg.sum() != 0.0:\n",
        "            lesion_dice = compute_dice_score(p_arg[:,2,:,:], lesion_seg)\n",
        "            lesion_counts.append(data.size()[0])\n",
        "            lesion_dices.append(lesion_dice)\n",
        "\n",
        "    avg_background_dice = sum([dice * size for dice, size in zip(background_dices, background_counts)])/sum(background_counts)\n",
        "    avg_liver_dice = sum([dice * size for dice, size in zip(liver_dices, liver_counts)])/sum(liver_counts)\n",
        "    avg_lesion_dice = sum([dice * size for dice, size in zip(lesion_dices, lesion_counts)])/sum(lesion_counts)\n",
        "\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, background_counts)]) / sum(background_counts)\n",
        "    print(f\"Epoch: {epoch+1},\\t Test Loss: {avg_loss:.4f},\\t Liver Dice Score: {avg_liver_dice:.4f}, \\t Lesion Dice Score: {avg_lesion_dice:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oDnF9k7ZHhkE",
      "metadata": {
        "id": "oDnF9k7ZHhkE"
      },
      "outputs": [],
      "source": [
        "# Try looking at some images and predicted segmentations to see how badly or how well you've done\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}