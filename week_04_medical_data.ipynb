{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8cad4012",
      "metadata": {
        "id": "8cad4012"
      },
      "source": [
        "# Chapter 2 - Machine Learning in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n-arwS8e9YwS",
      "metadata": {
        "id": "n-arwS8e9YwS"
      },
      "source": [
        "In preparation for the coming lessons, we want to work on a machine with GPUs. To switch to one (if one is available), click on `Runtime` > `Change runtime type` and select a runtime which includes a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49feb29",
      "metadata": {
        "id": "e49feb29"
      },
      "source": [
        "**This week's exercise has 3 tasks for a total of 6 points. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "Since the overall goal for students of this seminar is to become capable of solving practical tasks in *modern* medical machine learning, we will have to learn model building and training with a modern machine learning library - PyTorch. Before we can do that, we need a few \"tools to put in our toolbox\". We will reuse these tools in the coming weeks. In this chapter, we want you to become proficient at the following tasks:\n",
        "\n",
        "- Handling medical data in different formats\n",
        "- Visualizing different medical data\n",
        "- Using PyTorch tensors\n",
        "- Building a PyTorch dataset, using medical data\n",
        "\n",
        "The code you see below is code for a standard bash terminal, which downloads some data for us (and which you do not need to learn). Colab offers a friendly way to execute such terminal commands directly from our notebook, by putting a `!` before them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "269c176c",
      "metadata": {
        "id": "269c176c"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "# Get QIN LUNG CT\n",
        "!pip install idc-index -q\n",
        "!gdown 1qBSweWSIMxhpsq1MEQIQk8wpr5lbnfE8\n",
        "!idc download ./manifest_20250930_060727_aws.s5cmd\n",
        "!find ./qin_lung_ct/ -depth -name 'SEG_*' -exec rm -rf {} \\;\n",
        "\n",
        "# Get LiTS-PNG\n",
        "!gdown 1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
        "!rm -rf ./sample_data/\n",
        "!unzip -qq Clean_LiTS.zip\n",
        "!rm ./Clean_LiTS.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fed692bf",
      "metadata": {
        "id": "fed692bf"
      },
      "source": [
        "Much like on a home computer, you can see and browse through the files we downloaded in a file browser. To access this file browser, look to the left of the code, and click on the little folder symbol."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba0347e7",
      "metadata": {
        "id": "ba0347e7"
      },
      "source": [
        "#### Chapter 2.1 - Reading and visualizing medical data\n",
        "\n",
        "Medical data can come in a variety of formats. In this chapter, we will look at the two most common ones in medical imaging: DICOM files and regular image files (e.g. JPG/PNG).\n",
        "\n",
        "As example data, we have downloaded two sets of data, the QIN LUNG CT dataset (https://doi.org/10.7937/K9/TCIA.2015.NPGZYZBZ, acquired from the TCIA via https://portal.imaging.datacommons.cancer.gov/explore) for DICOM files, and the LiTS 2017 dataset (https://doi.org/10.1016/j.media.2022.102680, acquired via Kaggle at https://www.kaggle.com/datasets/andrewmvd/lits-png)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4c2d6ac",
      "metadata": {
        "id": "a4c2d6ac"
      },
      "source": [
        "**DICOM Files** - DICOM files are files that adhere to the DICOM standard (https://www.dicomstandard.org/). Typically, a DICOM file contains two important things:\n",
        "- a header section, which is basically one large, standardized dictionary containing meta information like patient name (anonymized here), age, the hospital location, what kind of study was performed and on what body part, and a ton of other things.\n",
        "- a data section called \"pixel_array\", which typically holds one 2D image.\n",
        "\n",
        "Typically, DICOM files are structured like this:\n",
        "- One DICOM file holds a single 2D image\n",
        "- If the overall image is supposed to be 3D, like in CT images or MR images, the 3D image is split into 2D *slices*, and each slice has its own DICOM file. All of these have the same header information, but different image information\n",
        "- However many files are needed to have a full scan - 1 for an X-ray image, and sometimes up to 1000 for a really detailed CT - these files together make up what is called a *series*.\n",
        "- A *study* consists of one or several series. You can think of it as a single session, e.g. having injured your ribs in a car crash, the hospital is making several X-rays of your chest, one in a frontal view, and one in a lateral view.\n",
        "- The typical folder structure for DICOM files looks something like this: Each patient has a folder, each study of that patient has a sub-folder in that folder, and each series in a study has a sub-sub-folder, which is filled with our .dcm files.\n",
        "\n",
        "Let's look at an example file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OsY5dqLU5zLF",
      "metadata": {
        "id": "OsY5dqLU5zLF"
      },
      "outputs": [],
      "source": [
        "!pip install pydicom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18ed5df1",
      "metadata": {
        "id": "18ed5df1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pydicom as pdc\n",
        "\n",
        "# pydicom's dcmread function can be used to open a DICOM file like so:\n",
        "meta = pdc.dcmread(\"./qin_lung_ct/QIN-LSC-0003/1.3.6.1.4.1.14519.5.2.1.4320.7007.203059346048546067166621241946/CT_1.3.6.1.4.1.14519.5.2.1.4320.7007.113686129632252779806152571225/0243eb18-d571-450d-a665-0c62ac7df9c0.dcm\")\n",
        "\n",
        "# DICOM files\n",
        "print(meta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e595081",
      "metadata": {
        "id": "4e595081"
      },
      "source": [
        "As you can see, DICOM files contain a lot of information, structured like a dictionary. You can find a list of all DICOM tags, which are the keys of this dictionary here: https://www.dicomlibrary.com/dicom/dicom-tags/.\n",
        "\n",
        "We can access parts of the header, e.g. the modality (CT) like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09489022",
      "metadata": {
        "id": "09489022"
      },
      "outputs": [],
      "source": [
        "# Information can be accessed by DICOM tag number\n",
        "modality = meta[0x0008, 0x0060]\n",
        "print(modality)\n",
        "\n",
        "# or by DICOM tag name\n",
        "modality = meta[\"Modality\"]\n",
        "print(modality)\n",
        "\n",
        "# Any object - or DataElement - we get from this isn't quite one of the usual Python types\n",
        "print(type(modality))\n",
        "\n",
        "# But we can get the usual native Python types by accessing the value of the object\n",
        "print(modality.value, type(modality.value))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "740975c1",
      "metadata": {
        "id": "740975c1"
      },
      "source": [
        "We can also grab the image itself from the DICOM file like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e24bd7",
      "metadata": {
        "id": "13e24bd7"
      },
      "outputs": [],
      "source": [
        "image = meta.pixel_array\n",
        "plt.imshow(image, cmap = \"bone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b1d222",
      "metadata": {
        "id": "54b1d222"
      },
      "source": [
        "Now it's time for a little exercise, to get comfortable with the data format.\n",
        "\n",
        "**Task 1 (1 point)**: Go through the QIN LUNG CT dataset, and calculate the average slice thickness of each scan by extracting the information from the DICOM header."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "527bd7fb",
      "metadata": {
        "id": "527bd7fb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ae0b6e6d",
      "metadata": {
        "id": "ae0b6e6d"
      },
      "source": [
        "**JPEG/PNG**\n",
        "\n",
        "For this lesson, and later computer vision lessons, we will work with a dataset called `LiTS 2017`. LiTS is shorthand for \"**Li**ver **T**umor **S**egmentation\". It contains computed tomography (CT) images, once again split into 2D slices, and the goal in the LiTS challenge was to make a neural network accurately locate the liver as well as any tumors, in those images.\n",
        "\n",
        "Later on, we will first try to classify the images into three categories: No Liver Visible, Liver Visible, and Liver Tumor Visible. We will later also try to solve the segmentation challenge with our neural networks, i.e. painting in the actual areas in the image that contain liver tissue or tumors (lesions).\n",
        "\n",
        "Unlike the DICOM files from before, JPEG/PNG files do not contain the exhaustive meta information stored in the DICOM header - just the image itself. As such, there is nothing you need to learn, except opening them, and then using or visualizing the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d470afb",
      "metadata": {
        "id": "0d470afb"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch, torchvision\n",
        "\n",
        "# Let's first load an image. For this purpose, we use PIL ('Pillow'),\n",
        "# which can read jpg or png images from the disk.\n",
        "with PIL.Image.open(\"./Clean_LiTS/train/volume-10_337.png\") as f:\n",
        "\n",
        "    # Now we grab the array from the image.\n",
        "    img_array = np.array(f, dtype = np.uint8)\n",
        "\n",
        "# Let's see what size our image is.\n",
        "print(img_array.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85526a3",
      "metadata": {
        "id": "e85526a3"
      },
      "outputs": [],
      "source": [
        "# We can already visualize the image using matplotlib\n",
        "plt.figure()\n",
        "plt.imshow(img_array)\n",
        "plt.xlim((0, 256))\n",
        "plt.ylim((0, 256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b0ec879",
      "metadata": {
        "id": "7b0ec879"
      },
      "outputs": [],
      "source": [
        "# Lets make our plot look a little nicer.\n",
        "\n",
        "# Let us first load the image\n",
        "with PIL.Image.open(\"./Clean_LiTS/train/volume-10_337.png\") as f:\n",
        "\n",
        "    # This time, we convert the image to grayscale immediately.\n",
        "    # It always was already grayscale in a sense, but had 3 channels - RGB.\n",
        "    f = f.convert(\"L\")\n",
        "    # Now it has 1.\n",
        "\n",
        "    # We again grab the array from the image.\n",
        "    img_array = np.array(f, dtype = np.uint8)\n",
        "\n",
        "# This time, we also load the segmentation of the liver and potential tumors.\n",
        "# Think of it as a map of where in the image something - e.g. the liver or a\n",
        "# tumor - can be found:\n",
        "with PIL.Image.open(\"./Clean_LiTS/train/segmentation-10_livermask_337.png\") as f:\n",
        "    f = f.convert(\"L\")\n",
        "    liver = np.array(f, dtype = np.uint8)\n",
        "with PIL.Image.open(\"./Clean_LiTS/train/segmentation-10_lesionmask_337.png\") as f:\n",
        "    f = f.convert(\"L\")\n",
        "    tumors = np.array(f, dtype = np.uint8)\n",
        "\n",
        "# We make what is called a meshgrid - they are a tool designed for exactly the\n",
        "# kind of plot that we are making right now. A meshgrid is essentially every\n",
        "# combination of x and y coordinates in our image.\n",
        "X, Y = np.meshgrid(np.arange(256), np.arange(256))\n",
        "plt.figure()\n",
        "# We show the image again, but since we only have one channel now,\n",
        "# we get to use colormaps, which look a lot nicer than regular grayscale images.\n",
        "plt.imshow(img_array, cmap = \"bone\")\n",
        "plt.xlim((0, 256))\n",
        "plt.ylim((0, 256))\n",
        "# We can draw the 'truth' into our image with the help of the countour function\n",
        "plt.contour(X, Y, liver, colors = \"g\", alpha = 0.25, linewidths = 0.5)\n",
        "plt.contour(X, Y, tumors, colors = \"r\", alpha = 0.25, linewidths = 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe4a4347",
      "metadata": {
        "id": "fe4a4347"
      },
      "source": [
        "#### Chapter 2.2 - PyTorch Tensors\n",
        "\n",
        "We will now introduce a Python package called PyTorch. Besides TensorFlow, it is probably *the* premier machine learning environment. It comes with a lot of features and a couple things that may trip up new users. Therefore, we will try to slowly ease ourselves into using PyTorch.\n",
        "\n",
        "The `Tensor` sits at the heart of the PyTorch package. A tensor in machine learning is basically the same as an n-dimensional matrix. Under the hood, tensors work almost the same way as the numpy arrays we already know. However, tensors come with a couple of additional functionalities. Attached to each tensor is information about the data type it contains, the device it currently resides in (for example the regular memory, or the memory of a GPU), and most importantly information about its gradients. We will come back to the last bit later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa7b9176",
      "metadata": {
        "id": "fa7b9176"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# How do we make a tensor? Essentially the same as a numpy array.\n",
        "my_tensor = torch.tensor([1, 2, 3])\n",
        "print(my_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8752fb86",
      "metadata": {
        "id": "8752fb86"
      },
      "outputs": [],
      "source": [
        "# In fact, we can even make a numpy array into a tensor:\n",
        "my_array = np.array([2, 3, 4, 5])\n",
        "my_tensor = torch.tensor(my_array)\n",
        "print(my_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d587f6e7",
      "metadata": {
        "id": "d587f6e7"
      },
      "outputs": [],
      "source": [
        "# We can also get back our numpy array from a tensor:\n",
        "my_array = my_tensor.numpy()\n",
        "print(my_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2cd6a2c",
      "metadata": {
        "id": "e2cd6a2c"
      },
      "outputs": [],
      "source": [
        "# Tensors come with most of the same functionality as numpy arrays, such as\n",
        "# Knowing its own dimensions\n",
        "my_tensor = torch.zeros((1, 4))\n",
        "print(my_tensor.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9a7af8",
      "metadata": {
        "id": "5c9a7af8"
      },
      "outputs": [],
      "source": [
        "# Transposition\n",
        "print(my_tensor.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8432a889",
      "metadata": {
        "id": "8432a889"
      },
      "outputs": [],
      "source": [
        "# Reshaping\n",
        "print(my_tensor.reshape((2, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af36c4f",
      "metadata": {
        "id": "9af36c4f"
      },
      "outputs": [],
      "source": [
        "# Flattening\n",
        "print(my_tensor.flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241ed832",
      "metadata": {
        "id": "241ed832"
      },
      "outputs": [],
      "source": [
        "# Operations along an axis\n",
        "my_tensor = torch.tensor([[1, 2], [4, 3]])\n",
        "print(my_tensor.argmax(axis=0))\n",
        "print(my_tensor.argmax(axis=1))\n",
        "# and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6f6153",
      "metadata": {
        "id": "7a6f6153"
      },
      "outputs": [],
      "source": [
        "# Occasionally, you will come across tensors with only one component.\n",
        "# These are still, technically, a sort of matrix. However, you can turn\n",
        "# them into just a regular scalar if need be:\n",
        "t = torch.tensor(1)\n",
        "print(t)\n",
        "print(t.item())\n",
        "\n",
        "# As you may have already noticed, the syntax of the tensor operations is\n",
        "# almost the same as that for numpy arrays, but not always exactly the same.\n",
        "# Don't worry - PyTorch is well-documented. If you know what you want,\n",
        "# you can find it on Google or their documentation website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db5018b6",
      "metadata": {
        "id": "db5018b6"
      },
      "outputs": [],
      "source": [
        "# Tensors know where they currently live.\n",
        "# This attribute of a tensor is called 'device'.\n",
        "my_tensor = torch.tensor([[1, 2], [3, 4]])\n",
        "print(my_tensor.device)\n",
        "\n",
        "# We can manually move tensors and other PyTorch objects around on devices.\n",
        "# We do so with the '.to' method. The method expects the name of the device\n",
        "# we send it to. This could be \"cpu\", \"cuda\", or something like \"cuda:0\".\n",
        "\n",
        "# 'cuda' is the name of the engine under the hood, so to speak, and when we\n",
        "# specify 'cuda' as the target device, our tensor is put into the memory of\n",
        "# a GPU. If we specify the number, we select the nth GPU specifically.\n",
        "my_tensor = my_tensor.to(\"cuda:0\")\n",
        "print(my_tensor.device)\n",
        "\n",
        "# If you don't know whether you have a GPU available, you can even check:\n",
        "if torch.cuda.is_available() == True:\n",
        "    device = \"cuda:0\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "my_tensor = my_tensor.to(device)\n",
        "print(device, my_tensor.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1d2ea14",
      "metadata": {
        "id": "c1d2ea14"
      },
      "source": [
        "#### Chapter 2.3 - The PyTorch Recipe\n",
        "\n",
        "Machine learning in PyTorch commonly follows a very specific pattern. The more sophisticated your training algorithm and the more complex your data, the more you will eventually move away from this recipe. Conceptually, however, the steps remain largely the same:\n",
        "\n",
        "- I) Take some input images and do some preprocessing to them. This includes things such as guaranteeing the images to all have the same shape, cutting off uninteresting parts of the image, or performing image augmentations like adding noise (we will learn more about this later on).\n",
        "- II) Feed them to the GPU, if we have one.\n",
        "- III) Let the model make some predictions from our data.\n",
        "- IV) Compute the loss, given our predictions and the ground truths. (\"How wrong were my predictions?\")\n",
        "- V) Derive the gradients of the model parameters with respect to the loss. (\"In which direction must my parameters move to decrease my loss and thereby improve earlier predictions?\")\n",
        "- VI) Move all model parameters into this direction for a certain distance. (\"Ask, how far must I go in this direction, then go.\")\n",
        "- VII) Repeat 1-6 until the predictions become good.\n",
        "- VIII) Occasionally validate your performance on previously unseen data to check whether they are good.\n",
        "- IX) Finally, evaluate the model on previously unseen data. This is the final result, and the thing reported in papers.\n",
        "\n",
        "For now, we will go through the recipe and get an understanding of what we have to do and why. Most of what you see should be familiar to you because of the lecture. Once we understand everything, we will start building all the components ourselves to get the hang of coding in PyTorch.\n",
        "\n",
        "If something does not work later on, it can always be good to go back to the recipe and try to see what the recipe does that you maybe don't."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d075e218",
      "metadata": {
        "collapsed": true,
        "id": "d075e218"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "As promised, this ready-to-go example will actually train something,\n",
        "although it is not particularly sophisticated. Try to execute it,\n",
        "and try to follow what each part of the code does!\n",
        "\n",
        "1) Imports\n",
        "First, we handle our imports. Typically we do this at the top of our\n",
        "program. If looking at the text starts bothering you because it is so\n",
        "much, you are free to delete it or edit it. If you ever need it again,\n",
        "the original is still on GitHub.\n",
        "\"\"\"\n",
        "\n",
        "import torch, torchvision\n",
        "\n",
        "# We also check if CUDA (read: at least one GPU) is available\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\"\"\"\n",
        "2) The Dataset\n",
        "PyTorch has specific requirements that datasets need to fulfill.\n",
        "A dataset is a class and it handles your data (surprise). In the\n",
        "next chapter, we will make our own, and we will discuss in more\n",
        "detail what actually needs to be done to replace one.\n",
        "\n",
        "For now, we will stick with a pre-existing one, called CIFAR10,\n",
        "which already exists. CIFAR10 contains images of flowers, which we\n",
        "want to try and classify. Once you understand the recipe, we will\n",
        "replace CIFAR10 with medical data, and you will make the dataset\n",
        "yourself.\n",
        "\n",
        "(Since someone thought it would be a fantastic idea for the dataset to\n",
        "return PIL Images instead of tensors, we add a ToTensor transform. We\n",
        "will come back to transforms and how to use them later. For now, you\n",
        "don't have to concern yourself with that.)\n",
        "\n",
        "Finally, since we will want to gain an understanding of the actual\n",
        "performance we have reached, we will split our dataset into a train,\n",
        "validation, and test set, containing 80%, 10% and 10% of the data\n",
        "respectively. This lets us\n",
        "\"\"\"\n",
        "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()]) # Ignore this bit for now\n",
        "dataset = torchvision.datasets.CIFAR10(\"../data/CIFAR10/\", download = True, transform = transforms)\n",
        "dl = len(dataset)\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(dataset = dataset, lengths = [int(0.8*dl), int(0.1*dl), int(0.1*dl)])\n",
        "\n",
        "# 3) The Dataloader\n",
        "\"\"\"\n",
        "A dataloader wraps a dataset. For the most part, it just grabs random\n",
        "data points from your dataset and glues the tensors together along the\n",
        "first dimension and then returns them. We will discuss the options that\n",
        "the dataloader affords us in more detail below, but we will not have to\n",
        "write our own.\n",
        "\n",
        "Example: If your data is comprised of 256x256 pixel images, and your\n",
        "batch size is 32, then your dataset might return a 256x256 tensor,\n",
        "while the dataloader would tell its 4 workers \"I need 32 images\", gets\n",
        "8 from each worker, and returns to you a size 32x256x256 tensor.\n",
        "\"\"\"\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = train_set,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 4,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        "    )\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset = val_set,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 0,\n",
        "    shuffle = False,\n",
        "    drop_last = False\n",
        "    )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset = test_set,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 0,\n",
        "    shuffle = False,\n",
        "    drop_last = False\n",
        "    )\n",
        "\n",
        "# 4) The Model\n",
        "\"\"\"\n",
        "If machine learning is a meal, then the model is the meat. In a sense,\n",
        "the mathematical operations a model can perform are completely arbitrary,\n",
        "so long as they are almost everywhere  differentiable (and some of them\n",
        "are non-linear), so that normal gradient descent can be performed. A\n",
        "model simply takes some tensor(s) as input and uses that input and its\n",
        "own model parameters to create some output tensor(s). We will revisit\n",
        "models, how they typically look, why they look like that - and how we can\n",
        "make some fairly powerful ones ourselves - next week.\n",
        "\n",
        "For the moment, we will simply pick up a pre-existing model and not make\n",
        "one ourselves. The only thing we change is the final layer of the model,\n",
        "since the model was originally designed for different data. Don't concern\n",
        "yourself with that last step for now, we will come back to this.\n",
        "\"\"\"\n",
        "model = torchvision.models.resnet50(pretrained = True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 10) # Ignore me\n",
        "model = model.to(device)\n",
        "\n",
        "# 5) The Loss function\n",
        "\"\"\"\n",
        "Models typically learn by creating some output, comparing it against\n",
        "a target, and then adjusting the model parameters so that the model\n",
        "would have created an output closer to the target. This necessitates\n",
        "a mathematical way of comparing the model output to said target. This\n",
        "function is called a Loss Function.\n",
        "\n",
        "In order for the model to be able to benefit from this representation of\n",
        "its quality, said function needs to fulfill several criteria: It has to\n",
        "be differentiable (almost) everywhere, so we can derive gradients for\n",
        "our model parameters from it and it needs to return lower values when\n",
        "the output and targets match up.\n",
        "\n",
        "As previously mentioned, we are trying to train a classification model\n",
        "which can tell apart the different kinds of flowers in CIFAR-10. Thus,\n",
        "our outputs (or \"predictions\") will be size 10 tensors, where each value\n",
        "corresponds to one of the 10 types of flowers in CIFAR-10. Our targets\n",
        "will be tensors of the same size and will be 1 for the correct class\n",
        "and 0 everywhere else.\n",
        "\n",
        "If the model output is also normed to one, we can apply some intuitive\n",
        "thinking here. Say that the model output after normalization was:\n",
        "[0.0, 0.0, 0.0, 0.2, 0.5, 0.1, 0.1, 0.0, 0.0, 0.1] .\n",
        "Our target was:\n",
        "[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] .\n",
        "In a sense, the model has given the prediction that the input image\n",
        "was class 4 with 20% probability, class 5 with 50% probability, class 6\n",
        "with 10% probability, etc. A perfect model should have predicted 100%\n",
        "probability for class 5 and 0% probability for everything else. It\n",
        "is important to note that model outputs, even when normalized, are NOT\n",
        "actually predictions. It helps to think about them this way, but\n",
        "models are often overconfident (and rarely underconfident) compared\n",
        "to their actual ability to classify something - a model may make\n",
        "every prediction with 90% probability and still only have 30% correct\n",
        "predictions. When the two actually match up - there are techniques to\n",
        "encourage this in models - a model is called calibrated. Most of the\n",
        "time, this isn't done, because we care more about the prediction\n",
        "itself than the pseudo-probability that its correct.\n",
        "\n",
        "To optimize our models (or \"let them learn\"), we will later compute\n",
        "the loss for each batch of predictions and targets and then compute\n",
        "the gradient of that loss with respect to each parameter. Effectively,\n",
        "the model finds out in which directions its parameters must be adjusted\n",
        "to minimize the loss on this batch (and probably next time, too).\n",
        "\n",
        "Writing such a loss function is comparatively easy, and we will be doing\n",
        "so much later during the course. For now, we will use an out-of-the-box\n",
        "loss function that PyTorch provides, called CrossEntropyLoss. You can\n",
        "find the exact formula of how predictions are \"rated\" here:\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "\"\"\"\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# 6) The Optimizer\n",
        "\"\"\"\n",
        "This is where a large part of the magic happens. The optimizer has access\n",
        "to all of a model's parameters. When each parameter has figured out in\n",
        "which 'direction' the greatest improvement to the earlier predictions was\n",
        "to be found, the optimizer figures out, how far to actually move the\n",
        "parameters in this direction.\n",
        "\n",
        "(Good) optimizers are quite complicated to write, and few people fully\n",
        "understand the complex math involved in guaranteeing convergence and\n",
        "stability of the learning process. We may look at a couple different ones\n",
        "and give some overview of how they operate. However, we will not be making\n",
        "our own. For now, we will simply pick up a proven, existing one.\n",
        "\n",
        "Optimizers have a multitude of different settings. The most important one\n",
        "is the learning rate. Some may adjust the learning rate per parameter or\n",
        "try to learn it as well, but generally you always have to supply this\n",
        "value or at least a starting value to the optimizer. We pick 10^-4 as a\n",
        "starting point here, but this is arbitrary and it's almost always worth\n",
        "trying out more values. We will look at other settings later in the\n",
        "seminar.\n",
        "\"\"\"\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4)\n",
        "\n",
        "# 7) The Training Loop\n",
        "\"\"\"\n",
        "Now it's time to stick all of it together.\n",
        "We follow steps I-IX from the recipe.\n",
        "\"\"\"\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # I) Grab my data\n",
        "    for step, (data, targets) in enumerate(train_loader):\n",
        "\n",
        "        # This command is a peculiarity of PyTorch.\n",
        "        # PyTorch accumulates gradients of parameters instead of overwriting\n",
        "        # them when new predictions are made. This is a useful feature, but\n",
        "        # most of the time, people do not use it. zero_grad() manually resets\n",
        "        # this accumulation process.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # II) Put it onto the GPU.\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        # III) Make some predictions.\n",
        "        predictions = model(data)\n",
        "\n",
        "        # IV) How wrong were our predictions?\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "\n",
        "        # At this point it's also useful to print out performance indicators.\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_set)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # V) In which direction do we have to go to make them better?\n",
        "        # .backward() computes the gradients of the loss with respect to each individual\n",
        "        # parameter. It figures out the correct differentiation on its own.\n",
        "        loss.backward()\n",
        "\n",
        "        # VI) How far do I have to go in this direction?\n",
        "        # Once I know it, move the parameters accordingly.\n",
        "        optimizer.step()\n",
        "\n",
        "        # VII) Repeat.\n",
        "\n",
        "    # VIII) Let's occasionally validate our model's performance on unseen data.\n",
        "    if epoch % 2 == 0:\n",
        "\n",
        "        # We first tell our model to not try and train parameters for now.\n",
        "        model.eval()\n",
        "\n",
        "        # We also tell PyTorch to not collect any gradients for the time being.\n",
        "        with torch.no_grad():\n",
        "\n",
        "            hits = 0\n",
        "            losses = []\n",
        "            batch_sizes = []\n",
        "\n",
        "            # This time, we grab the validation data our model has not seen before.\n",
        "            for step, (data, targets) in enumerate(val_loader):\n",
        "\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                predictions = model(data)\n",
        "                loss = loss_criterion(predictions, targets)\n",
        "                losses.append(loss.item())\n",
        "                batch_sizes.append(data.size()[0])\n",
        "\n",
        "                # In the hope that our performance during validation is indicative\n",
        "                # of our performance on the test set (which is usually reported in\n",
        "                # papers and challenges), let's measure our actual accuracy.\n",
        "                # First, we convert our predictions from a tensor of dimensions\n",
        "                # N x C (batch size x classes) to one of dimension N.\n",
        "                class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                # Then, we count the number of correct predictions.\n",
        "                hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "            accuracy = hits / len(val_set)\n",
        "            avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "            print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # After we are done validating, let's not forget to go back to storing gradients!\n",
        "        model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c5324b",
      "metadata": {
        "id": "a1c5324b"
      },
      "source": [
        "**Task 2 (1 point)**: The script above performs training and validation, but never actually uses the test set for anything. Complete the script above by using the test set to evaluate performance after training is complete. To speed up the process, you can reduce the number of training epochs, or skip training and evaluate immediately, just to see if it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad30c603",
      "metadata": {
        "id": "ad30c603"
      },
      "outputs": [],
      "source": [
        "# IX) Let's check our final performance on the test set.\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27e3e53c",
      "metadata": {
        "id": "27e3e53c"
      },
      "source": [
        "#### Chapter 2.4 - PyTorch Datasets and DataLoaders\n",
        "\n",
        "Our eventual goal is to train models using PyTorch. In almost every case, doing so will involve so-called `Dataset` and `DataLoader`objects.\n",
        "\n",
        "Let's start with the dataset - What is a dataset? A dataset is a Python class. There are two types of dataset, only one of which will be relevant during this course, the so-called map-style dataset. The map-style dataset has to implement three methods. Typically, you will implement more than that, and there is no limit to the amount of functionality you can try and cram into it, but these three are the absolute minimum that must exist: an \\_\\_init\\_\\_ method, a \\_\\_len\\_\\_ method, and a \\_\\_getitem\\_\\_ method. We will look at what they do down below, by making a small `Example_Dataset` class.\n",
        "\n",
        "Helpfully, PyTorch has a basic dataset class, which we can inherit from, giving us access to more useful methods. However, this is not strictly a requirement, and we can work with any improvised dataset as long as it has the three functions mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aed8aef",
      "metadata": {
        "id": "7aed8aef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List\n",
        "\n",
        "class Example_Dataset(Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    The first function any dataset needs is the __init__ function. It is implicitly\n",
        "    called when the dataset instance is created. We can define it to accept any\n",
        "    input we like, or even none at all. Of course, it does need to take the self\n",
        "    attribute as its first argument.\n",
        "\n",
        "    Typically, this function contains things like the dataset path, or maybe\n",
        "    some settings, if your dataset is sophisticated enough.\n",
        "\n",
        "    To show you how it works, we have constructed this very bare-bones example.\n",
        "    Later, you will try your hand at making a more sophisticated one yourselves,\n",
        "    and, should that not work out, there is also a more sophisticated version\n",
        "    available in the utility functions we will provide you with.\n",
        "    \"\"\"\n",
        "    def __init__(self, data: List[torch.Tensor, ]):\n",
        "        # If we do inherit from PyTorch's Dataset class, let's also call it's\n",
        "        # __init__ function. This is generally a sensible idea when inheriting.\n",
        "        super(Example_Dataset, self).__init__()\n",
        "\n",
        "        # Any parameters that the __init__ function later expects, we probably\n",
        "        # want to remember for later use, so we tack them onto self like so:\n",
        "        self.data = data\n",
        "\n",
        "        # If we want to compute something, it might make sense to do it here,\n",
        "        # once, at the start, and remember that result for later.\n",
        "        # For example, let's check how many training images we have:\n",
        "        self.dataset_length = len(self.data)\n",
        "\n",
        "        return None\n",
        "\n",
        "    \"\"\"\n",
        "    The second function our dataset class requires is the __len__ function.\n",
        "    If we later want to use len(my_dataset) for example, and of course simply\n",
        "    for the dataloader to function, we need to figure out how much data is in\n",
        "    our dataset.\n",
        "\n",
        "    We could hardcode this value, if we know the amount of images we have.\n",
        "    Alternatively, we could check the amount of files ending on '.png' in the\n",
        "    folder containing our data. All that matters, is that the function returns\n",
        "    an integer (an integer that makes sense, preferably).\n",
        "    \"\"\"\n",
        "    def __len__(self):\n",
        "        # Since we already computed the amount of data we have above, we can\n",
        "        # just return this value.\n",
        "\n",
        "        return self.dataset_length\n",
        "\n",
        "    \"\"\"\n",
        "    The third function is the most important of the functions we need. It is\n",
        "    called the __getitem__ function. It takes as input an integer, which is\n",
        "    typically called index or simply idx, and that specifies which of our\n",
        "    datapoints in the dataset we would like to have returned.\n",
        "\n",
        "    Funnily enough, despite being the most important function, what we do\n",
        "    inside of it is largely up to us. We can load our images from the disk,\n",
        "    we can preprocess them so that they have a certain shape, or contain\n",
        "    only values in a certain range, etc etc.\n",
        "\n",
        "    Customarily, we return two things in this function: The image, and the\n",
        "    associated target. The target is the \"correct answer\" from which models\n",
        "    learn. For example, if we have a dataset of liver CTs which either\n",
        "    contain a tumor or no tumor, and we want to perform classification,\n",
        "    the target would be a tensor containing a zero if there is no tumor in\n",
        "    the image, and a one if there is a tumor in the image.\n",
        "\n",
        "    (More complex models or training methods may have different targets and\n",
        "    sometimes even no targets, although the above description is the easiest\n",
        "    and most common scenario.)\n",
        "    \"\"\"\n",
        "    def __getitem__(self, idx: int):\n",
        "\n",
        "        # Load the correct image\n",
        "        image = self.data[idx]\n",
        "\n",
        "        # Let's make up a target for our example images\n",
        "        target = torch.tensor(0, dtype = torch.long)\n",
        "\n",
        "        return image, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "508acf42",
      "metadata": {
        "id": "508acf42"
      },
      "outputs": [],
      "source": [
        "# Let's make some random images and put them in a list ...\n",
        "random_images = []\n",
        "for n in range(100):\n",
        "    random_image = torch.randn(size = (256, 256))\n",
        "    random_images.append(random_image)\n",
        "\n",
        "# ... and construct an example dataset using our class from before\n",
        "my_example_dataset = Example_Dataset(data = random_images)\n",
        "\n",
        "# This should now work:\n",
        "image, target = my_example_dataset.__getitem__(idx = 42)\n",
        "\n",
        "print(image)\n",
        "print(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c63f935",
      "metadata": {
        "id": "9c63f935"
      },
      "source": [
        "A PyTorch `DataLoader` is a wrapper around the `Dataset` class, which helps do a lot of the stuff we don't want to do manually:\n",
        "- It randomizes the order in which datapoints are loaded\n",
        "- It creates a number of worker processes, all of which have the dataset in them\n",
        "- It loads a number of datapoints, each worker working in parallel\n",
        "- It collates all of the loaded data into one object called a `Batch`\n",
        "\n",
        "This is how we make a `DataLoader` from our `Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb9007e1",
      "metadata": {
        "id": "bb9007e1"
      },
      "outputs": [],
      "source": [
        "my_example_dataloader = DataLoader(\n",
        "    dataset = my_example_dataset,\n",
        "    batch_size = 2,\n",
        "    num_workers = 1,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e8c2b8",
      "metadata": {
        "collapsed": true,
        "id": "e5e8c2b8"
      },
      "outputs": [],
      "source": [
        "# And now, we can get as much data as we want in a simple for-loop\n",
        "# and everything else is being done for us!\n",
        "\n",
        "for images, targets in my_example_dataloader:\n",
        "    print(images.size())\n",
        "    print(targets.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa28376",
      "metadata": {
        "id": "5aa28376"
      },
      "source": [
        "The most important configuration options for the `DataLoader` are the ones we access above, although there are many more that are worth checking out in the official documentation.\n",
        "\n",
        "**Batch Size** - The amount of data in a single batch. This is the amount of images (or data in general) that our neural networks will \"see at once\". The perfect batch size exists in a sweet spot; Too few in a batch, and we overfit, relying entirely on a few data poitns to make decisions. Too many in a batch and the data may no longer fit into RAM/VRAM. (If the batch size gets sufficiently high, thousands or even millions, the resulting gradients sometimes also decrease in quality - instead of the parameters dynamically changing, the average gradient for all parameters gets close to 0 and the model just gets stuck)\n",
        "\n",
        "**Number of Workers** - The amount of processes PyTorch spawns for our DataLoader. The higher the number, the faster the data loading. However, there are diminishing returns, because creating a process takes a little time, and communicating with it also does. If the number exceeds the batch size, you even make your data loading *slower*, because now you're creating processes that don't even do anything.\n",
        "\n",
        "**Shuffling** - This randomizes the order in which data points are drawn from your `Dataset` by the `DataLoader`. In general, we almost always want to do this, because a lot of the time, our data is sorted in some way. If we had a batch where every image was almost the same, for example, this would hurt the training, similarly to having a small batch size.\n",
        "\n",
        "**Drop Last Batch** - Let's say our Dataset contains 37 images and that our batch size is 16. The first two batches would contain 16 images, but the third and final one of the epoch would only contain 5. A lot of models will stumble over this, and therefore the last batch is often dropped, which means we simply don't use it. If you ever see your training behaving weirdly at the end of an epoch, check if you forgot to turn this feature on."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30615577",
      "metadata": {
        "id": "30615577"
      },
      "source": [
        "Now it's your turn!\n",
        "\n",
        "**Task 3 (4 points)**: Create a fully functional PyTorch Dataset for the LiTS 2017 data we downloaded at the start.\n",
        "As a guideline, here is what you probably want to do:\n",
        "- Use Python to read the .csv file for the LiTS training data in `./Clean_LiTS/` **(1 point)**\n",
        "- Store all the information you need in the dataset class you're writing\n",
        "- In the \\_\\_getitem\\_\\_ method, load the corresponding image and return it and the target (The targets are also in the .csv file) **(1 point)**\n",
        "- Alternatively, you can load all images in the init method already, store them in the dataset class, and simply return them in the \\_\\_getitem\\_\\_ method - what do you think might be the advantages and disadvantages if you did that? When would you do it or not do it? Explain. **(1 point)**\n",
        "- Can you think of a clever way to make a dataset which contains the training data, validation data and testing data? If yes, write one **(1 point)**. Otherwise, you can just make separate training dataset, validation dataset, and test dataset objects.\n",
        "\n",
        "We will train our own model using these datasets in the next exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56474ae2",
      "metadata": {
        "id": "56474ae2"
      },
      "outputs": [],
      "source": [
        "class LiTS_Dataset(Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    Your documentation goes here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_csv: str, val_csv: str, test_csv: str):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DSqlp5vEyqF9",
      "metadata": {
        "id": "DSqlp5vEyqF9"
      },
      "outputs": [],
      "source": [
        "lits_dataset = LiTS_Dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de080de",
      "metadata": {
        "id": "3de080de"
      },
      "outputs": [],
      "source": [
        "lits_dataloader = DataLoader(\n",
        "    dataset = lits_dataset,\n",
        "    batch_size = 16,\n",
        "    num_workers = 0, # 0 workers means you get 1 worker and it works from the main process - nice for debugging, because error messages are much more legible and there is only one instead of four.\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "# This should now work:\n",
        "for images, targets in tqdm(lits_dataloader):\n",
        "    print(images.size(), targets.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68487889",
      "metadata": {
        "id": "68487889"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}